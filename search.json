[
  {
    "objectID": "starwars2/starwars_df2.html",
    "href": "starwars2/starwars_df2.html",
    "title": "Starwars 2",
    "section": "",
    "text": "Let’s analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "starwars2/starwars_df2.html#variable-description-for-starwars-data.frame",
    "href": "starwars2/starwars_df2.html#variable-description-for-starwars-data.frame",
    "title": "Starwars 2",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "starwars2/starwars_df2.html#human-vs.-droid",
    "href": "starwars2/starwars_df2.html#human-vs.-droid",
    "title": "Starwars 2",
    "section": "Human vs. Droid",
    "text": "Human vs. Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "seaborn_basics.html",
    "href": "seaborn_basics.html",
    "title": "Seaborn Example",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 56, 78]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.set(style=\"whitegrid\")  # Optional: Set a clean grid style\nplt.figure(figsize=(8, 6))  # Set the figure size\nsns.barplot(data=df, x='Category', y='Values', palette='viridis')\n\n# Customize the plot\nplt.title(\"Bar Plot Example\", fontsize=16)\nplt.xlabel(\"Category\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\n\n# Show the plot\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=df, x='Category', y='Values', palette='viridis')"
  },
  {
    "objectID": "project-210.html",
    "href": "project-210.html",
    "title": "DANL210 Project",
    "section": "",
    "text": "This project will address the relationship between Environmental, Governance, and Social (ESG) risk ratings and stock performance. ESG risk ratings evaluate a company’s ethical performance and how well they manage risks and opportunities in environmental, social, and governance areas. Given this, I will be able to analyze the relationship between stock performance and ethical performance. All data was collected from Yahoo Finance."
  },
  {
    "objectID": "project-210.html#variable-description",
    "href": "project-210.html#variable-description",
    "title": "DANL210 Project",
    "section": "2.1 Variable Description",
    "text": "2.1 Variable Description\n\n2.1.1 esg_data\n\ntotal_esg: Total ESG rating. Environmental, social, and government risk ratings combined.\nenviro_risk: Environmental risk rating.\nsocial_risk: Social risk rating.\ngov_risk: Governmental risk rating.\ncontroversy: Controversy level."
  },
  {
    "objectID": "project-210.html#summary-statistics",
    "href": "project-210.html#summary-statistics",
    "title": "DANL210 Project",
    "section": "3.1 Summary Statistics",
    "text": "3.1 Summary Statistics\n\nesg_data['enviro_risk'].describe()\n\ncount    537.000000\nmean       5.827188\nstd        5.301806\nmin        0.000000\n25%        1.800000\n50%        4.000000\n75%        9.000000\nmax       27.300000\nName: enviro_risk, dtype: float64\n\nesg_data['social_risk'].describe()\n\ncount    537.000000\nmean       9.042644\nstd        3.612652\nmin        0.800000\n25%        6.700000\n50%        8.900000\n75%       11.100000\nmax       22.500000\nName: social_risk, dtype: float64\n\nesg_data['gov_risk'].describe()\n\ncount    537.000000\nmean       6.811173\nstd        2.378037\nmin        2.400000\n25%        5.200000\n50%        6.300000\n75%        7.900000\nmax       19.400000\nName: gov_risk, dtype: float64\n\n\n\ntickers_history['Close'].describe()\n\ncount    357202.000000\nmean        142.152567\nstd         297.241789\nmin           0.980000\n25%          40.449798\n50%          81.209999\n75%         152.080002\nmax        8099.959961\nName: Close, dtype: float64\n\ntickers_history['High'].describe()\n\ncount    357202.000000\nmean        143.808907\nstd         300.607248\nmin           1.060000\n25%          40.966236\n50%          82.190002\n75%         153.830002\nmax        8158.990234\nName: High, dtype: float64\n\ntickers_history['Low'].describe()\n\ncount    357202.000000\nmean        140.424490\nstd         293.761505\nmin           0.780000\n25%          39.901238\n50%          80.220911\n75%         150.270004\nmax        8010.000000\nName: Low, dtype: float64\n\n\nMerge the data frames esg_data and tickers_history in order to compare:\n\nmerged_df = pd.merge(esg_data, tickers_history, left_index = True, right_index = True, how = 'inner')\n\nDistribution of Close:\n\nsns.histplot(merged_df['Close'])\n\n\n\n\nDistribution of Environmental Risk Rating:\n\nsns.histplot(merged_df['enviro_risk'])"
  },
  {
    "objectID": "project-210.html#correlation-heat-map-of-esg-data",
    "href": "project-210.html#correlation-heat-map-of-esg-data",
    "title": "DANL210 Project",
    "section": "4.1 Correlation Heat Map of ESG Data:",
    "text": "4.1 Correlation Heat Map of ESG Data:\n\nimport matplotlib.pyplot as plt\ncorr = esg_data.corr()\nplt.figure(figsize=(8,6))\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths = .5)\nplt.title('Correlation Heatmap of ESG Data')\nplt.show()"
  },
  {
    "objectID": "project-210.html#relationship-between-close-price-and-environmental-risk-score",
    "href": "project-210.html#relationship-between-close-price-and-environmental-risk-score",
    "title": "DANL210 Project",
    "section": "4.2 Relationship between Close Price and Environmental Risk Score:",
    "text": "4.2 Relationship between Close Price and Environmental Risk Score:\n\nsns.lmplot(merged_df, x = 'enviro_risk', y = 'Close')\n\n\n\n\n\n\n\nThere is not much correlation between Close Price and Environmental Risk Score. Our scatter plot and best fit line are relatively flat. If anything, there is a slight positive relationship. Higher risk might mean a higher close price, however, we cannot make a conclusion from this."
  },
  {
    "objectID": "project-210.html#next-relationship-between-close-price-and-social-risk-score",
    "href": "project-210.html#next-relationship-between-close-price-and-social-risk-score",
    "title": "DANL210 Project",
    "section": "4.3 Next, relationship between Close Price and Social Risk Score:",
    "text": "4.3 Next, relationship between Close Price and Social Risk Score:\n\nsns.lmplot(merged_df, x = 'social_risk', y = 'Close')\n\n\n\n\n\n\n\nAgain, there is not much correlation between Close Price and Social Risk Score. Based on the graph, there could be a slight negative relationship, if any. Higher risk could mean lower close price. However, we cannot conclude this."
  },
  {
    "objectID": "project-210.html#finally-relationship-between-close-price-and-government-risk-score",
    "href": "project-210.html#finally-relationship-between-close-price-and-government-risk-score",
    "title": "DANL210 Project",
    "section": "4.4 Finally, relationship between Close Price and Government Risk Score:",
    "text": "4.4 Finally, relationship between Close Price and Government Risk Score:\n\nsns.lmplot(merged_df, x = 'gov_risk', y = 'Close')\n\n\n\n\n\n\n\nOnce again, there is not much correlation between the two. Like environmental risk, there could be a slight positive relationship. As risk increases, so does close price. However, we cannot conclude this."
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html",
    "href": "posts/spotify/spotify_willie_nelson.html",
    "title": "Spotify Data Frame",
    "section": "",
    "text": "Below is the Spotify Data Frame that reads the file spotify_all.csv containing data of Spotify users’ playlist information (Source: Spotify Million Playlist Dataset Challenge).\nimport pandas as pd\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\nspotify\n\nWarning: total number of rows (198005) exceeds max_rows (20000). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\nartist_name\ntrack_name\nduration_ms\nalbum_name\n\n\n\n\n0\n0\nThrowbacks\n0\nMissy Elliott\nLose Control (feat. Ciara & Fat Man Scoop)\n226863\nThe Cookbook\n\n\n1\n0\nThrowbacks\n1\nBritney Spears\nToxic\n198800\nIn The Zone\n\n\n2\n0\nThrowbacks\n2\nBeyoncé\nCrazy In Love\n235933\nDangerously In Love (Alben für die Ewigkeit)\n\n\n3\n0\nThrowbacks\n3\nJustin Timberlake\nRock Your Body\n267266\nJustified\n\n\n4\n0\nThrowbacks\n4\nShaggy\nIt Wasn't Me\n227600\nHot Shot\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198000\n999998\n✝️\n6\nChris Tomlin\nWaterfall\n209573\nLove Ran Red\n\n\n198001\n999998\n✝️\n7\nChris Tomlin\nThe Roar\n220106\nLove Ran Red\n\n\n198002\n999998\n✝️\n8\nCrowder\nLift Your Head Weary Sinner (Chains)\n224666\nNeon Steeple\n\n\n198003\n999998\n✝️\n9\nChris Tomlin\nWe Fall Down\n280960\nHow Great Is Our God: The Essential Collection\n\n\n198004\n999998\n✝️\n10\nCaleb and Kelsey\n10,000 Reasons / What a Beautiful Name\n178189\n10,000 Reasons / What a Beautiful Name\n\n\n\n\n\n198005 rows × 7 columns"
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html#variable-description",
    "href": "posts/spotify/spotify_willie_nelson.html#variable-description",
    "title": "Spotify Data Frame",
    "section": "Variable Description",
    "text": "Variable Description\n\npid: playlist ID; unique ID for playlist\nplaylist_name: a name of playlist\npos: a position of the track within a playlist (starting from 0)\nartist_name: name of the track’s primary artist\ntrack_name: name of the track\nduration_ms: duration of the track in milliseconds\nalbum_name: name of the track’s album"
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html#favorite-artist",
    "href": "posts/spotify/spotify_willie_nelson.html#favorite-artist",
    "title": "Spotify Data Frame",
    "section": "Favorite Artist",
    "text": "Favorite Artist\nMy favorite artist in the Spotify data frame is Willie Nelson.\nAll of Willie Nelson’s songs in the Spotify data frame and their details are displayed below:\n\n(\n    spotify\n    .set_index('artist_name')\n    .loc['Willie Nelson']\n)\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n9\nold country\n0\nHighwayman\n182973\nNashville Rebel\n\n\nWillie Nelson\n9\nold country\n1\nHighwayman\n182973\nNashville Rebel\n\n\nWillie Nelson\n83\nFlorida\n66\nAlways On My Mind\n212666\nAlways On My Mind\n\n\nWillie Nelson\n83\nFlorida\n67\nLuckenback Texas\n94520\nCountry Outlaws\n\n\nWillie Nelson\n90\nFor the Road\n67\nHighwayman\n182973\nNashville Rebel\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nWillie Nelson\n999873\ndads\n2\nI Gotta Get Drunk\n129759\nWalking the Line\n\n\nWillie Nelson\n999873\ndads\n6\nPancho and Lefty\n286066\nWalking the Line\n\n\nWillie Nelson\n999873\ndads\n45\nSeven Spanish Angels (With Ray Charles)\n229533\nHalf Nelson\n\n\nWillie Nelson\n999897\nKings\n153\nOn the Road Again\n153106\nGreatest Hits (& Some That Will Be)\n\n\nWillie Nelson\n999936\nRoad Trip\n67\nOn the Road Again\n153106\nGreatest Hits (& Some That Will Be)\n\n\n\n\n\n62 rows × 6 columns"
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html#number-of-songs",
    "href": "posts/spotify/spotify_willie_nelson.html#number-of-songs",
    "title": "Spotify Data Frame",
    "section": "Number of Songs",
    "text": "Number of Songs\n\nwillie_nelson = (\n    spotify\n    .set_index('artist_name')\n    .loc['Willie Nelson']\n)\nlen(willie_nelson)\n\n62\n\n\nThere are 62 Willie Nelson songs in this data frame."
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html#longest-song",
    "href": "posts/spotify/spotify_willie_nelson.html#longest-song",
    "title": "Spotify Data Frame",
    "section": "Longest Song",
    "text": "Longest Song\n\n(\n    willie_nelson\n    .sort_values(['duration_ms'], ascending = False)\n    .head(1)\n)\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n114\nWillie\n13\nSunday Mornin' Comin' Down\n422173\nWillie Nelson Sings Kristofferson\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nThe longest Willie Nelson song in this data set is “Sunday Mornin’ Comin’ Down” at 422173 milliseconds."
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html#shortest-song",
    "href": "posts/spotify/spotify_willie_nelson.html#shortest-song",
    "title": "Spotify Data Frame",
    "section": "Shortest Song",
    "text": "Shortest Song\n\n(\n    willie_nelson\n    .sort_values(['duration_ms'], ascending = True)\n    .head(1)\n)\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n83\nFlorida\n67\nLuckenback Texas\n94520\nCountry Outlaws\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nThe shortest Willie Nelson song is “Luckenback Texas” at 94520 milliseconds."
  },
  {
    "objectID": "posts/spotify/spotify_willie_nelson.html#songs-between-150000-and-200000-ms-in-the-willie-playlist",
    "href": "posts/spotify/spotify_willie_nelson.html#songs-between-150000-and-200000-ms-in-the-willie-playlist",
    "title": "Spotify Data Frame",
    "section": "Songs between 150000 and 200000 ms in the “Willie” playlist",
    "text": "Songs between 150000 and 200000 ms in the “Willie” playlist\nBelow are all songs in the “Willie” playlist that are greater than 150000 ms but less than 200000 ms:\n\ngreater_than_15 = willie_nelson['duration_ms'] &gt; 150000\nless_than_20 = willie_nelson['duration_ms'] &lt; 200000\nwillie_playlist = willie_nelson['playlist_name'] == 'Willie'\nwillie_nelson[greater_than_15 & less_than_20 & willie_playlist]\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n114\nWillie\n1\nLittle House on the Hill\n182080\nGod's Problem Child\n\n\nWillie Nelson\n114\nWillie\n3\nNothing I Can Do About It Now\n199160\nA Horse Called Music\n\n\nWillie Nelson\n114\nWillie\n8\nNight Life\n197813\nFor the Good Times: A Tribute to Ray Price\n\n\nWillie Nelson\n114\nWillie\n10\nIf You Can Touch Her At All\n183866\nFunny How Time Slips Away - The Best Of\n\n\nWillie Nelson\n114\nWillie\n11\nStay All Night (Stay A Little Longer)\n154160\nShotgun Willie\n\n\nWillie Nelson\n114\nWillie\n12\nSad Songs And Waltzes\n185213\nShotgun Willie\n\n\nWillie Nelson\n114\nWillie\n19\nFly Me To The Moon\n169960\nAmerican Classic\n\n\nWillie Nelson\n114\nWillie\n21\nMona Lisa\n151826\nSomewhere over the Rainbow\n\n\nWillie Nelson\n114\nWillie\n22\nTexas On A Saturday Night (With Mel Tillis)\n159533\nHalf Nelson\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nwillie_songs = willie_nelson[greater_than_15 & less_than_20 & willie_playlist]\nlen(willie_songs)\n\n9\n\n\nThere are 9 songs in the “Willie” playlist within this range."
  },
  {
    "objectID": "posts/PySpark Basics/pyspark-basics.html",
    "href": "posts/PySpark Basics/pyspark-basics.html",
    "title": "PySpark Basics",
    "section": "",
    "text": "Apache Hadoop is an open source software framework. It is used for storing and processing large data sets. It has two components: Hadoop Distributed File System (HDFS) and MapReduce. HDFS is used for distributed data storage and MapReduce is used as the data processing model. Hadoop allows distributed processing of large data sets across computer clusters. First, large data sets are split into smaller blocks and are stored across multiple servers in the cluster. Then, they are processed with MapReduce."
  },
  {
    "objectID": "posts/PySpark Basics/pyspark-basics.html#sparksession-entry-point",
    "href": "posts/PySpark Basics/pyspark-basics.html#sparksession-entry-point",
    "title": "PySpark Basics",
    "section": "SparkSession Entry Point",
    "text": "SparkSession Entry Point\nThe entry point provides the functionality for data transformation.\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "posts/PySpark Basics/pyspark-basics.html#reading-a-web-csv-file-into-the-spark-framework",
    "href": "posts/PySpark Basics/pyspark-basics.html#reading-a-web-csv-file-into-the-spark-framework",
    "title": "PySpark Basics",
    "section": "Reading a Web CSV File into the Spark Framework",
    "text": "Reading a Web CSV File into the Spark Framework\nWe can convert the Pandas DataFrame to a Spark DataFrame.\nimport pandas as pd\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\ndf_pd = pd.read_csv('https://bcdanl.github.io/data/nba.csv')\ndf = spark.createDataFrame(df_pd)"
  },
  {
    "objectID": "posts/PySpark Basics/pyspark-basics.html#pyspark-basics",
    "href": "posts/PySpark Basics/pyspark-basics.html#pyspark-basics",
    "title": "PySpark Basics",
    "section": "PySpark Basics",
    "text": "PySpark Basics\n\nGetting a Summary\n\nprintSchema()\ndescribe()\n\n\n\nSelecting and Reordering\n\nselect()\n\n\n\nCounting Values\n\ngroupBy().count()\ncountDistinct()\ncount()\n\n\n\nSorting\n\norderBy()\n\n\n\nAdding a New Variable\n\nwithColumn()\n\n\n\nRenaming a Variable\n\nwithColumnRenamed()\n\n\n\nConverting Data Types\n\ncast()\n\n\n\nFiltering Observations\n\nfilter()\n\n\n\nDealing with Missing Values and Duplicates\n\nna.drop()\nna.fill()\ndropDuplicates()"
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html",
    "href": "posts/hw2-icecream/hw2_part2.html",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "",
    "text": "In this post, we’ll take a look at a dataset containing information about household Ben & Jerry’s ice cream purchases. First, we’ll analyze the data using descriptive statistics, filtering, and group operations. After that, we’ll be able to build a simple linear regression model to predict the price per serving of Ben & Jerry’s ice cream based on different household characteristics."
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#introduction",
    "href": "posts/hw2-icecream/hw2_part2.html#introduction",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "",
    "text": "In this post, we’ll take a look at a dataset containing information about household Ben & Jerry’s ice cream purchases. First, we’ll analyze the data using descriptive statistics, filtering, and group operations. After that, we’ll be able to build a simple linear regression model to predict the price per serving of Ben & Jerry’s ice cream based on different household characteristics."
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#loading-packages-udfs-anddataframe",
    "href": "posts/hw2-icecream/hw2_part2.html#loading-packages-udfs-anddataframe",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "Loading Packages, UDFS, andDataFrame",
    "text": "Loading Packages, UDFS, andDataFrame\nBefore we get started, we must load the packages and UDFs necessary for our analysis and Linear Regression.\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row. The table includes separate columns\n    for the 95% confidence interval lower and upper bounds for each coefficient (computed at the 5% significance level)\n    and an \"Observations\" row (using model.summary.numInstances) above the R² row.\n    The RMSE row is placed as the last row.\n\n    The columns are ordered as:\n        Metric | Value | Significance | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    For the \"Value\", \"Std. Error\", \"95% CI Lower\", and \"95% CI Upper\" columns, commas are inserted every three digits,\n    with 3 decimal places (except for Observations which is formatted as an integer with commas).\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    # t_stats = coeffs / std_errors\n    t_stats = model.summary.tValues\n\n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n\n    # Compute the t-critical value for a 95% confidence interval (two-tailed, 5% significance)\n    t_critical = stats.t.ppf(0.975, df)\n\n    # Compute two-tailed p-values for each feature coefficient\n    # p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    p_values = model.summary.pValues\n\n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build the table rows.\n    # Order: Metric, Value, Significance, Std. Error, p-value, 95% CI Lower, 95% CI Upper.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n        table.append([\n            \"Beta: \" + feature,       # Metric name\n            beta,                     # Beta estimate (Value)\n            significance_stars(p),    # Significance stars\n            se,                       # Standard error\n            p,                        # p-value\n            ci_lower,                 # 95% CI lower bound\n            ci_upper                  # 95% CI upper bound\n        ])\n\n    # Compute and add the intercept row with its SE, p-value, significance, and CI (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n\n    table.append([\n        \"Intercept\",\n        model.intercept,\n        intercept_sig,\n        intercept_se,\n        intercept_p,\n        ci_intercept_lower,\n        ci_intercept_upper\n    ])\n\n    # Append overall model metrics:\n    # Insert an Observations row using model.summary.numInstances,\n    # then an R² row, and finally the RMSE row as the last row.\n    table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table.\n    # For the \"Value\" (index 1), \"Std. Error\" (index 3), \"95% CI Lower\" (index 5), and \"95% CI Upper\" (index 6) columns,\n    # format with commas and 3 decimal places, except for Observations which should be an integer with commas.\n    # For the p-value (index 4), format to 3 decimal places.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                # Format Observations as integer with commas, no decimals.\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if i in [1, 3, 5, 6]:\n                    formatted_row.append(f\"{item:,.3f}\")\n                elif i == 4:\n                    formatted_row.append(f\"{item:.3f}\")\n                else:\n                    formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Generate the table string using tabulate.\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n    )\n\n    # Insert a dashed line after the Intercept row for clarity.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n\nice_cream = pd.read_csv('https://bcdanl.github.io/data/ben-and-jerry-cleaned.csv')\n\n\nice_cream\n\n\n  \n    \n\n\n\n\n\n\npriceper1\nflavor_descr\nsize1_descr\nhousehold_id\nhousehold_income\nhousehold_size\nusecoup\ncouponper1\nregion\nmarried\nrace\nhispanic_origin\nmicrowave\ndishwasher\nsfh\ninternet\ntvcable\n\n\n\n\n0\n3.41\nCAKE BATTER\n16.0 MLOZ\n2001456\n130000\n2\nTrue\n0.5\nCentral\nFalse\nwhite\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n1\n3.50\nVAN CARAMEL FUDGE\n16.0 MLOZ\n2001456\n130000\n2\nFalse\n0.0\nCentral\nFalse\nwhite\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n2\n3.50\nVAN CARAMEL FUDGE\n16.0 MLOZ\n2001456\n130000\n2\nFalse\n0.0\nCentral\nFalse\nwhite\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n3\n3.00\nW-N-C-P-C\n16.0 MLOZ\n2001637\n70000\n1\nFalse\n0.0\nWest\nFalse\nwhite\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\n\n\n4\n3.99\nAMERICONE DREAM\n16.0 MLOZ\n2002791\n130000\n3\nFalse\n0.0\nSouth\nTrue\nwhite\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21969\n3.34\nDUBLIN MUDSLIDE\n16.0 MLOZ\n9171249\n80000\n4\nFalse\n0.0\nSouth\nTrue\nwhite\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n21970\n1.99\nPHISH FOOD\n16.0 MLOZ\n9171249\n80000\n4\nFalse\n0.0\nSouth\nTrue\nwhite\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n21971\n4.99\nVAN\n16.0 MLOZ\n9176214\n80000\n1\nFalse\n0.0\nEast\nFalse\nwhite\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n21972\n3.50\nVAN\n16.0 MLOZ\n9176214\n80000\n1\nFalse\n0.0\nEast\nFalse\nwhite\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n21973\n3.50\nVAN\n16.0 MLOZ\n9176214\n80000\n1\nFalse\n0.0\nEast\nFalse\nwhite\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n\n\n\n21974 rows × 17 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nice_cream['tvcable'] = ice_cream['tvcable'].astype(bool)\n\n\ndf = spark.createDataFrame(ice_cream)"
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#descriptive-statistics",
    "href": "posts/hw2-icecream/hw2_part2.html#descriptive-statistics",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\ndf.describe().show()\n\n+-------+------------------+---------------+-----------+--------------------+------------------+------------------+-------------------+-------+-----+\n|summary|         priceper1|   flavor_descr|size1_descr|        household_id|  household_income|    household_size|         couponper1| region| race|\n+-------+------------------+---------------+-----------+--------------------+------------------+------------------+-------------------+-------+-----+\n|  count|             21974|          21974|      21974|               21974|             21974|             21974|              21974|  21974|21974|\n|   mean| 3.314627108010865|           NULL|       NULL|1.6612005039000638E7|125290.79821607354|2.4564030217529806|0.12557882800885908|   NULL| NULL|\n| stddev|0.6656263892402016|           NULL|       NULL|1.1685954458195915E7| 57188.36322324326|1.3368209496554313| 0.5178886507790131|   NULL| NULL|\n|    min|               0.0|AMERICONE DREAM|  16.0 MLOZ|             2000358|             40000|                 1|                0.0|Central|asian|\n|    max|              9.48|  WHITE RUSSIAN|  32.0 MLOZ|            30440689|            310000|                 9|               8.98|   West|white|\n+-------+------------------+---------------+-----------+--------------------+------------------+------------------+-------------------+-------+-----+\n\n\n\nThe descriptive statistics give us a summary of the central tendencies and variation for the continuous variables in the dataset. This gives us an idea of how much Ben & Jerry’s ice cream normally costs and the characteristics of each household. For example, this helps us to understand the range of houshold income and household size."
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#counting-and-filtering",
    "href": "posts/hw2-icecream/hw2_part2.html#counting-and-filtering",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "Counting and Filtering",
    "text": "Counting and Filtering\nNext, let’s perform some basic counting and filtering operations on the variables we’ll be working with.\n\ncoupon_usage = ice_cream['usecoup'].value_counts()\nprint(coupon_usage)\n\nusecoup\nFalse    19629\nTrue      2345\nName: count, dtype: int64\n\n\nThe above counting operation shows us how many households used a coupon on their ice cream purchase. In this case, 2,345 households used a coupon.\n\ncoupon_used = ice_cream[ice_cream['usecoup'] == True]\navg_price_coupon_used = coupon_used['priceper1'].mean()\nprint(avg_price_coupon_used)\n\n3.3771737739872068\n\n\nFor households that used a coupon, the average price of the ice cream they purchased was $3.38.\n\ncoupon_not_used = ice_cream[ice_cream['usecoup'] == False]\navg_price_coupon_not_used = coupon_not_used['priceper1'].mean()\nprint(avg_price_coupon_not_used)\n\n3.307154902003595\n\n\nFor households that did not use a coupon, the average price of the ice cream they purchased was $3.31."
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#group-operations",
    "href": "posts/hw2-icecream/hw2_part2.html#group-operations",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "Group Operations",
    "text": "Group Operations\nWe can use group operations to analyze how different variables are related to each other. For example, we can look at how the price of ice cream changes across different types of households. Let’s take a look at how price of ice cream changes based on geography.\n\nprice_by_region = ice_cream.groupby('region')['priceper1'].mean()\nprint(price_by_region)\n\nregion\nCentral    3.277892\nEast       3.422322\nSouth      3.258359\nWest       3.325974\nName: priceper1, dtype: float64\n\n\nOn average, Ben & Jerry’s ice cream is the least expensive in the South region and most expensive in the East region.\nSimilarly, we can group the data by household income to analyze how price changes based on income.\n\nprice_by_income = ice_cream.groupby('household_income')['priceper1'].mean()\nprint(price_by_income)\n\nhousehold_income\n40000     3.343187\n50000     3.344026\n60000     3.456909\n70000     3.402600\n80000     3.318014\n110000    3.306350\n130000    3.325455\n150000    3.318129\n160000    3.266847\n170000    3.214524\n180000    3.263123\n190000    3.257739\n210000    3.316303\n230000    3.223304\n240000    3.324874\n260000    3.205166\n280000    3.445214\n300000    3.135817\n310000    3.205312\nName: priceper1, dtype: float64\n\n\nFinally, we can group the data by household size. This may give us some insights into whether or not household size impacts how much the household spends on ice cream.\n\nprice_by_size = ice_cream.groupby('household_size')['priceper1'].mean()\nprint(price_by_size)\n\nhousehold_size\n1    3.362255\n2    3.312518\n3    3.295934\n4    3.273378\n5    3.331149\n6    3.254255\n7    3.248901\n8    3.149375\n9    3.041636\nName: priceper1, dtype: float64\n\n\nOn average, it looks like larger households tend to spend less on ice cream than smaller households."
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#linear-regression",
    "href": "posts/hw2-icecream/hw2_part2.html#linear-regression",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "Linear Regression",
    "text": "Linear Regression\nNext, let’s build a linear regression model using the Ben & Jerry’s dataset to predict the price per serving of ice cream. We will use household income, household size, and coupon usage as predictor variables.\n\ndtrain, dtest = df.randomSplit([.67, .33], seed = 1234)\n\n\nx_cols_1 = ['household_income', 'household_size', 'usecoup']\n\nassembler_1 = VectorAssembler(inputCols=x_cols_1, outputCol=\"predictors\")\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n        LinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"priceper1\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)\n\n\nprint(regression_table(model_1, assembler_1))\n\n+------------------------+--------+------+------------+---------+--------------+--------------+\n| Metric                 |  Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+------------------------+--------+------+------------+---------+--------------+--------------+\n| Beta: household_income | -0.000 | ***  |      0.004 |   0.000 |       -0.008 |        0.008 |\n| Beta: household_size   | -0.028 | ***  |      0.018 |   0.000 |       -0.063 |        0.007 |\n| Beta: usecoup          |  0.064 | ***  |      0.019 |   0.000 |        0.026 |        0.101 |\n| Intercept              |  3.476 | ***  |      0.000 |   0.000 |        3.476 |        3.476 |\n-----------------------------------------------------------------------------------------------\n| Observations           | 14,734 |      |            |         |              |              |\n| R²                     |  0.007 |      |            |         |              |              |\n| RMSE                   |  0.666 |      |            |         |              |              |\n+------------------------+--------+------+------------+---------+--------------+--------------+\n\n\nThis model helps us analyze how household income, household size, and coupon usage affect the price of Ben & Jerry’s ice cream. The coefficients, or beta values, show us to what extent each feature predicts or influences the price.\nThe coefficient for household income is 0. This means that an increase or decrease in household income does not affect the price of ice cream. The coefficient for household size is -0.028. This means that for every additional person in the household, the ice cream price per serving decreases by approximately 0.03. Perhaps larger households tend to buy larger amounts of ice cream that have a lower unit price. The coefficient for coupon usage is 0.064. This shows that if a coupon was used, the price per serving of ice cream increases by approximately 0.06. This does not make much sense because coupons usually reduce the price of the item purchased. A possible explanation for this is that households could be using coupons on more expensive, specialty ice cream products. Coupon usage seems to have the most impact on price per serving because it has the largest coefficient value."
  },
  {
    "objectID": "posts/hw2-icecream/hw2_part2.html#conclusion",
    "href": "posts/hw2-icecream/hw2_part2.html#conclusion",
    "title": "HW 2 - Ben & Jerry’s Ice Cream",
    "section": "Conclusion",
    "text": "Conclusion\nBy analyzing this dataset, we were able to show how different household characteristics, such as household income, household size, and coupon usage, affect and predict the price per serving of Ben & Jerry’s ice cream. We also applied a linear regression model, which helped us further understand the relationship between these household characteristics and ice cream price. This information could be useful in marketing, as Ben & Jerry’s may want to tailor promotions to different income levels, household sizes, or other household characteristics."
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html",
    "href": "posts/Homes DataFrame/hw4_pt_3.html",
    "title": "HW 4 - Homes DataFrame",
    "section": "",
    "text": "In this post, we will be considering the homes DataFrame from the 2004 American Housing Survey. This contains data on home values, demographics, schools, income, finance, mortgages, sales, neighborhood characteristics, noise, smells, state geography, and urban classification. We will be applying Linear Regression and Logistic Regression models in order to analyze how different factors impact or predict home value."
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#loading-packages-and-settings",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#loading-packages-and-settings",
    "title": "HW 4 - Homes DataFrame",
    "section": "Loading Packages and Settings",
    "text": "Loading Packages and Settings\n\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#udfs",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#udfs",
    "title": "HW 4 - Homes DataFrame",
    "section": "UDFs",
    "text": "UDFs\nWe will be using a variety of User Defined Functions (UDFs) in order to perform functions necessary for our Logistic Regression Models. We will define our UDFs before we begin.\n\nmarginal_effects\n\ndef marginal_effects(model, means):\n    \"\"\"\n    Compute marginal effects for all predictors in a PySpark GeneralizedLinearRegression model (logit)\n    and return a formatted table with statistical significance and standard errors.\n\n    Parameters:\n        model: Fitted GeneralizedLinearRegression model (with binomial family and logit link).\n        means: List of mean values for the predictor variables.\n\n    Returns:\n        - A formatted string containing the marginal effects table.\n        - A Pandas DataFrame with marginal effects, standard errors, confidence intervals, and significance stars.\n    \"\"\"\n    global assembler_predictors  # Use the global assembler_predictors list\n\n    # Extract model coefficients, standard errors, and intercept\n    coeffs = np.array(model.coefficients)\n    std_errors = np.array(model.summary.coefficientStandardErrors)\n    intercept = model.intercept\n\n    # Compute linear combination of means and coefficients (XB)\n    XB = np.dot(means, coeffs) + intercept\n\n    # Compute derivative of logistic function (G'(XB))\n    G_prime_XB = np.exp(XB) / ((1 + np.exp(XB)) ** 2)\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Create lists to store results\n    results = []\n    df_results = []  # For Pandas DataFrame\n\n    for i, predictor in enumerate(assembler_predictors):\n        # Compute marginal effect\n        marginal_effect = G_prime_XB * coeffs[i]\n\n        # Compute standard error of the marginal effect\n        std_error = G_prime_XB * std_errors[i]\n\n        # Compute z-score and p-value\n        z_score = marginal_effect / std_error if std_error != 0 else np.nan\n        p_value = 2 * (1 - norm.cdf(abs(z_score))) if not np.isnan(z_score) else np.nan\n\n        # Compute confidence interval (95%)\n        ci_lower = marginal_effect - 1.96 * std_error\n        ci_upper = marginal_effect + 1.96 * std_error\n\n        # Append results for table formatting\n        results.append([\n            predictor,\n            f\"{marginal_effect: .4f}\",\n            significance_stars(p_value),\n            f\"{std_error: .4f}\",\n            f\"{ci_lower: .4f}\",\n            f\"{ci_upper: .4f}\"\n        ])\n\n        # Append results for Pandas DataFrame\n        df_results.append({\n            \"Variable\": predictor,\n            \"Marginal Effect\": marginal_effect,\n            \"Significance\": significance_stars(p_value),\n            \"Std. Error\": std_error,\n            \"95% CI Lower\": ci_lower,\n            \"95% CI Upper\": ci_upper\n        })\n\n    # Convert results to formatted table\n    table_str = tabulate(results, headers=[\"Variable\", \"Marginal Effect\", \"Significance\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"],\n                         tablefmt=\"pretty\", colalign=(\"left\", \"decimal\", \"left\", \"decimal\", \"decimal\", \"decimal\"))\n\n    # Convert results to Pandas DataFrame\n    df_results = pd.DataFrame(df_results)\n\n    return table_str, df_results\n\n# Example usage:\n# means = [0.5, 30]  # Mean values for x1 and x2\n# assembler_predictors = ['x1', 'x2']  # Define globally before calling the function\n# table_output, df_output = marginal_effects(fitted_model, means)\n# print(table_output)\n# display(df_output)\n\n\n\nregression_table\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler.\n\n    If the model’s labelCol (retrieved using getLabelCol()) starts with \"log\", an extra column showing np.exp(coeff)\n    is added immediately after the beta estimate column for predictor rows. Additionally, np.exp() of the 95% CI\n    Lower and Upper bounds is also added unless the predictor's name includes \"log_\". The Intercept row does not\n    include exponentiated values.\n\n    When labelCol starts with \"log\", the columns are ordered as:\n        y: [label] | Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper)\n\n    Otherwise, the columns are:\n        y: [label] | Beta | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute and a labelCol).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Determine if we should display exponential values for coefficients.\n    is_log = model.getLabelCol().lower().startswith(\"log\")\n\n    # Extract coefficients and standard errors as NumPy arrays.\n    coeffs = model.coefficients.toArray()\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element).\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Use provided tValues and pValues.\n    df = model.summary.numInstances - len(coeffs) - 1\n    t_critical = stats.t.ppf(0.975, df)\n    p_values = model.summary.pValues\n\n    # Helper: significance stars.\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build table rows for each feature.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n\n        # Check if predictor contains \"log_\" to determine if exponentiation should be applied\n        apply_exp = is_log and \"log_\" not in feature.lower()\n\n        exp_beta = np.exp(beta) if apply_exp else \"\"\n        exp_ci_lower = np.exp(ci_lower) if apply_exp else \"\"\n        exp_ci_upper = np.exp(ci_upper) if apply_exp else \"\"\n\n        if is_log:\n            table.append([\n                feature,            # Predictor name\n                beta,               # Beta estimate\n                exp_beta,           # Exponential of beta (or blank)\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper,\n                exp_ci_lower,       # Exponential of 95% CI lower bound\n                exp_ci_upper        # Exponential of 95% CI upper bound\n            ])\n        else:\n            table.append([\n                feature,\n                beta,\n                significance_stars(p),\n                se,\n                p,\n                ci_lower,\n                ci_upper\n            ])\n\n    # Process intercept.\n    if intercept_se is not None:\n        intercept_p = model.summary.pValues[0] if model.summary.pValues is not None else None\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n        intercept_se = \"\"\n\n    if is_log:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            \"\",                    # Removed np.exp(model.intercept)\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            \"\",\n            ci_intercept_upper,\n            \"\"\n        ])\n    else:\n        table.append([\n            \"Intercept\",\n            model.intercept,\n            intercept_sig,\n            intercept_se,\n            \"\",\n            ci_intercept_lower,\n            ci_intercept_upper\n        ])\n\n    # Append overall model metrics.\n    if is_log:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"])\n    else:\n        table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n        table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table rows.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            # Format Observations as integer with commas.\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if is_log:\n                    # When is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Exp(Beta), 3: Sig, 4: Std. Error, 5: p-value,\n                    # 6: 95% CI Lower, 7: 95% CI Upper, 8: Exp(95% CI Lower), 9: Exp(95% CI Upper).\n                    if i in [1, 2, 4, 6, 7, 8, 9]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 5:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n                else:\n                    # When not is_log, the columns are:\n                    # 0: Metric, 1: Beta, 2: Sig, 3: Std. Error, 4: p-value, 5: 95% CI Lower, 6: 95% CI Upper.\n                    if i in [1, 3, 5, 6]:\n                        formatted_row.append(f\"{item:,.3f}\")\n                    elif i == 4:\n                        formatted_row.append(f\"{item:.3f}\")\n                    else:\n                        formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Set header and column alignment based on whether label starts with \"log\"\n    if is_log:\n        headers = [\n            f\"y: {model.getLabelCol()}\",\n            \"Beta\", \"Exp(Beta)\", \"Sig.\", \"Std. Error\", \"p-value\",\n            \"95% CI Lower\", \"95% CI Upper\", \"Exp(95% CI Lower)\", \"Exp(95% CI Upper)\"\n        ]\n        colalign = (\"left\", \"right\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\", \"right\", \"right\")\n    else:\n        headers = [f\"y: {model.getLabelCol()}\", \"Beta\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"]\n        colalign = (\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n\n    table_str = tabulate(\n        formatted_table,\n        headers=headers,\n        tablefmt=\"pretty\",\n        colalign=colalign\n    )\n\n    # Insert a dashed line after the Intercept row.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n# Example usage:\n# print(regression_table(model_1, assembler_1))\n\n\n\nadd_dummy_variables\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\n\nadd_interaction_terms\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])\n\n\n\ncompare_reg_models\n\ndef compare_reg_models(models, assemblers, names=None):\n    \"\"\"\n    Produces a single formatted table comparing multiple regression models.\n\n    For each predictor (the union across models, ordered by first appearance), the table shows\n    the beta estimate (with significance stars) from each model (blank if not used).\n    For a predictor, if a model's outcome (model.getLabelCol()) starts with \"log\", the cell displays\n    both the beta and its exponential (separated by \" / \"), except when the predictor's name includes \"log_\".\n    (The intercept row does not display exp(.))\n\n    Additional rows for Intercept, Observations, R², and RMSE are appended.\n\n    The header's first column is labeled \"Predictor\", and subsequent columns are\n    \"y: [outcome] ([name])\" for each model.\n\n    The table is produced in grid format (with vertical lines). A dashed line (using '-' characters)\n    is inserted at the top, immediately after the header, and at the bottom.\n    Additionally, immediately after the Intercept row, the border line is replaced with one using '='\n    (to appear as, for example, \"+==============================================+==========================+...\").\n\n    Parameters:\n        models (list): List of fitted LinearRegression models.\n        assemblers (list): List of corresponding VectorAssembler objects.\n        names (list, optional): List of model names; defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing the combined regression table.\n    \"\"\"\n    # Default model names.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(models))]\n\n    # For each model, get outcome and determine if that model is log-transformed.\n    outcomes = [m.getLabelCol() for m in models]\n    is_log_flags = [out.lower().startswith(\"log\") for out in outcomes]\n\n    # Build an ordered union of predictors based on first appearance.\n    ordered_predictors = []\n    for assembler in assemblers:\n        for feat in assembler.getInputCols():\n            if feat not in ordered_predictors:\n                ordered_predictors.append(feat)\n\n    # Helper for significance stars.\n    def significance_stars(p):\n        if p is None:\n            return \"\"\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build rows for each predictor.\n    rows = []\n    for feat in ordered_predictors:\n        row = [feat]\n        for m, a, is_log in zip(models, assemblers, is_log_flags):\n            feats_model = a.getInputCols()\n            if feat in feats_model:\n                idx = feats_model.index(feat)\n                beta = m.coefficients.toArray()[idx]\n                p_val = m.summary.pValues[idx] if m.summary.pValues is not None else None\n                stars = significance_stars(p_val)\n                cell = f\"{beta:.3f}{stars}\"\n                # Only add exp(beta) if model is log and predictor name does NOT include \"log_\"\n                if is_log and (\"log_\" not in feat.lower()):\n                    cell += f\" / {np.exp(beta):,.3f}\"\n                row.append(cell)\n            else:\n                row.append(\"\")\n        rows.append(row)\n\n    # Build intercept row (do NOT compute exp(intercept)).\n    intercept_row = [\"Intercept\"]\n    for m in models:\n        std_all = np.array(m.summary.coefficientStandardErrors)\n        coeffs = m.coefficients.toArray()\n        if len(std_all) == len(coeffs) + 1:\n            intercept_p = m.summary.pValues[0] if m.summary.pValues is not None else None\n        else:\n            intercept_p = None\n        sig = significance_stars(intercept_p)\n        cell = f\"{m.intercept:.3f}{sig}\"\n        intercept_row.append(cell)\n    rows.append(intercept_row)\n\n    # Add Observations row.\n    obs_row = [\"Observations\"]\n    for m in models:\n        obs = m.summary.numInstances\n        obs_row.append(f\"{int(obs):,}\")\n    rows.append(obs_row)\n\n    # Add R² row.\n    r2_row = [\"R²\"]\n    for m in models:\n        r2_row.append(f\"{m.summary.r2:.3f}\")\n    rows.append(r2_row)\n\n    # Add RMSE row.\n    rmse_row = [\"RMSE\"]\n    for m in models:\n        rmse_row.append(f\"{m.summary.rootMeanSquaredError:.3f}\")\n    rows.append(rmse_row)\n\n    # Build header: first column \"Predictor\", then for each model: \"y: [outcome] ([name])\"\n    header = [\"Predictor\"]\n    for out, name in zip(outcomes, names):\n        header.append(f\"y: {out} ({name})\")\n\n    # Create table string using grid format.\n    table_str = tabulate(rows, headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(models))\n\n    # Split into lines.\n    lines = table_str.split(\"\\n\")\n\n    # Create a dashed line spanning the full width.\n    full_width = len(lines[0])\n    dash_line = '-' * full_width\n    # Create an equals line by replacing '-' with '='.\n    eq_line = dash_line.replace('-', '=')\n\n    # Insert a dashed line after the header row.\n    lines = table_str.split(\"\\n\")\n    # In grid format, header and separator are usually the first two lines.\n    lines.insert(2, dash_line)\n\n    # Insert an equals line after the Intercept row.\n    for i, line in enumerate(lines):\n        if line.startswith(\"|\") and \"Intercept\" in line:\n            if i+1 &lt; len(lines):\n                lines[i+1] = eq_line\n            break\n\n    # Add dashed lines at the very top and bottom.\n    final_table = dash_line + \"\\n\" + \"\\n\".join(lines) + \"\\n\" + dash_line\n\n    return final_table\n\n# Example usage:\n# print(compare_reg_models([model_1, model_2, model_3],\n#                          [assembler_1, assembler_2, assembler_3],\n#                          [\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\ncompare_rmse\n\ndef compare_rmse(test_dfs, label_col, pred_col=\"prediction\", names=None):\n    \"\"\"\n    Computes and compares RMSE values for a list of test DataFrames.\n\n    For each DataFrame in test_dfs, this function calculates the RMSE between the actual outcome\n    (given by label_col) and the predicted value (given by pred_col, default \"prediction\"). It then\n    produces a formatted table where the first column header is empty and the first row's first cell is\n    \"RMSE\", with each model's RMSE in its own column.\n\n    Parameters:\n        test_dfs (list): List of test DataFrames.\n        label_col (str): The name of the outcome column.\n        pred_col (str, optional): The name of the prediction column (default \"prediction\").\n        names (list, optional): List of model names corresponding to the test DataFrames.\n                                Defaults to \"Model 1\", \"Model 2\", etc.\n\n    Returns:\n        A formatted string containing a table that compares RMSE values for each test DataFrame,\n        with one model per column.\n    \"\"\"\n    # Set default model names if none provided.\n    if names is None:\n        names = [f\"Model {i+1}\" for i in range(len(test_dfs))]\n\n    rmse_values = []\n    for df in test_dfs:\n        # Create a column for squared error.\n        df = df.withColumn(\"error_sq\", pow(col(label_col) - col(pred_col), 2))\n        # Calculate RMSE: square root of the mean squared error.\n        rmse = df.agg(sqrt(avg(\"error_sq\")).alias(\"rmse\")).collect()[0][\"rmse\"]\n        rmse_values.append(rmse)\n\n    # Build a single row table: first cell \"RMSE\", then one cell per model with the RMSE value.\n    row = [\"RMSE\"] + [f\"{rmse:.3f}\" for rmse in rmse_values]\n\n    # Build header: first column header is empty, then model names.\n    header = [\"\"] + names\n\n    table_str = tabulate([row], headers=header, tablefmt=\"grid\", colalign=(\"left\",) + (\"right\",)*len(names))\n    return table_str\n\n# Example usage:\n# print(compare_rmse([dtest_1, dtest_2, dtest_3], \"log_sales\", names=[\"Model 1\", \"Model 2\", \"Model 3\"]))\n\n\n\nresidual_plot\n\ndef residual_plot(df, label_col, model_name):\n    \"\"\"\n    Generates a residual plot for a given test dataframe.\n\n    Parameters:\n        df (DataFrame): Spark DataFrame containing the test set with predictions.\n        label_col (str): The column name of the actual outcome variable.\n        title (str): The title for the residual plot.\n\n    Returns:\n        None (displays the plot)\n    \"\"\"\n    # Convert to Pandas DataFrame\n    df_pd = df.select([\"prediction\", label_col]).toPandas()\n    df_pd[\"residual\"] = df_pd[label_col] - df_pd[\"prediction\"]\n\n    # Scatter plot of residuals vs. predicted values\n    plt.scatter(df_pd[\"prediction\"], df_pd[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n    # Use LOWESS smoothing for trend line\n    smoothed = sm.nonparametric.lowess(df_pd[\"residual\"], df_pd[\"prediction\"])\n    plt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\n\n    # Add reference line at y=0\n    plt.axhline(y=0, color=\"red\", linestyle=\"--\")\n\n    # Labels and title (model_name)\n    plt.xlabel(\"Predicted Values\")\n    plt.ylabel(\"Residuals\")\n    model_name = \"Residual Plot for \" + model_name\n    plt.title(model_name)\n\n    # Show plot\n    plt.show()\n\n# Example usage:\n# residual_plot(dtest_1, \"log_sales\", \"Model 1\")"
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#loading-dataframe---homes",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#loading-dataframe---homes",
    "title": "HW 4 - Homes DataFrame",
    "section": "Loading DataFrame - Homes",
    "text": "Loading DataFrame - Homes\n\nhomes = pd.read_csv(\n  'https://bcdanl.github.io/data/american_housing_survey.csv'\n)\n\nhomes.head()\n\nWarning: Total number of columns (29) exceeds max_columns (20). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\nLPRICE\nVALUE\nSTATE\nMETRO\nZINC2\nHHGRAD\nBATHS\nBEDRMS\nPER\nZADULT\n...\nEABAN\nHOWH\nHOWN\nODORA\nSTRNA\nAMMORT\nINTW\nMATBUY\nDWNPAY\nFRSTHO\n\n\n\n\n0\n85000\n150000\nGA\nrural\n15600\nNo HS\n2\n3\n1\n1\n...\n0\ngood\ngood\n0\n0\n50000\n9\n1\nother\n0\n\n\n1\n76500\n130000\nGA\nrural\n61001\nHS Grad\n2\n3\n5\n2\n...\n0\ngood\nbad\n0\n1\n70000\n5\n1\nother\n1\n\n\n2\n93900\n135000\nGA\nrural\n38700\nHS Grad\n2\n3\n4\n2\n...\n0\ngood\ngood\n0\n0\n117000\n6\n0\nother\n1\n\n\n3\n100000\n140000\nGA\nrural\n80000\nNo HS\n3\n4\n2\n2\n...\n0\ngood\ngood\n0\n1\n100000\n7\n1\nprev home\n0\n\n\n4\n100000\n135000\nGA\nrural\n61000\nHS Grad\n2\n3\n2\n2\n...\n0\ngood\ngood\n0\n0\n100000\n4\n1\nother\n1\n\n\n\n\n\n5 rows × 29 columns"
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#exploratory-data-analysis",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#exploratory-data-analysis",
    "title": "HW 4 - Homes DataFrame",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore applying our Logistic Regression Model, let’s explore some of the relationships in this DataFrame.\n\nHome Value vs Household Income\n\nhomes['log_income'] = np.log(homes['ZINC2'])\nhomes['log_value'] = np.log(homes['VALUE'])\n\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n\n\n\nsns.scatterplot(x=homes['log_income'], y=homes['log_value'], alpha=0.3)\n\n&lt;Axes: xlabel='log_income', ylabel='log_value'&gt;\n\n\n\n\n\nThis scatterplot shows the relationship between the log of household income and the log of home values. I took the log of each variable to reduce skewness and make it easier to see patterns in the scatterplot. Here, we can see that a higher household income is generally associated with a higher house value.\n\n\nNumber of Bedrooms vs House Value\n\nsns.boxplot(x=homes['BEDRMS'], y=homes['VALUE'])\n\n&lt;Axes: xlabel='BEDRMS', ylabel='VALUE'&gt;\n\n\n\n\n\nThese boxplots show the distribution of house values for different numbers of bedrooms. Houses with 6 and 8 bedrooms have the widest range in house value, whereas houses with only 1 bedroom have the least amount of variation in value. Houses with more bedrooms may have greater variation in value because they could range from older, lower value properties to newer, high value estates. Overall, as more bedrooms are added, home values tend to increase but with greater variability.\n\n\nJunk in the Streets vs House Value\n\nsns.boxplot(x=homes['EJUNK'], y=homes['VALUE'])\n\n&lt;Axes: xlabel='EJUNK', ylabel='VALUE'&gt;\n\n\n\n\n\nThese boxplots show the distribution of house values for the amount of junk in the streets. Houses without junk in the streets have a slightly wider range of values than houses with junk in the streets. Additionally, houses with junk in the streets tend to be slightly less expensive than houses without junk in the streets."
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#linear-regression",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#linear-regression",
    "title": "HW 4 - Homes DataFrame",
    "section": "Linear Regression",
    "text": "Linear Regression\nNext, we will build a Linear Regression Model to predict home values based on several variables and analyze the accurary of its predictions. For the predictor variables, we will use all of the variables in the DataFrame, excluding home value, the amount of the first mortgage payment, and the purchase price of the unit and land. The outcome variable will be the log value of the home.\n\nLog Transformation\n\ndf = spark.createDataFrame(homes)\ndf = (\n    df\n    .withColumn(\"LOG_VALUE\",\n                log(df['VALUE']) )\n)\n\n\n\nTraining-Test Split\n\ndtrain, dtest = df.randomSplit([0.6, 0.4], seed = 1234)\n\n\n\nAssigning Dummy Variables\nBefore we begin training our models, we must add dummy variables for our categorical variables. To do this, we will apply our UDF for adding dummy variables. All observations must be numerical in order to train the Linear Regression Model.\n\ndummy_cols_STATE, ref_category_STATE = add_dummy_variables('STATE', 0)\ndummy_cols_METRO, ref_category_METRO = add_dummy_variables('METRO', 0)\ndummy_cols_HHGRAD, ref_category_HHGRAD = add_dummy_variables('HHGRAD', 1)\ndummy_cols_HOWH, ref_category_HOWH = add_dummy_variables('HOWH', 0)\ndummy_cols_HOWN, ref_category_HOWN = add_dummy_variables('HOWN', 0)\ndummy_cols_DWNPAY, ref_category_DWNPAY = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Bach\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other\n\n\n\n\nLinear Regression (Model 1)\n\n# assembling predictors\nconti_cols = ['ZINC2', 'BATHS', 'BEDRMS', 'PER', 'ZADULT', 'NUNITS', 'EAPTBL', 'ECOM1', 'ECOM2', 'EGREEN', 'EJUNK', 'ELOW1',\n              'ESFD', 'ETRANS', 'EABAN', 'ODORA', 'STRNA', 'INTW', 'MATBUY', 'FRSTHO']\n\ndummy_cols = dummy_cols_STATE + dummy_cols_METRO + dummy_cols_HHGRAD + dummy_cols_HOWH + dummy_cols_HOWN + dummy_cols_DWNPAY\n\nassembler_predictors_1 = (\n    conti_cols +\n    dummy_cols\n)\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors_1,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"LOG_VALUE\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtest_1 = model_1.transform(dtest_1)\n\n# makting regression table\nprint( regression_table(model_1, assembler_1) )\n\n+------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| y: LOG_VALUE     |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper) |\n+------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| ZINC2            |  0.000 |     1.000 | ***  |      0.014 |   0.000 |       -0.028 |        0.028 |             0.972 |             1.029 |\n| BATHS            |  0.211 |     1.235 | ***  |      0.013 |   0.000 |        0.186 |        0.236 |             1.205 |             1.266 |\n| BEDRMS           |  0.089 |     1.093 | ***  |      0.008 |   0.000 |        0.074 |        0.105 |             1.077 |             1.110 |\n| PER              |  0.005 |     1.005 |      |      0.014 |   0.533 |       -0.022 |        0.032 |             0.978 |             1.032 |\n| ZADULT           | -0.022 |     0.978 |      |      0.001 |   0.106 |       -0.023 |       -0.021 |             0.977 |             0.979 |\n| NUNITS           | -0.001 |     0.999 |  **  |      0.030 |   0.016 |       -0.060 |        0.057 |             0.942 |             1.058 |\n| EAPTBL           | -0.020 |     0.980 |      |      0.024 |   0.491 |       -0.068 |        0.027 |             0.934 |             1.028 |\n| ECOM1            | -0.006 |     0.994 |      |      0.062 |   0.818 |       -0.126 |        0.115 |             0.881 |             1.122 |\n| ECOM2            | -0.067 |     0.935 |      |      0.018 |   0.274 |       -0.102 |       -0.033 |             0.903 |             0.968 |\n| EGREEN           |  0.006 |     1.006 |      |      0.065 |   0.717 |       -0.121 |        0.134 |             0.886 |             1.143 |\n| EJUNK            | -0.215 |     0.807 | ***  |      0.029 |   0.001 |       -0.272 |       -0.158 |             0.762 |             0.854 |\n| ELOW1            |  0.022 |     1.022 |      |      0.037 |   0.445 |       -0.050 |        0.094 |             0.951 |             1.099 |\n| ESFD             |  0.279 |     1.322 | ***  |      0.033 |   0.000 |        0.215 |        0.343 |             1.240 |             1.409 |\n| ETRANS           | -0.075 |     0.928 |  **  |      0.046 |   0.022 |       -0.165 |        0.015 |             0.848 |             1.015 |\n| EABAN            | -0.178 |     0.837 | ***  |      0.041 |   0.000 |       -0.260 |       -0.097 |             0.771 |             0.907 |\n| ODORA            | -0.010 |     0.990 |      |      0.020 |   0.811 |       -0.049 |        0.030 |             0.952 |             1.030 |\n| STRNA            | -0.028 |     0.972 |      |      0.006 |   0.162 |       -0.039 |       -0.017 |             0.962 |             0.983 |\n| INTW             | -0.048 |     0.953 | ***  |      0.017 |   0.000 |       -0.082 |       -0.014 |             0.922 |             0.986 |\n| MATBUY           | -0.017 |     0.983 |      |      0.022 |   0.319 |       -0.060 |        0.025 |             0.942 |             1.026 |\n| FRSTHO           | -0.083 |     0.920 | ***  |      0.037 |   0.000 |       -0.155 |       -0.011 |             0.856 |             0.989 |\n| STATE_CO         | -0.294 |     0.745 | ***  |      0.039 |   0.000 |       -0.371 |       -0.218 |             0.690 |             0.804 |\n| STATE_CT         | -0.327 |     0.721 | ***  |      0.039 |   0.000 |       -0.403 |       -0.251 |             0.668 |             0.778 |\n| STATE_GA         | -0.615 |     0.540 | ***  |      0.071 |   0.000 |       -0.754 |       -0.477 |             0.470 |             0.621 |\n| STATE_IL         | -0.862 |     0.422 | ***  |      0.038 |   0.000 |       -0.937 |       -0.787 |             0.392 |             0.455 |\n| STATE_IN         | -0.736 |     0.479 | ***  |      0.046 |   0.000 |       -0.828 |       -0.645 |             0.437 |             0.524 |\n| STATE_LA         | -0.712 |     0.490 | ***  |      0.043 |   0.000 |       -0.796 |       -0.629 |             0.451 |             0.533 |\n| STATE_MO         | -0.645 |     0.525 | ***  |      0.041 |   0.000 |       -0.725 |       -0.565 |             0.484 |             0.569 |\n| STATE_OH         | -0.636 |     0.529 | ***  |      0.041 |   0.000 |       -0.717 |       -0.556 |             0.488 |             0.573 |\n| STATE_OK         | -0.996 |     0.369 | ***  |      0.043 |   0.000 |       -1.080 |       -0.913 |             0.340 |             0.401 |\n| STATE_PA         | -0.891 |     0.410 | ***  |      0.043 |   0.000 |       -0.975 |       -0.806 |             0.377 |             0.447 |\n| STATE_TX         | -1.045 |     0.352 | ***  |      0.039 |   0.000 |       -1.120 |       -0.969 |             0.326 |             0.379 |\n| STATE_WA         | -0.089 |     0.915 |  **  |      0.023 |   0.022 |       -0.133 |       -0.044 |             0.875 |             0.957 |\n| METRO_urban      |  0.092 |     1.096 | ***  |      0.029 |   0.000 |        0.034 |        0.149 |             1.035 |             1.160 |\n| HHGRAD_Assoc     | -0.153 |     0.858 | ***  |      0.026 |   0.000 |       -0.205 |       -0.102 |             0.815 |             0.903 |\n| HHGRAD_Grad      |  0.079 |     1.083 | ***  |      0.021 |   0.003 |        0.038 |        0.121 |             1.039 |             1.128 |\n| HHGRAD_HS_Grad   | -0.178 |     0.837 | ***  |      0.037 |   0.000 |       -0.251 |       -0.106 |             0.778 |             0.899 |\n| HHGRAD_No_HS     | -0.321 |     0.725 | ***  |      0.033 |   0.000 |       -0.386 |       -0.257 |             0.680 |             0.774 |\n| HOWH_good        |  0.180 |     1.197 | ***  |      0.027 |   0.000 |        0.126 |        0.233 |             1.134 |             1.263 |\n| HOWN_good        |  0.111 |     1.118 | ***  |      0.022 |   0.000 |        0.067 |        0.155 |             1.070 |             1.168 |\n| DWNPAY_prev_home |  0.133 |     1.142 | ***  |      0.076 |   0.000 |       -0.017 |        0.283 |             0.983 |             1.326 |\n| Intercept        | 11.708 |           | ***  |      0.000 |         |       11.708 |              |            11.708 |                   |\n---------------------------------------------------------------------------------------------------------------------------------------------\n| Observations     |  9,351 |           |      |            |         |              |              |                   |                   |\n| R²               |  0.318 |           |      |            |         |              |              |                   |                   |\n| RMSE             |  0.793 |           |      |            |         |              |              |                   |                   |\n+------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n\n\nEach predictor variable’s statistical significance is rated by stars under the column “Sig.”. Predictors that are not statisically significant do not have any stars. We can see in the regression table that not all of the predictor variables are statistically significant. In our next Linear Regression Model, we will only include variables that are statistically significant. We can then compare the two models and decide whether or not removing non-statistically significant variables impacts the accuracy of the model and functionality of the model.\n\n\nNew Linear Regression (Model 2)\n\n# assembling predictors\nconti_cols_2 = ['ZINC2', 'BATHS', 'BEDRMS', 'NUNITS', 'EJUNK',\n              'ESFD', 'ETRANS', 'EABAN', 'INTW', 'FRSTHO']\n\ndummy_cols = dummy_cols_STATE + dummy_cols_METRO + dummy_cols_HHGRAD + dummy_cols_HOWH + dummy_cols_HOWN + dummy_cols_DWNPAY\n\nassembler_predictors_2 = (\n    conti_cols_2 +\n    dummy_cols\n)\n\nassembler_2 = VectorAssembler(\n    inputCols = assembler_predictors_2,\n    outputCol = \"predictors\"\n)\n\ndtrain_2 = assembler_2.transform(dtrain)\ndtest_2  = assembler_2.transform(dtest)\n\n# training model\nmodel_2 = (\n    LinearRegression(featuresCol=\"predictors\",\n                     labelCol=\"LOG_VALUE\")\n    .fit(dtrain_2)\n)\n\n# making prediction\ndtest_2 = model_2.transform(dtest_2)\n\n# makting regression table\nprint( regression_table(model_2, assembler_2) )\n\n+------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| y: LOG_VALUE     |   Beta | Exp(Beta) | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper | Exp(95% CI Lower) | Exp(95% CI Upper) |\n+------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n| ZINC2            |  0.000 |     1.000 | ***  |      0.014 |   0.000 |       -0.028 |        0.028 |             0.972 |             1.028 |\n| BATHS            |  0.213 |     1.238 | ***  |      0.012 |   0.000 |        0.190 |        0.236 |             1.210 |             1.267 |\n| BEDRMS           |  0.087 |     1.091 | ***  |      0.001 |   0.000 |        0.086 |        0.088 |             1.089 |             1.092 |\n| NUNITS           | -0.002 |     0.998 |  **  |      0.065 |   0.011 |       -0.129 |        0.126 |             0.879 |             1.134 |\n| EJUNK            | -0.219 |     0.803 | ***  |      0.036 |   0.001 |       -0.290 |       -0.148 |             0.748 |             0.862 |\n| ESFD             |  0.273 |     1.315 | ***  |      0.032 |   0.000 |        0.211 |        0.336 |             1.235 |             1.399 |\n| ETRANS           | -0.088 |     0.916 | ***  |      0.045 |   0.006 |       -0.177 |        0.001 |             0.838 |             1.001 |\n| EABAN            | -0.188 |     0.829 | ***  |      0.005 |   0.000 |       -0.198 |       -0.177 |             0.820 |             0.838 |\n| INTW             | -0.049 |     0.952 | ***  |      0.022 |   0.000 |       -0.092 |       -0.007 |             0.912 |             0.993 |\n| FRSTHO           | -0.084 |     0.919 | ***  |      0.037 |   0.000 |       -0.156 |       -0.013 |             0.856 |             0.988 |\n| STATE_CO         | -0.290 |     0.748 | ***  |      0.039 |   0.000 |       -0.366 |       -0.213 |             0.693 |             0.808 |\n| STATE_CT         | -0.328 |     0.720 | ***  |      0.038 |   0.000 |       -0.403 |       -0.253 |             0.668 |             0.777 |\n| STATE_GA         | -0.615 |     0.541 | ***  |      0.071 |   0.000 |       -0.754 |       -0.477 |             0.471 |             0.621 |\n| STATE_IL         | -0.867 |     0.420 | ***  |      0.038 |   0.000 |       -0.942 |       -0.793 |             0.390 |             0.453 |\n| STATE_IN         | -0.736 |     0.479 | ***  |      0.046 |   0.000 |       -0.826 |       -0.645 |             0.438 |             0.525 |\n| STATE_LA         | -0.716 |     0.489 | ***  |      0.043 |   0.000 |       -0.799 |       -0.633 |             0.450 |             0.531 |\n| STATE_MO         | -0.645 |     0.525 | ***  |      0.041 |   0.000 |       -0.724 |       -0.565 |             0.485 |             0.568 |\n| STATE_OH         | -0.640 |     0.527 | ***  |      0.041 |   0.000 |       -0.720 |       -0.561 |             0.487 |             0.571 |\n| STATE_OK         | -0.997 |     0.369 | ***  |      0.042 |   0.000 |       -1.081 |       -0.914 |             0.339 |             0.401 |\n| STATE_PA         | -0.892 |     0.410 | ***  |      0.043 |   0.000 |       -0.976 |       -0.808 |             0.377 |             0.446 |\n| STATE_TX         | -1.047 |     0.351 | ***  |      0.039 |   0.000 |       -1.122 |       -0.971 |             0.326 |             0.379 |\n| STATE_WA         | -0.089 |     0.915 |  **  |      0.022 |   0.021 |       -0.133 |       -0.045 |             0.876 |             0.956 |\n| METRO_urban      |  0.087 |     1.090 | ***  |      0.029 |   0.000 |        0.030 |        0.144 |             1.030 |             1.154 |\n| HHGRAD_Assoc     | -0.157 |     0.855 | ***  |      0.026 |   0.000 |       -0.208 |       -0.105 |             0.812 |             0.900 |\n| HHGRAD_Grad      |  0.080 |     1.083 | ***  |      0.021 |   0.003 |        0.039 |        0.121 |             1.039 |             1.128 |\n| HHGRAD_HS_Grad   | -0.183 |     0.833 | ***  |      0.037 |   0.000 |       -0.255 |       -0.111 |             0.775 |             0.895 |\n| HHGRAD_No_HS     | -0.329 |     0.719 | ***  |      0.033 |   0.000 |       -0.394 |       -0.265 |             0.675 |             0.767 |\n| HOWH_good        |  0.180 |     1.197 | ***  |      0.027 |   0.000 |        0.127 |        0.233 |             1.136 |             1.262 |\n| HOWN_good        |  0.119 |     1.126 | ***  |      0.022 |   0.000 |        0.075 |        0.163 |             1.078 |             1.177 |\n| DWNPAY_prev_home |  0.133 |     1.143 | ***  |      0.073 |   0.000 |       -0.009 |        0.276 |             0.991 |             1.318 |\n| Intercept        | 11.677 |           | ***  |      0.000 |         |       11.677 |              |            11.677 |                   |\n---------------------------------------------------------------------------------------------------------------------------------------------\n| Observations     |  9,351 |           |      |            |         |              |              |                   |                   |\n| R²               |  0.318 |           |      |            |         |              |              |                   |                   |\n| RMSE             |  0.794 |           |      |            |         |              |              |                   |                   |\n+------------------+--------+-----------+------+------------+---------+--------------+--------------+-------------------+-------------------+\n\n\n\n\nModel Comparison\nWe can compare the models by looking for differences in their beta, \\(R^2\\), and RMSE values, as well as their residual plots.\n\nBeta and \\(R^2\\) values\n\nprint(compare_reg_models([model_1, model_2],\n                        [assembler_1, assembler_2],\n                        [\"Model 1\", \"Model 2\"]))\n\n--------------------------------------------------------------------------\n+------------------+--------------------------+--------------------------+\n| Predictor        |   y: LOG_VALUE (Model 1) |   y: LOG_VALUE (Model 2) |\n--------------------------------------------------------------------------\n+==================+==========================+==========================+\n| ZINC2            |         0.000*** / 1.000 |         0.000*** / 1.000 |\n+------------------+--------------------------+--------------------------+\n| BATHS            |         0.211*** / 1.235 |         0.213*** / 1.238 |\n+------------------+--------------------------+--------------------------+\n| BEDRMS           |         0.089*** / 1.093 |         0.087*** / 1.091 |\n+------------------+--------------------------+--------------------------+\n| PER              |            0.005 / 1.005 |                          |\n+------------------+--------------------------+--------------------------+\n| ZADULT           |           -0.022 / 0.978 |                          |\n+------------------+--------------------------+--------------------------+\n| NUNITS           |         -0.001** / 0.999 |         -0.002** / 0.998 |\n+------------------+--------------------------+--------------------------+\n| EAPTBL           |           -0.020 / 0.980 |                          |\n+------------------+--------------------------+--------------------------+\n| ECOM1            |           -0.006 / 0.994 |                          |\n+------------------+--------------------------+--------------------------+\n| ECOM2            |           -0.067 / 0.935 |                          |\n+------------------+--------------------------+--------------------------+\n| EGREEN           |            0.006 / 1.006 |                          |\n+------------------+--------------------------+--------------------------+\n| EJUNK            |        -0.215*** / 0.807 |        -0.219*** / 0.803 |\n+------------------+--------------------------+--------------------------+\n| ELOW1            |            0.022 / 1.022 |                          |\n+------------------+--------------------------+--------------------------+\n| ESFD             |         0.279*** / 1.322 |         0.273*** / 1.315 |\n+------------------+--------------------------+--------------------------+\n| ETRANS           |         -0.075** / 0.928 |        -0.088*** / 0.916 |\n+------------------+--------------------------+--------------------------+\n| EABAN            |        -0.178*** / 0.837 |        -0.188*** / 0.829 |\n+------------------+--------------------------+--------------------------+\n| ODORA            |           -0.010 / 0.990 |                          |\n+------------------+--------------------------+--------------------------+\n| STRNA            |           -0.028 / 0.972 |                          |\n+------------------+--------------------------+--------------------------+\n| INTW             |        -0.048*** / 0.953 |        -0.049*** / 0.952 |\n+------------------+--------------------------+--------------------------+\n| MATBUY           |           -0.017 / 0.983 |                          |\n+------------------+--------------------------+--------------------------+\n| FRSTHO           |        -0.083*** / 0.920 |        -0.084*** / 0.919 |\n+------------------+--------------------------+--------------------------+\n| STATE_CO         |        -0.294*** / 0.745 |        -0.290*** / 0.748 |\n+------------------+--------------------------+--------------------------+\n| STATE_CT         |        -0.327*** / 0.721 |        -0.328*** / 0.720 |\n+------------------+--------------------------+--------------------------+\n| STATE_GA         |        -0.615*** / 0.540 |        -0.615*** / 0.541 |\n+------------------+--------------------------+--------------------------+\n| STATE_IL         |        -0.862*** / 0.422 |        -0.867*** / 0.420 |\n+------------------+--------------------------+--------------------------+\n| STATE_IN         |        -0.736*** / 0.479 |        -0.736*** / 0.479 |\n+------------------+--------------------------+--------------------------+\n| STATE_LA         |        -0.712*** / 0.490 |        -0.716*** / 0.489 |\n+------------------+--------------------------+--------------------------+\n| STATE_MO         |        -0.645*** / 0.525 |        -0.645*** / 0.525 |\n+------------------+--------------------------+--------------------------+\n| STATE_OH         |        -0.636*** / 0.529 |        -0.640*** / 0.527 |\n+------------------+--------------------------+--------------------------+\n| STATE_OK         |        -0.996*** / 0.369 |        -0.997*** / 0.369 |\n+------------------+--------------------------+--------------------------+\n| STATE_PA         |        -0.891*** / 0.410 |        -0.892*** / 0.410 |\n+------------------+--------------------------+--------------------------+\n| STATE_TX         |        -1.045*** / 0.352 |        -1.047*** / 0.351 |\n+------------------+--------------------------+--------------------------+\n| STATE_WA         |         -0.089** / 0.915 |         -0.089** / 0.915 |\n+------------------+--------------------------+--------------------------+\n| METRO_urban      |         0.092*** / 1.096 |         0.087*** / 1.090 |\n+------------------+--------------------------+--------------------------+\n| HHGRAD_Assoc     |        -0.153*** / 0.858 |        -0.157*** / 0.855 |\n+------------------+--------------------------+--------------------------+\n| HHGRAD_Grad      |         0.079*** / 1.083 |         0.080*** / 1.083 |\n+------------------+--------------------------+--------------------------+\n| HHGRAD_HS_Grad   |        -0.178*** / 0.837 |        -0.183*** / 0.833 |\n+------------------+--------------------------+--------------------------+\n| HHGRAD_No_HS     |        -0.321*** / 0.725 |        -0.329*** / 0.719 |\n+------------------+--------------------------+--------------------------+\n| HOWH_good        |         0.180*** / 1.197 |         0.180*** / 1.197 |\n+------------------+--------------------------+--------------------------+\n| HOWN_good        |         0.111*** / 1.118 |         0.119*** / 1.126 |\n+------------------+--------------------------+--------------------------+\n| DWNPAY_prev_home |         0.133*** / 1.142 |         0.133*** / 1.143 |\n+------------------+--------------------------+--------------------------+\n| Intercept        |                11.708*** |                11.677*** |\n==========================================================================\n| Observations     |                    9,351 |                    9,351 |\n+------------------+--------------------------+--------------------------+\n| R²               |                    0.318 |                    0.318 |\n+------------------+--------------------------+--------------------------+\n| RMSE             |                    0.793 |                    0.794 |\n+------------------+--------------------------+--------------------------+\n--------------------------------------------------------------------------\n\n\nThe two models generally have similar or the same beta values for each predictor. They also have the same \\(R^2\\) values. This suggests that one of the models may not be more accurate in predicting house value than the other and that the two are functionally equivalent.\n\n\nRMSE\n\nprint(compare_rmse([dtest_1, dtest_2], \"LOG_VALUE\"))\n\n+------+-----------+-----------+\n|      |   Model 1 |   Model 2 |\n+======+===========+===========+\n| RMSE |     0.851 |     0.852 |\n+------+-----------+-----------+\n\n\nThe RMSE values for the two models are relatively the same, with Model 1 having a slightly lower value. This suggests that Model 1 may be a slightly better fit for the data.\n\n\nResidual Plots\n\nresidual_plot(dtest_1, \"LOG_VALUE\", \"Model 1\")\n\n\n\n\n\nresidual_plot(dtest_2, \"LOG_VALUE\", \"Model 2\")\n\n\n\n\nThe residual plots for Model 1 and Model 2 look relatively the same. This shows us that the models are able to predict the home value with the same or similar accuracy.\nOverall, the residual plots, beta values, \\(R^2\\) values, and RMSE values were relatively the same for both models. This proves that the variables removed in Model 2 were in fact not statistically significant. Their removal did not have a major impact on the Linear Regression model."
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#logistic-regression",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#logistic-regression",
    "title": "HW 4 - Homes DataFrame",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nNext, we will build a Logistic Regression Model using the same DataFrame, homes. However, this time we will have the model predict whether the buyer made a down payment of 20% or more. This outcome variable will be a new variable named “GT20DOWN”. Our predictor variables will once again be all available variables, excluding the amount of the first mortgage and the purchase price of the unit and land.\n\nOutcome Variable\n\ndf = df.withColumn(\"GT20DOWN\", when((col(\"LPRICE\") - col(\"AMMORT\")) / col(\"LPRICE\") &gt; 0, 1).otherwise(0))\n\n\n\nSplit into Training/Testing\n\ndtrain, dtest = df.randomSplit([0.6, 0.4], seed=1234)\n\n\n\nDummy Variables\n\ndummy_cols_STATE, ref_category_STATE = add_dummy_variables('STATE', 0)\ndummy_cols_METRO, ref_category_METRO = add_dummy_variables('METRO', 0)\ndummy_cols_HHGRAD, ref_category_HHGRAD = add_dummy_variables('HHGRAD', 1)\ndummy_cols_HOWH, ref_category_HOWH = add_dummy_variables('HOWH', 0)\ndummy_cols_HOWN, ref_category_HOWN = add_dummy_variables('HOWN', 0)\ndummy_cols_DWNPAY, ref_category_DWNPAY = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Bach\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other\n\n\n\n\n# Keep the name assembler_predictors unchanged,\n#   as it will be used as a global variable in the marginal_effects UDF.\nassembler_predictors = (\n    conti_cols_2 +\n    dummy_cols\n)\n\nassembler_3 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_3 = assembler_3.transform(dtrain)\ndtest_3  = assembler_3.transform(dtest)\n\n\n# training the model\nmodel_3 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"GT20DOWN\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_3)\n)\n\n\ndtrain_3 = model_3.transform(dtrain_3)\ndtest_3 = model_3.transform(dtest_3)\n\n\nmodel_3.summary\n\nCoefficients:\n         Feature Estimate Std Error  T Value P Value\n     (Intercept)   0.7348    0.1998   3.6781  0.0002\n           ZINC2   0.0000    0.0000   0.8766  0.3807\n           BATHS   0.2872    0.0407   7.0522  0.0000\n          BEDRMS  -0.0784    0.0323  -2.4263  0.0153\n          NUNITS   0.0129    0.0061   2.1077  0.0351\n           EJUNK  -0.2353    0.1723  -1.3658  0.1720\n            ESFD  -0.2221    0.0997  -2.2287  0.0258\n          ETRANS  -0.1349    0.0844  -1.5981  0.1100\n           EABAN  -0.3862    0.1205  -3.2045  0.0014\n            INTW  -0.0686    0.0148  -4.6394  0.0000\n          FRSTHO  -0.3151    0.0568  -5.5470  0.0000\n        STATE_CO  -0.1912    0.0984  -1.9438  0.0519\n        STATE_CT   0.7224    0.1092   6.6139  0.0000\n        STATE_GA   0.0132    0.1038   0.1277  0.8984\n        STATE_IL   0.3506    0.1939   1.8087  0.0705\n        STATE_IN   0.2030    0.1031   1.9687  0.0490\n        STATE_LA   0.4599    0.1255   3.6651  0.0002\n        STATE_MO   0.2093    0.1154   1.8130  0.0698\n        STATE_OH   0.5330    0.1115   4.7800  0.0000\n        STATE_OK   0.0986    0.1092   0.9034  0.3663\n        STATE_PA   0.2630    0.1146   2.2937  0.0218\n        STATE_TX   0.1307    0.1147   1.1400  0.2543\n        STATE_WA   0.1071    0.1041   1.0287  0.3036\n     METRO_urban   0.0673    0.0603   1.1174  0.2638\n    HHGRAD_Assoc  -0.6806    0.0783  -8.6891  0.0000\n     HHGRAD_Grad  -0.0532    0.0764  -0.6958  0.4865\n  HHGRAD_HS_Grad  -0.6810    0.0579 -11.7598  0.0000\n    HHGRAD_No_HS  -0.8795    0.0977  -9.0009  0.0000\n       HOWH_good   0.0314    0.0871   0.3603  0.7186\n       HOWN_good   0.2052    0.0715   2.8701  0.0041\nDWNPAY_prev_home   0.6042    0.0620   9.7496  0.0000\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 12511.4865 on 9320 degrees of freedom\nResidual deviance: 11489.7938 on 9320 degrees of freedom\nAIC: 11551.7938\n\n\n\n\nMarginal Effects\nWe can use the marginal effect of a predictor variable to analyze the relationship or association between the predictor and outcome variable. In this case, we will interpret the association between first-time homeownership and the probability of making a 20%+ down payment. Additionally, we will be able to interpret the association between number of bedrooms in the home and the probability of making a 20%+ down payment.\n\n# Compute means\nmeans_df = dtrain_3.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\n\ntable_output, df_ME = marginal_effects(model_3, means_list)\nprint(table_output)\n\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable         | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| ZINC2            |          0.0000 |              |     0.0000 |      -0.0000 |       0.0000 |\n| BATHS            |          0.0674 | ***          |     0.0096 |       0.0487 |       0.0861 |\n| BEDRMS           |         -0.0184 | **           |     0.0076 |      -0.0333 |      -0.0035 |\n| NUNITS           |          0.0030 | **           |     0.0014 |       0.0002 |       0.0058 |\n| EJUNK            |         -0.0552 |              |     0.0404 |      -0.1345 |       0.0240 |\n| ESFD             |         -0.0521 | **           |     0.0234 |      -0.0980 |      -0.0063 |\n| ETRANS           |         -0.0316 |              |     0.0198 |      -0.0705 |       0.0072 |\n| EABAN            |         -0.0906 | ***          |     0.0283 |      -0.1461 |      -0.0352 |\n| INTW             |         -0.0161 | ***          |     0.0035 |      -0.0229 |      -0.0093 |\n| FRSTHO           |         -0.0739 | ***          |     0.0133 |      -0.1001 |      -0.0478 |\n| STATE_CO         |         -0.0449 | *            |     0.0231 |      -0.0901 |       0.0004 |\n| STATE_CT         |          0.1695 | ***          |     0.0256 |       0.1193 |       0.2198 |\n| STATE_GA         |          0.0031 |              |     0.0244 |      -0.0446 |       0.0508 |\n| STATE_IL         |          0.0823 | *            |     0.0455 |      -0.0069 |       0.1714 |\n| STATE_IN         |          0.0476 | **           |     0.0242 |       0.0002 |       0.0951 |\n| STATE_LA         |          0.1079 | ***          |     0.0294 |       0.0502 |       0.1656 |\n| STATE_MO         |          0.0491 | *            |     0.0271 |      -0.0040 |       0.1022 |\n| STATE_OH         |          0.1251 | ***          |     0.0262 |       0.0738 |       0.1764 |\n| STATE_OK         |          0.0231 |              |     0.0256 |      -0.0271 |       0.0734 |\n| STATE_PA         |          0.0617 | **           |     0.0269 |       0.0090 |       0.1144 |\n| STATE_TX         |          0.0307 |              |     0.0269 |      -0.0221 |       0.0834 |\n| STATE_WA         |          0.0251 |              |     0.0244 |      -0.0228 |       0.0730 |\n| METRO_urban      |          0.0158 |              |     0.0141 |      -0.0119 |       0.0435 |\n| HHGRAD_Assoc     |         -0.1597 | ***          |     0.0184 |      -0.1957 |      -0.1237 |\n| HHGRAD_Grad      |         -0.0125 |              |     0.0179 |      -0.0476 |       0.0227 |\n| HHGRAD_HS_Grad   |         -0.1598 | ***          |     0.0136 |      -0.1864 |      -0.1332 |\n| HHGRAD_No_HS     |         -0.2064 | ***          |     0.0229 |      -0.2513 |      -0.1614 |\n| HOWH_good        |          0.0074 |              |     0.0204 |      -0.0327 |       0.0474 |\n| HOWN_good        |          0.0482 | ***          |     0.0168 |       0.0153 |       0.0810 |\n| DWNPAY_prev_home |          0.1418 | ***          |     0.0145 |       0.1133 |       0.1703 |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n\n\nThe marginal effect of first time homebuyers (FRSTHO) is \\(-0.0739\\). This shows that first time home buyers are 7.39% less likely to make a 20% down payment.\nThe marginal effect of the number of bedrooms (BEDRMS) is \\(-0.0184\\). This shows that an additional bedroom is associated with a 1.84% lower probability of making a 20% down payment."
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#adding-interaction-terms",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#adding-interaction-terms",
    "title": "HW 4 - Homes DataFrame",
    "section": "Adding Interaction Terms",
    "text": "Adding Interaction Terms\nLet’s take a closer look at the relationship between number of bedrooms, first-time homebuyers, and the probability of making a 20%+ down payment. To do this, we will fit a new Logistic Regression Model using all the previously included predictors, plus the interaction between first-time homebuyers, FRSTHO, and number of bedrooms, BEDRMS. This will allow us to interpret how the relationship between the number of bedrooms and the probability of making a 20%+ down payment varies depending on whether the buyer is a first-time homebuyer.\n\nInteraction Between FRSTHO and BEDRMS\n\ninteraction_cols_FRSTHO_BEDRMS = add_interaction_terms(['FRSTHO'], ['BEDRMS'])\n\n\n\nNew Logistic Regression (with Interaction Terms)\n\nassembler_predictors = (\n    conti_cols_2 +\n    dummy_cols +\n    interaction_cols_FRSTHO_BEDRMS\n)\n\nassembler_4 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_4 = assembler_4.transform(dtrain)\ndtest_4  = assembler_4.transform(dtest)\n\n\n# training the model\nmodel_4 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"GT20DOWN\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_4)\n)\n\n\ndtrain_4 = model_4.transform(dtrain_4)\ndtest_4 = model_4.transform(dtest_4)\n\n\nmodel_4.summary\n\nCoefficients:\n         Feature Estimate Std Error  T Value P Value\n     (Intercept)   0.5973    0.2126   2.8099  0.0050\n           ZINC2   0.0000    0.0000   0.8195  0.4125\n           BATHS   0.2829    0.0408   6.9307  0.0000\n          BEDRMS  -0.0322    0.0406  -0.7929  0.4278\n          NUNITS   0.0126    0.0060   2.0798  0.0375\n           EJUNK  -0.2374    0.1721  -1.3793  0.1678\n            ESFD  -0.2240    0.0995  -2.2500  0.0244\n          ETRANS  -0.1363    0.0844  -1.6158  0.1061\n           EABAN  -0.3890    0.1205  -3.2279  0.0012\n            INTW  -0.0684    0.0148  -4.6252  0.0000\n          FRSTHO   0.0198    0.1860   0.1066  0.9151\n        STATE_CO  -0.1957    0.0984  -1.9878  0.0468\n        STATE_CT   0.7175    0.1092   6.5688  0.0000\n        STATE_GA   0.0121    0.1039   0.1167  0.9071\n        STATE_IL   0.3454    0.1938   1.7820  0.0748\n        STATE_IN   0.2013    0.1031   1.9519  0.0509\n        STATE_LA   0.4578    0.1255   3.6480  0.0003\n        STATE_MO   0.2041    0.1155   1.7673  0.0772\n        STATE_OH   0.5293    0.1116   4.7445  0.0000\n        STATE_OK   0.0965    0.1092   0.8835  0.3770\n        STATE_PA   0.2544    0.1147   2.2174  0.0266\n        STATE_TX   0.1285    0.1147   1.1201  0.2627\n        STATE_WA   0.1046    0.1042   1.0044  0.3152\n     METRO_urban   0.0646    0.0603   1.0714  0.2840\n    HHGRAD_Assoc  -0.6796    0.0783  -8.6746  0.0000\n     HHGRAD_Grad  -0.0526    0.0764  -0.6876  0.4917\n  HHGRAD_HS_Grad  -0.6788    0.0579 -11.7180  0.0000\n    HHGRAD_No_HS  -0.8770    0.0977  -8.9786  0.0000\n       HOWH_good   0.0339    0.0871   0.3892  0.6971\n       HOWN_good   0.2036    0.0715   2.8474  0.0044\nDWNPAY_prev_home   0.5951    0.0622   9.5746  0.0000\n FRSTHO_*_BEDRMS  -0.1076    0.0569  -1.8905  0.0587\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 12511.4865 on 9319 degrees of freedom\nResidual deviance: 11486.2166 on 9319 degrees of freedom\nAIC: 11550.2166\n\n\nThe coefficient of the interaction term between first time homebuyers (FRSTHO) and number of bedrooms (BEDRMS) is \\(-0.1076\\). This means that for first time homebuyers, each additional bedroom decreases the chance that the homebuyer will make a 20% down payment by 0.1076 units.\n\n\nMarginal Effects\n\n# Compute means\nmeans_df = dtrain_4.select([mean(col).alias(col) for col in assembler_predictors])\n\n# Collect the results as a list\nmeans = means_df.collect()[0]\nmeans_list = [means[col] for col in assembler_predictors]\n\n\ntable_output, df_ME = marginal_effects(model_4, means_list) # Instead of mean values, some other representative values can also be chosen.\nprint(table_output)\n\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| Variable         | Marginal Effect | Significance | Std. Error | 95% CI Lower | 95% CI Upper |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n| ZINC2            |          0.0000 |              |     0.0000 |      -0.0000 |       0.0000 |\n| BATHS            |          0.0664 | ***          |     0.0096 |       0.0476 |       0.0852 |\n| BEDRMS           |         -0.0076 |              |     0.0095 |      -0.0262 |       0.0111 |\n| NUNITS           |          0.0029 | **           |     0.0014 |       0.0002 |       0.0057 |\n| EJUNK            |         -0.0557 |              |     0.0404 |      -0.1349 |       0.0235 |\n| ESFD             |         -0.0526 | **           |     0.0234 |      -0.0983 |      -0.0068 |\n| ETRANS           |         -0.0320 |              |     0.0198 |      -0.0708 |       0.0068 |\n| EABAN            |         -0.0913 | ***          |     0.0283 |      -0.1467 |      -0.0358 |\n| INTW             |         -0.0160 | ***          |     0.0035 |      -0.0228 |      -0.0092 |\n| FRSTHO           |          0.0047 |              |     0.0436 |      -0.0809 |       0.0902 |\n| STATE_CO         |         -0.0459 | **           |     0.0231 |      -0.0912 |      -0.0006 |\n| STATE_CT         |          0.1684 | ***          |     0.0256 |       0.1181 |       0.2186 |\n| STATE_GA         |          0.0028 |              |     0.0244 |      -0.0449 |       0.0506 |\n| STATE_IL         |          0.0810 | *            |     0.0455 |      -0.0081 |       0.1702 |\n| STATE_IN         |          0.0472 | *            |     0.0242 |      -0.0002 |       0.0947 |\n| STATE_LA         |          0.1074 | ***          |     0.0294 |       0.0497 |       0.1651 |\n| STATE_MO         |          0.0479 | *            |     0.0271 |      -0.0052 |       0.1010 |\n| STATE_OH         |          0.1242 | ***          |     0.0262 |       0.0729 |       0.1755 |\n| STATE_OK         |          0.0226 |              |     0.0256 |      -0.0276 |       0.0729 |\n| STATE_PA         |          0.0597 | **           |     0.0269 |       0.0069 |       0.1125 |\n| STATE_TX         |          0.0301 |              |     0.0269 |      -0.0226 |       0.0829 |\n| STATE_WA         |          0.0245 |              |     0.0244 |      -0.0234 |       0.0724 |\n| METRO_urban      |          0.0151 |              |     0.0141 |      -0.0126 |       0.0429 |\n| HHGRAD_Assoc     |         -0.1595 | ***          |     0.0184 |      -0.1955 |      -0.1234 |\n| HHGRAD_Grad      |         -0.0123 |              |     0.0179 |      -0.0475 |       0.0228 |\n| HHGRAD_HS_Grad   |         -0.1593 | ***          |     0.0136 |      -0.1859 |      -0.1326 |\n| HHGRAD_No_HS     |         -0.2058 | ***          |     0.0229 |      -0.2507 |      -0.1609 |\n| HOWH_good        |          0.0080 |              |     0.0204 |      -0.0321 |       0.0480 |\n| HOWN_good        |          0.0478 | ***          |     0.0168 |       0.0149 |       0.0807 |\n| DWNPAY_prev_home |          0.1396 | ***          |     0.0146 |       0.1110 |       0.1682 |\n| FRSTHO_*_BEDRMS  |         -0.0253 | *            |     0.0134 |      -0.0514 |       0.0009 |\n+------------------+-----------------+--------------+------------+--------------+--------------+\n\n\nThe marginal effect of the interaction term between first time homebuyers (FRSTHO) and number of bedrooms (BEDRMS) is \\(-0.0253\\). This means that for each additional bedroom, first time homebuyers are 2.53% less likely to make a 20% down payment compared to non-first time homebuyers."
  },
  {
    "objectID": "posts/Homes DataFrame/hw4_pt_3.html#two-more-logistic-regression-models",
    "href": "posts/Homes DataFrame/hw4_pt_3.html#two-more-logistic-regression-models",
    "title": "HW 4 - Homes DataFrame",
    "section": "Two More Logistic Regression Models",
    "text": "Two More Logistic Regression Models\nFinally, we will fit two more Logistic Regression Models in order to predict home value for two subsets of home data. One subset and model will feature only homes worth greater than or equal to 175k. The other subset will feature homes worth less than 175k. We can compare the models’ residual deviance, \\(RMSE\\), and classification performance to evaluate whether or not performance changes for different home value levels.\n\nSplit DataFrame\n\ngreater_than_equal_175000 = df[df['VALUE'] &gt;= 175000]\nless_than_175000 = df[df['VALUE'] &lt; 175000]\n\n\n\nHomes worth VALUE ≥ 175k\n\nSplit into Training/Testing\n\ndtrain, dtest = greater_than_equal_175000.randomSplit([0.6, 0.4], seed=1234)\n\n\n\nDummy Variables\n\ndummy_cols_STATE, ref_category_STATE = add_dummy_variables('STATE', 0)\ndummy_cols_METRO, ref_category_METRO = add_dummy_variables('METRO', 0)\ndummy_cols_HHGRAD, ref_category_HHGRAD = add_dummy_variables('HHGRAD', 1)\ndummy_cols_HOWH, ref_category_HOWH = add_dummy_variables('HOWH', 0)\ndummy_cols_HOWN, ref_category_HOWN = add_dummy_variables('HOWN', 0)\ndummy_cols_DWNPAY, ref_category_DWNPAY = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Bach\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other\n\n\n\n\nLogistic Regression Model\n\nassembler_predictors = (\n    conti_cols_2 +\n    dummy_cols\n)\n\nassembler_5 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_5 = assembler_5.transform(dtrain)\ndtest_5  = assembler_5.transform(dtest)\n\n\n# training the model\nmodel_5 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"GT20DOWN\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_5)\n)\n\n\ndtrain_5 = model_5.transform(dtrain_5)\ndtest_5 = model_5.transform(dtest_5)\n\n\nmodel_5.summary\n\nCoefficients:\n         Feature Estimate Std Error T Value P Value\n     (Intercept)   0.6684    0.3251  2.0562  0.0398\n           ZINC2   0.0000    0.0000 -0.0764  0.9391\n           BATHS   0.3301    0.0567  5.8208  0.0000\n          BEDRMS  -0.1040    0.0465 -2.2361  0.0253\n          NUNITS   0.0234    0.0134  1.7432  0.0813\n           EJUNK  -0.5695    0.3093 -1.8415  0.0656\n            ESFD  -0.3985    0.1853 -2.1513  0.0315\n          ETRANS  -0.1742    0.1410 -1.2356  0.2166\n           EABAN  -0.6480    0.2510 -2.5822  0.0098\n            INTW  -0.0567    0.0275 -2.0606  0.0393\n          FRSTHO  -0.3161    0.0888 -3.5608  0.0004\n        STATE_CO  -0.0415    0.1061 -0.3908  0.6959\n        STATE_CT   0.8530    0.1275  6.6922  0.0000\n        STATE_GA   0.2964    0.1367  2.1676  0.0302\n        STATE_IL   0.4020    0.4308  0.9331  0.3508\n        STATE_IN   0.7344    0.1791  4.1011  0.0000\n        STATE_LA   0.7326    0.2326  3.1494  0.0016\n        STATE_MO   0.8763    0.1818  4.8199  0.0000\n        STATE_OH   0.7728    0.1790  4.3168  0.0000\n        STATE_OK   1.0786    0.2655  4.0626  0.0000\n        STATE_PA   0.7274    0.2256  3.2237  0.0013\n        STATE_TX   0.6446    0.2659  2.4241  0.0153\n        STATE_WA   0.2529    0.1120  2.2579  0.0240\n     METRO_urban  -0.0364    0.0961 -0.3785  0.7051\n    HHGRAD_Assoc  -0.6786    0.1142 -5.9400  0.0000\n     HHGRAD_Grad   0.0146    0.0994  0.1471  0.8830\n  HHGRAD_HS_Grad  -0.5142    0.0840 -6.1181  0.0000\n    HHGRAD_No_HS  -0.8903    0.1717 -5.1846  0.0000\n       HOWH_good  -0.1540    0.1552 -0.9925  0.3209\n       HOWN_good   0.4110    0.1202  3.4188  0.0006\nDWNPAY_prev_home   0.5943    0.0853  6.9678  0.0000\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 5908.5931 on 4680 degrees of freedom\nResidual deviance: 5328.8668 on 4680 degrees of freedom\nAIC: 5390.8668\n\n\n\n# Compute confusion matrix\ndtest_5 = dtest_5.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix = dtest_5.groupBy(\"GT20DOWN\", \"predicted_class\").count().orderBy(\"GT20DOWN\", \"predicted_class\")\n\nTP = dtest_5.filter((col(\"GT20DOWN\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_5.filter((col(\"GT20DOWN\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_5.filter((col(\"GT20DOWN\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_5.filter((col(\"GT20DOWN\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |        0   |      960  |\n------------+------------+------------\nActual Pos. |        0   |     2160  |\n------------+------------+------------\nAccuracy:  0.6923\nPrecision: 0.6923\nRecall (Sensitivity): 1.0000\nSpecificity:  0.0000\nAverage Rate: 0.6923\nEnrichment:   1.0000 (Relative Precision)\n\n\n\n\nAUC and ROC\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"GT20DOWN\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_5)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_5.select(\"prediction\", \"GT20DOWN\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"GT20DOWN\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.6741\n\n\n\n\n\n\n\n\nHomes worth VALUE &lt; 175k\n\ndtrain, dtest = less_than_175000.randomSplit([0.6, 0.4], seed=1234)\n\n\ndummy_cols_STATE, ref_category_STATE = add_dummy_variables('STATE', 0)\ndummy_cols_METRO, ref_category_METRO = add_dummy_variables('METRO', 0)\ndummy_cols_HHGRAD, ref_category_HHGRAD = add_dummy_variables('HHGRAD', 1)\ndummy_cols_HOWH, ref_category_HOWH = add_dummy_variables('HOWH', 0)\ndummy_cols_HOWN, ref_category_HOWN = add_dummy_variables('HOWN', 0)\ndummy_cols_DWNPAY, ref_category_DWNPAY = add_dummy_variables('DWNPAY', 0)\n\nReference category (dummy omitted): CA\nReference category (dummy omitted): rural\nReference category (dummy omitted): Bach\nReference category (dummy omitted): bad\nReference category (dummy omitted): bad\nReference category (dummy omitted): other\n\n\n\nassembler_predictors = (\n    conti_cols_2 +\n    dummy_cols\n)\n\nassembler_6 = VectorAssembler(\n    inputCols = assembler_predictors,\n    outputCol = \"predictors\"\n)\n\ndtrain_6 = assembler_6.transform(dtrain)\ndtest_6  = assembler_6.transform(dtest)\n\n\n# training the model\nmodel_6 = (\n    GeneralizedLinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"GT20DOWN\",\n                                family=\"binomial\",\n                                link=\"logit\")\n    .fit(dtrain_6)\n)\n\n\ndtrain_6 = model_6.transform(dtrain_6)\ndtest_6 = model_6.transform(dtest_6)\n\n\nmodel_6.summary\n\nCoefficients:\n         Feature Estimate Std Error T Value P Value\n     (Intercept)   1.0004    0.3609  2.7724  0.0056\n           ZINC2   0.0000    0.0000  2.4733  0.0134\n           BATHS   0.2564    0.0641  4.0021  0.0001\n          BEDRMS  -0.1801    0.0469 -3.8381  0.0001\n          NUNITS   0.0288    0.0117  2.4521  0.0142\n           EJUNK  -0.4302    0.2098 -2.0506  0.0403\n            ESFD  -0.1264    0.1233 -1.0256  0.3051\n          ETRANS  -0.0607    0.1006 -0.6037  0.5461\n           EABAN  -0.2049    0.1320 -1.5520  0.1207\n            INTW  -0.0681    0.0177 -3.8552  0.0001\n          FRSTHO  -0.2598    0.0747 -3.4807  0.0005\n        STATE_CO  -0.1942    0.3192 -0.6083  0.5430\n        STATE_CT   0.5233    0.3094  1.6911  0.0908\n        STATE_GA  -0.4573    0.2993 -1.5280  0.1265\n        STATE_IL   0.2285    0.3448  0.6625  0.5076\n        STATE_IN  -0.1152    0.2925 -0.3939  0.6936\n        STATE_LA   0.1373    0.3037  0.4520  0.6513\n        STATE_MO  -0.2127    0.3007 -0.7074  0.4793\n        STATE_OH   0.3449    0.2978  1.1583  0.2468\n        STATE_OK  -0.1493    0.2936 -0.5086  0.6110\n        STATE_PA   0.0185    0.2978  0.0621  0.9505\n        STATE_TX   0.0126    0.2965  0.0425  0.9661\n        STATE_WA   0.2563    0.3452  0.7425  0.4578\n     METRO_urban  -0.0106    0.0778 -0.1367  0.8913\n    HHGRAD_Assoc  -0.5577    0.1074 -5.1936  0.0000\n     HHGRAD_Grad   0.0209    0.1275  0.1640  0.8697\n  HHGRAD_HS_Grad  -0.5503    0.0821 -6.7068  0.0000\n    HHGRAD_No_HS  -0.7198    0.1241 -5.7995  0.0000\n       HOWH_good   0.1855    0.1070  1.7341  0.0829\n       HOWN_good  -0.0093    0.0888 -0.1051  0.9163\nDWNPAY_prev_home   0.4973    0.0946  5.2586  0.0000\n\n(Dispersion parameter for binomial family taken to be 1.0000)\n    Null deviance: 6427.1456 on 4622 degrees of freedom\nResidual deviance: 6060.5538 on 4622 degrees of freedom\nAIC: 6122.5538\n\n\n\n# Compute confusion matrix\ndtest_6 = dtest_6.withColumn(\"predicted_class\", when(col(\"prediction\") &gt; .02, 1).otherwise(0))\nconf_matrix = dtest_6.groupBy(\"GT20DOWN\", \"predicted_class\").count().orderBy(\"GT20DOWN\", \"predicted_class\")\n\nTP = dtest_6.filter((col(\"GT20DOWN\") == 1) & (col(\"predicted_class\") == 1)).count()\nFP = dtest_6.filter((col(\"GT20DOWN\") == 0) & (col(\"predicted_class\") == 1)).count()\nFN = dtest_6.filter((col(\"GT20DOWN\") == 1) & (col(\"predicted_class\") == 0)).count()\nTN = dtest_6.filter((col(\"GT20DOWN\") == 0) & (col(\"predicted_class\") == 0)).count()\n\naccuracy = (TP + TN) / (TP + FP + FN + TN)\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nspecificity = TN / (TN + FP)\naverage_rate = (TP + FN) / (TP + TN + FP + FN)  # Proportion of actual at-risk babies\nenrichment = precision / average_rate\n\n\n# Print formatted confusion matrix with labels\nprint(\"\\n Confusion Matrix:\\n\")\nprint(\"                     Predicted\")\nprint(\"            |  Negative  |  Positive  \")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Neg. |    {TN:5}   |    {FP:5}  |\")\nprint(\"------------+------------+------------\")\nprint(f\"Actual Pos. |    {FN:5}   |    {TP:5}  |\")\nprint(\"------------+------------+------------\")\n\n\nprint(f\"Accuracy:  {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall (Sensitivity): {recall:.4f}\")\nprint(f\"Specificity:  {specificity:.4f}\")\nprint(f\"Average Rate: {average_rate:.4f}\")\nprint(f\"Enrichment:   {enrichment:.4f} (Relative Precision)\")\n\n\n Confusion Matrix:\n\n                     Predicted\n            |  Negative  |  Positive  \n------------+------------+------------\nActual Neg. |        0   |     1422  |\n------------+------------+------------\nActual Pos. |        0   |     1659  |\n------------+------------+------------\nAccuracy:  0.5385\nPrecision: 0.5385\nRecall (Sensitivity): 1.0000\nSpecificity:  0.0000\nAverage Rate: 0.5385\nEnrichment:   1.0000 (Relative Precision)\n\n\n\nAUC and ROC\n\n# Use probability of the positive class (y=1)\nevaluator = BinaryClassificationEvaluator(labelCol=\"GT20DOWN\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n\n# Evaluate AUC\nauc = evaluator.evaluate(dtest_6)\n\nprint(f\"AUC: {auc:.4f}\")  # Higher is better (closer to 1)\n\n# Convert to Pandas\npdf = dtest_6.select(\"prediction\", \"GT20DOWN\").toPandas()\n\n# Compute ROC curve\nfpr, tpr, _ = roc_curve(pdf[\"GT20DOWN\"], pdf[\"prediction\"])\n\n# Plot ROC curve\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.4f})\")\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nAUC: 0.6519\n\n\n\n\n\n\n\n\nInterpretation\nThe logistic regression model for houses with a value of 175k or greater has a residual deviance of \\(5328.8668\\). The model for houses with a value of less than 175k has a residual deviance of \\(6060.5538\\). This shows that the model for houses with a value of 175k or greater fits the data better because it has a lower residual deviance. In other words, the predictors more accurately predict if the homebuyer makes a 20% down payment for houses with a value of 175k or greater.\nThe model for houses with a value of 175k or higher has an accuracy of \\(0.6923\\), whereas the model for houses with a value of less than 175k has an accuracy of \\(0.5385\\). Once again, the model for higher value houses makes more accurate predictions because of its higher accuracy.\nThe AUC of the model for houses with greater value is \\(0.6741\\). The AUC of the model for lower value houses is \\(0.6519\\). The AUC for the model with higher value houses is slightly closer to 1 than the model with lower value houses. This supports the previous two claims that the model with higher value houses more accurately predicts if the homebuyer makes a 20% down payment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ann Brennan",
    "section": "",
    "text": "Ann Brennan is pursuing a major in Data Analytics and minor in Mathematics at SUNY Geneseo. Ann also competes for the SUNY Geneseo cross country and track and field teams."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ann Brennan",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Data Analytics | Aug 2022 - May 2026  Minor in Mathematics"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ann Brennan",
    "section": "Experience",
    "text": "Experience\nGallagher | Financial & Actuarial Consulting Intern  June 2024 - August 2024"
  },
  {
    "objectID": "danl_proj_nba.html#salary-distribution-among-teams",
    "href": "danl_proj_nba.html#salary-distribution-among-teams",
    "title": "Data Analysis Project",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet’s start with the salary distribution among teams using seaborn for visualization. ​​\n\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n/var/folders/_m/d6jf0jhd2zzdfd5kzdhl_24w0000gn/T/ipykernel_79892/1671011424.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  nba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 16))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams.\nNotice that Portland Trail Blazers has the highest total salary followed by Golden State Warriors and Philadelphia 76ers, and Memphis Grizzlies has the lowest total salary."
  },
  {
    "objectID": "danl_proj_nba.html#player-age-distribution",
    "href": "danl_proj_nba.html#player-age-distribution",
    "title": "Data Analysis Project",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let’s explore the Player Age Distribution across the NBA. We’ll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ​​\n\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\n# nba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\n# nba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n/Users/bchoe/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players.\nNotice that the majority of players fall within an age range from 24 to 34. There are few players whose age is above 40."
  },
  {
    "objectID": "danl_proj_nba.html#position-wise-salary-insights",
    "href": "danl_proj_nba.html#position-wise-salary-insights",
    "title": "Data Analysis Project",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we’ll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let’s create a box plot to visualize the salary distribution for each position. ​​\n\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. PG-SG has the highest median salary."
  },
  {
    "objectID": "danl_proj_nba.html#top-10-highest-paid-players",
    "href": "danl_proj_nba.html#top-10-highest-paid-players",
    "title": "Data Analysis Project",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we’ll identify the Top 10 Highest Paid Players in the NBA. Let’s visualize this information.\n\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'PlayerName',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\nThe bar plot above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "danl-210-quarto-reticulate.html",
    "href": "danl-210-quarto-reticulate.html",
    "title": "Using Python and R Together!",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\nprint('Hello, world!')\n\nHello, world!\n\na = 1\na\n\n1\n\n\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#variables-and-data-types",
    "href": "danl-210-quarto-reticulate.html#variables-and-data-types",
    "title": "Using Python and R Together!",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\nprint('Hello, world!')\n\nHello, world!\n\na = 1\na\n\n1\n\n\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#control-structures",
    "href": "danl-210-quarto-reticulate.html#control-structures",
    "title": "Using Python and R Together!",
    "section": "0.2 Control Structures",
    "text": "0.2 Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\nFive is greater than two!"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#functions",
    "href": "danl-210-quarto-reticulate.html#functions",
    "title": "Using Python and R Together!",
    "section": "0.3 Functions",
    "text": "0.3 Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\nHello from a function"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#lists-and-dictionaries",
    "href": "danl-210-quarto-reticulate.html#lists-and-dictionaries",
    "title": "Using Python and R Together!",
    "section": "0.4 Lists and Dictionaries",
    "text": "0.4 Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#import-python-libraries",
    "href": "danl-210-quarto-reticulate.html#import-python-libraries",
    "title": "Using Python and R Together!",
    "section": "1.1 Import Python libraries",
    "text": "1.1 Import Python libraries\n\nimport pandas as pd\n\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\n\n\n\nCode!\noj\n\n\n         sales  price      brand  ad\n0       8256.0   3.87  tropicana   0\n1       6144.0   3.87  tropicana   0\n2       3840.0   3.87  tropicana   0\n3       8000.0   3.87  tropicana   0\n4       8896.0   3.87  tropicana   0\n...        ...    ...        ...  ..\n28942   2944.0   2.00  dominicks   0\n28943   4928.0   1.94  dominicks   0\n28944  13440.0   1.59  dominicks   0\n28945  55680.0   1.49  dominicks   0\n28946   7040.0   1.75  dominicks   0\n\n[28947 rows x 4 columns]\n\n\n\noj.describe()\n\n               sales         price            ad\ncount   28947.000000  28947.000000  28947.000000\nmean    17312.213356      2.282488      0.237261\nstd     27477.660437      0.648001      0.425411\nmin        64.000000      0.520000      0.000000\n25%      4864.000000      1.790000      0.000000\n50%      8384.000000      2.170000      0.000000\n75%     17408.000000      2.730000      0.000000\nmax    716416.000100      3.870000      1.000000"
  },
  {
    "objectID": "danl-210-quarto-reticulate.html#python-r-interaction",
    "href": "danl-210-quarto-reticulate.html#python-r-interaction",
    "title": "Using Python and R Together!",
    "section": "1.2 Python-R Interaction",
    "text": "1.2 Python-R Interaction\nBelow is using Python’s DataFrame oj to visualize using R’s ggplot\n\nlibrary(tidyverse)\n\n# Access the Python pandas DataFrame\noj &lt;- py$oj\n\n# Plot using ggplot2\nggplot(oj, aes(x = log(sales), y = log(price), \n               color = brand)) +\n  geom_point(alpha = .25) +\n  geom_smooth(method = lm) +\n  theme_minimal()\n\n\n\n\n\n1.2.1 Interactive DataFrame with R’s DT Package\n\n\n\n\n\n\n\nIn *.ipynb on Google Colab, we can use itables or just Google Colab’s default to print DataFrame.\n\n# !pip install itables\nfrom itables import init_notebook_mode, show\ninit_notebook_mode(all_interactive=False)\n\noj = pd.read_csv('https://bcdanl.github.io/data/dominick_oj.csv')\nshow(oj)"
  },
  {
    "objectID": "classwork.2.html#heading-2",
    "href": "classwork.2.html#heading-2",
    "title": "Untitled",
    "section": "Heading 2",
    "text": "Heading 2\n\nHeading 3"
  },
  {
    "objectID": "classwork.2.html#emphasis",
    "href": "classwork.2.html#emphasis",
    "title": "Untitled",
    "section": "Emphasis",
    "text": "Emphasis\nitalic\nitalic\nbold\nbold"
  },
  {
    "objectID": "classwork.2.html#lists",
    "href": "classwork.2.html#lists",
    "title": "Untitled",
    "section": "Lists",
    "text": "Lists\n\nItem 1\nItem 2\n\nSub item 2.1\nSub item 2.2\n\n\n\nFirst Item\nSecond Item"
  },
  {
    "objectID": "classwork.2.html#links-and-images",
    "href": "classwork.2.html#links-and-images",
    "title": "Untitled",
    "section": "Links and Images",
    "text": "Links and Images\nDANL 210"
  },
  {
    "objectID": "classwork.2.html#blockquote",
    "href": "classwork.2.html#blockquote",
    "title": "Untitled",
    "section": "Blockquote",
    "text": "Blockquote\n\nBe yourself. Everyone else is already taken. - Oscar Wilde."
  },
  {
    "objectID": "classwork.2.html#emojis",
    "href": "classwork.2.html#emojis",
    "title": "Untitled",
    "section": "Emojis",
    "text": "Emojis\n😄\n😸"
  },
  {
    "objectID": "classwork.2.html#code-blocks",
    "href": "classwork.2.html#code-blocks",
    "title": "Untitled",
    "section": "Code Blocks",
    "text": "Code Blocks\n\"string\"\nimport numpy as np"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "A. Brennan Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSpotify Data Frame\n\n\n\n\n\n\n\n\n\n1 min\n\n\n\n\n\n\n  \n\n\n\n\nPython Basics\n\n\n\n\n\n\n\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nHW 4 - Homes DataFrame\n\n\n\n\n\n\n\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nHW 3 - Beer Markets\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nHW 2 - Ben & Jerry’s Ice Cream\n\n\n\n\n\n\n\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nPySpark Basics\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nAnn Brennan\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nHomework 5 - NFL\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nAnn Brennan\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nAnn Brennan\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "12.1.html",
    "href": "12.1.html",
    "title": "Untitled",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n# Spread across two tibbles\ntable4a  # cases\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\ntable4a %&gt;% \n  pivot_longer(cols = c(`1999`, `2000`), \n               names_to = \"year\", \n               values_to = \"cases\")\n\n# A tibble: 6 × 3\n  country     year   cases\n  &lt;chr&gt;       &lt;chr&gt;  &lt;dbl&gt;\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766"
  },
  {
    "objectID": "11.29.html",
    "href": "11.29.html",
    "title": "Untitled",
    "section": "",
    "text": "Section"
  },
  {
    "objectID": "basic-python-intro.html",
    "href": "basic-python-intro.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#what-is-python",
    "href": "basic-python-intro.html#what-is-python",
    "title": "Introduction to Python",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "basic-python-intro.html#variables-and-data-types",
    "href": "basic-python-intro.html#variables-and-data-types",
    "title": "Introduction to Python",
    "section": "Variables and Data Types",
    "text": "Variables and Data Types\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "basic-python-intro.html#control-structures",
    "href": "basic-python-intro.html#control-structures",
    "title": "Introduction to Python",
    "section": "Control Structures",
    "text": "Control Structures\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "basic-python-intro.html#functions",
    "href": "basic-python-intro.html#functions",
    "title": "Introduction to Python",
    "section": "Functions",
    "text": "Functions\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "basic-python-intro.html#lists-and-dictionaries",
    "href": "basic-python-intro.html#lists-and-dictionaries",
    "title": "Introduction to Python",
    "section": "Lists and Dictionaries",
    "text": "Lists and Dictionaries\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "blog-post-title.html",
    "href": "blog-post-title.html",
    "title": "A. Brennan",
    "section": "",
    "text": "HW 2 - Ben & Jerry’s Ice Cream\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHW 3 - Beer Markets\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHW 4 - Homes DataFrame\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHomework 5 - NFL\n\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nAnn Brennan\n\n\n\n\n\n\n  \n\n\n\n\nPySpark Basics\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nAnn Brennan\n\n\n\n\n\n\n  \n\n\n\n\nPython Basics\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSpotify Data Frame\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nAnn Brennan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "danl-200-cw-10.html",
    "href": "danl-200-cw-10.html",
    "title": "Classwork 10 - Tidy Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nbillboard &lt;- read_csv('https://bcdanl.github.io/data/billboard.csv')"
  },
  {
    "objectID": "danl-200-cw-10.html#question-1",
    "href": "danl-200-cw-10.html#question-1",
    "title": "Classwork 10 - Tidy Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nbillboard &lt;- read_csv('https://bcdanl.github.io/data/billboard.csv')"
  },
  {
    "objectID": "danl-200-cw-10.html#q1a",
    "href": "danl-200-cw-10.html#q1a",
    "title": "Classwork 10 - Tidy Data",
    "section": "2 Q1a",
    "text": "2 Q1a\n\nDescribe how the distribution of rating varies across week 1, week 2, and week 3 using the faceted histogram.\n\n\nq1a &lt;- billboard %&gt;% \n  pivot_longer(cols = wk1:wk76,\n               names_to = \"week\",\n               values_to = \"rating\") %&gt;% \n  filter(week %in% c('wk1', 'wk2', 'wk3'))\nggplot(q1a,\n       aes(x = rating)) +\n  geom_histogram() +\n  facet_wrap(.~ week)"
  },
  {
    "objectID": "danl-200-cw-10.html#q1b",
    "href": "danl-200-cw-10.html#q1b",
    "title": "Classwork 10 - Tidy Data",
    "section": "3 Q1b",
    "text": "3 Q1b\n\nWhich artist(s) have the most number of tracks in billboard data.frame?\nDo not double-count an artist’s tracks if they appear in multiple weeks."
  },
  {
    "objectID": "danl-200-cw-10.html#question-2",
    "href": "danl-200-cw-10.html#question-2",
    "title": "Classwork 10 - Tidy Data",
    "section": "4 Question 2",
    "text": "4 Question 2\n\nny_pincp &lt;- read_csv('https://bcdanl.github.io/data/NY_pinc_wide.csv')"
  },
  {
    "objectID": "danl-200-cw-10.html#q2a",
    "href": "danl-200-cw-10.html#q2a",
    "title": "Classwork 10 - Tidy Data",
    "section": "5 Q2a",
    "text": "5 Q2a\n\nMake ny_pincp longer.\n\n\nq2a &lt;- ny_pincp %&gt;% \n  pivot_longer(cols = pincp1969:pincp2019,\n               names_to = \"year\",\n               values_to = \"pincp\")"
  },
  {
    "objectID": "danl-200-cw-10.html#q2b",
    "href": "danl-200-cw-10.html#q2b",
    "title": "Classwork 10 - Tidy Data",
    "section": "6 Q2b",
    "text": "6 Q2b\n\nProvide both (1) ggplot code and (2) a simple comment to describe how overall the yearly trend of NY counties’ average personal incomes are."
  },
  {
    "objectID": "danl-200-cw-10.html#question-3",
    "href": "danl-200-cw-10.html#question-3",
    "title": "Classwork 10 - Tidy Data",
    "section": "7 Question 3",
    "text": "7 Question 3\n\ncovid &lt;- read_csv('https://bcdanl.github.io/data/covid19_cases.csv')"
  },
  {
    "objectID": "danl-200-cw-10.html#q3a",
    "href": "danl-200-cw-10.html#q3a",
    "title": "Classwork 10 - Tidy Data",
    "section": "8 Q3a",
    "text": "8 Q3a\n\nKeep only the following three variables, date, countriesAndTerritories, and cases.\nThen make a wide-form data.frame of covid whose variable names are from countriesAndTerritories and values are from cases.\nThen drop the variable date."
  },
  {
    "objectID": "danl-200-cw-10.html#q3b",
    "href": "danl-200-cw-10.html#q3b",
    "title": "Classwork 10 - Tidy Data",
    "section": "9 Q3b",
    "text": "9 Q3b\n\nUse the wide-form data.frame of covid to find the top 10 countries in terms of the correlation between their cases and the USA case.\n\nUse cor(data.frame), which returns a matrix.\nThen convert it to data.frame using as.data.frame(matrix)"
  },
  {
    "objectID": "danl-320-python-basic.html",
    "href": "danl-320-python-basic.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "danl-320-python-basic.html#what-is-python",
    "href": "danl-320-python-basic.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\nprint('Hello, World!')"
  },
  {
    "objectID": "danl-320-python-basic.html#variables-and-data-types",
    "href": "danl-320-python-basic.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n10.5"
  },
  {
    "objectID": "danl-320-python-basic.html#control-structures",
    "href": "danl-320-python-basic.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\nThese conditions can be used in several ways, most commonly in ‘if statements’ and loops.\n\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "danl-320-python-basic.html#functions",
    "href": "danl-320-python-basic.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "danl-320-python-basic.html#lists-and-dictionaries",
    "href": "danl-320-python-basic.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "pandas_basics.html#creating-a-series",
    "href": "pandas_basics.html#creating-a-series",
    "title": "Pandas Basics",
    "section": "Creating a Series",
    "text": "Creating a Series\n\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40, 50]\nseries = pd.Series(data)\nseries\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30\n\n\n3\n40\n\n\n4\n50\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "pandas_basics.html#creating-a-dataframe",
    "href": "pandas_basics.html#creating-a-dataframe",
    "title": "Pandas Basics",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\n\n# Creating a DataFrame from a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAlice\n25\nNew York\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#exploring-data",
    "href": "pandas_basics.html#exploring-data",
    "title": "Pandas Basics",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n\n# Display the first few rows\ndf.head()\n\n# Display the shape of the DataFrame\nprint(\"Shape:\", df.shape)\n\n# Display summary statistics\ndf.describe()\n\nShape: (3, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAge\n\n\n\n\ncount\n3.0\n\n\nmean\n30.0\n\n\nstd\n5.0\n\n\nmin\n25.0\n\n\n25%\n27.5\n\n\n50%\n30.0\n\n\n75%\n32.5\n\n\nmax\n35.0"
  },
  {
    "objectID": "pandas_basics.html#selecting-data",
    "href": "pandas_basics.html#selecting-data",
    "title": "Pandas Basics",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n# Selecting a single column\ndf[\"Name\"]\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nAlice\n\n\n1\nBob\n\n\n2\nCharlie\n\n\n\n\ndtype: object\n\n\n\n# Selecting multiple columns\ndf[[\"Name\", \"City\"]]\n\n\n  \n    \n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAlice\nNew York\n\n\n1\nBob\nLos Angeles\n\n\n2\nCharlie\nChicago\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Selecting rows by index\ndf.iloc[0]\n\n\n\n\n\n\n\n\n0\n\n\n\n\nName\nAlice\n\n\nAge\n25\n\n\nCity\nNew York\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "pandas_basics.html#filtering-data",
    "href": "pandas_basics.html#filtering-data",
    "title": "Pandas Basics",
    "section": "Filtering Data",
    "text": "Filtering Data\n\n# Filtering rows where Age is greater than 25\nfiltered_df = df[df[\"Age\"] &gt; 25]\nfiltered_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#adding-a-new-column",
    "href": "pandas_basics.html#adding-a-new-column",
    "title": "Pandas Basics",
    "section": "Adding a New Column",
    "text": "Adding a New Column\n\n\n# Adding a new column\ndf[\"Salary\"] = [50000, 60000, 70000]\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\nSalary\n\n\n\n\n0\nAlice\n25\nNew York\n50000\n\n\n1\nBob\n30\nLos Angeles\n60000\n\n\n2\nCharlie\n35\nChicago\n70000\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n    ## Conclusion\n\n    This notebook covers the basic operations of pandas. You can explore more advanced features like merging,\n    joining, and working with time series data in pandas documentation: https://pandas.pydata.org/docs/"
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html",
    "href": "posts/homework/danl200-hw5-brennan-ann.html",
    "title": "Homework 5 - NFL",
    "section": "",
    "text": "My Github repository: https://github.com/annbrennan/annbrennan.github.io\nThe following is the data.frame for Question 2.\nNFL2022_stuffs &lt;- read_csv('https://bcdanl.github.io/data/NFL2022_stuffs.csv')"
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#variable-description",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#variable-description",
    "title": "Homework 5 - NFL",
    "section": "Variable Description",
    "text": "Variable Description\nplay_id: Numeric play identifier that when used with game_id and drive provides the unique identifier for a single play game_id: Ten digit identifier for NFL game. drive: Numeric drive number in the game. week: Season week. posteam: String abbreviation for the team with possession. qtr: Quarter of the game (5 is overtime). half_seconds_remaining: Numeric seconds remaining in the half. down: The down for the given play. Basically you get four attempts (aka downs) to move the ball 10 yards (by either running with it or passing it). If you make 10 yards then you get another set of four downs. pass: Binary indicator if the play was a pass play. wp: Estimated winning probability for the posteam given the current situation at the start of the given play."
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2a",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2a",
    "title": "Homework 5 - NFL",
    "section": "Q2a",
    "text": "Q2a\nIn data.frame, NFL2022_stuffs, remove observations for which values of posteam is missing.\n\nNFL2022_stuffs &lt;- na.omit(NFL2022_stuffs)"
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2b",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2b",
    "title": "Homework 5 - NFL",
    "section": "Q2b",
    "text": "Q2b\nSummarize the mean value of pass for each posteam when all the following conditions hold: 1. wp is greater than 20% and less than 75%; 2. down is less than or equal to 2; and 3. half_seconds_remaining is greater than 120.\n\nq2b &lt;- NFL2022_stuffs %&gt;%\n  filter(wp &gt; 0.2 & wp &lt; 0.75 & down &lt;= 2 & half_seconds_remaining &gt; 120) %&gt;%\n  group_by(posteam) %&gt;%\n  summarise(mean_pass = mean(pass, na.rm = TRUE))"
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2c",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2c",
    "title": "Homework 5 - NFL",
    "section": "Q2c",
    "text": "Q2c\nProvide both (1) a ggplot code with geom_point() using the resulting data.frame in Q2b and (2) a simple comments to describe the mean value of pass for each posteam.\nIn the ggplot, reorder the posteam categories based on the mean value of pass in ascending or in descending order.\n\nggplot(q2b, aes(x = reorder(posteam, mean_pass), y = mean_pass)) +\n  geom_point() +\n  labs(x = \"team with possession\",\n       y = \"percentage of pass plays\") +\n  coord_flip()\n\n\n\n\nThe teams listed at the top of the graph have the largest mean value of pass, whereas the teams at the bottom have the smallest mean value of pass. The teams are in descending order based on mean value of pass. CIN and KC have the highest mean value of pass. WAS and ATL have the lowest mean value of pass."
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2d",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2d",
    "title": "Homework 5 - NFL",
    "section": "Q2d",
    "text": "Q2d\nConsider the following data.frame, NFL2022_epa:\n\nNFL2022_epa &lt;- read_csv('https://bcdanl.github.io/data/NFL2022_epa.csv')\n\nCreate the data.frame, NFL2022_stuffs_EPA, that includes: - All the variables in the data.frame, NFL2022_stuffs; - The variables, passer, receiver, and epa, from the data.frame, NFL2022_epa. by joining the two data.frames.\nIn the resulting data.frame, NFL2022_stuffs_EPA, remove observations with NA in passer.\n\nNFL2022_stuffs_EPA &lt;- left_join(NFL2022_stuffs, NFL2022_epa[, c(\"play_id\", \"passer\", \"receiver\", \"epa\")], by = \"play_id\")\n\nNFL2022_stuffs_EPA &lt;- NFL2022_stuffs_EPA %&gt;%\n  filter(!is.na(passer))"
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2e",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2e",
    "title": "Homework 5 - NFL",
    "section": "Q2e",
    "text": "Q2e\nProvide both (1) a single ggplot and (2) a simple comment to describe the NFL weekly trend of weekly mean value of epa for each of the following two passers, 1. “J.Allen” 2. “P.Mahomes”\n\nq2e &lt;- NFL2022_stuffs_EPA %&gt;%\n  filter(passer %in% c(\"J.Allen\", \"P.Mahomes\"))\n\nggplot(q2e, aes(x = week, y = epa, color = passer)) +\n  geom_line(stat = \"summary\", fun = \"mean\") +\n  labs(x = \"Week\",\n       y = \"Mean EPA\",\n       color = \"Passer\") + scale_color_manual(values = c(\"J.Allen\" = \"blue\", \"P.Mahomes\" = \"red\"))\n\n\n\n\nOverall, P. Mahomes generally had a higher mean EPA than J. Allen. However, in the most recent weeks, J. Allen’s mean EPA has risen significantly and is now much higher than P. Mahomes. Meanwhile, P. Mahomes’ mean EPA decreased slightly."
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2f",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2f",
    "title": "Homework 5 - NFL",
    "section": "Q2f",
    "text": "Q2f\nCalculate the difference between the mean value of epa for “J.Allen” the mean value of epa for “P.Mahomes” for each value of week.\n\nmean_epa_by_week &lt;- q2e %&gt;%\n  group_by(passer, week) %&gt;%\n  summarise(mean_epa = mean(epa, na.rm = TRUE)) %&gt;% \n  pivot_wider(names_from = passer, values_from = mean_epa)\n\nmean_epa_by_week$epa_difference &lt;- mean_epa_by_week$`J.Allen` - mean_epa_by_week$`P.Mahomes`"
  },
  {
    "objectID": "posts/homework/danl200-hw5-brennan-ann.html#q2g",
    "href": "posts/homework/danl200-hw5-brennan-ann.html#q2g",
    "title": "Homework 5 - NFL",
    "section": "Q2g",
    "text": "Q2g\nSummarize the resulting data.frame in Q2d, with the following four variables:\n\nposteam: String abbreviation for the team with possession.\npasser: Name of the player who passed a ball to a receiver by initially taking a three-step drop, and backpedaling into the pocket to make a pass. (Mostly, they are quarterbacks.)\nmean_epa: Mean value of epa in 2022 for each passer\nn_pass: Number of observations for each passer\n\nThen find the top 10 NFL passers in 2022 in terms of the mean value of epa, conditioning that n_pass must be greater than or equal to the third quantile level of n_pass.\n\nsummary &lt;- NFL2022_stuffs_EPA %&gt;%\n  group_by(posteam, passer) %&gt;%\n  summarise(mean_epa = mean(epa, na.rm = TRUE),\n            n_pass = n())\n\nthird_quantile &lt;- quantile(summary$n_pass, 0.75)\n\nq2f &lt;- summary %&gt;%\n  filter(n_pass &gt;= third_quantile) %&gt;%\n  arrange(desc(mean_epa)) %&gt;%\n  slice_head(n = 10)"
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html",
    "href": "posts/hw3-beer/hw3_part2.html",
    "title": "HW 3 - Beer Markets",
    "section": "",
    "text": "In this post, we will be exploring beer pricing by analyzing the DataFrame “Beer Markets”. This DataFrame contains information about beer purchase details and household characteristics. We will analyze how these factors, such as household size, type of beer container, and household income impact the price of beer (per fluid ounce)."
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#loading-packages-and-settings",
    "href": "posts/hw3-beer/hw3_part2.html#loading-packages-and-settings",
    "title": "HW 3 - Beer Markets",
    "section": "Loading Packages and Settings",
    "text": "Loading Packages and Settings\n\nimport pandas as pd\nimport numpy as np\nfrom tabulate import tabulate  # for table summary\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm  # for lowess smoothing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import rand, col, pow, mean, avg, when, log, sqrt, exp\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression, GeneralizedLinearRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#udfs",
    "href": "posts/hw3-beer/hw3_part2.html#udfs",
    "title": "HW 3 - Beer Markets",
    "section": "UDFs",
    "text": "UDFs\nWe will be using a variety of User Defined Functions (UDFs) in order to perform functions necessary for our Linear Regression Models. We will define our UDFs before we begin.\n\ndef add_dummy_variables(var_name, reference_level, category_order=None):\n    \"\"\"\n    Creates dummy variables for the specified column in the global DataFrames dtrain and dtest.\n    Allows manual setting of category order.\n\n    Parameters:\n        var_name (str): The name of the categorical column (e.g., \"borough_name\").\n        reference_level (int): Index of the category to be used as the reference (dummy omitted).\n        category_order (list, optional): List of categories in the desired order. If None, categories are sorted.\n\n    Returns:\n        dummy_cols (list): List of dummy column names excluding the reference category.\n        ref_category (str): The category chosen as the reference.\n    \"\"\"\n    global dtrain, dtest\n\n    # Get distinct categories from the training set.\n    categories = dtrain.select(var_name).distinct().rdd.flatMap(lambda x: x).collect()\n\n    # Convert booleans to strings if present.\n    categories = [str(c) if isinstance(c, bool) else c for c in categories]\n\n    # Use manual category order if provided; otherwise, sort categories.\n    if category_order:\n        # Ensure all categories are present in the user-defined order\n        missing = set(categories) - set(category_order)\n        if missing:\n            raise ValueError(f\"These categories are missing from your custom order: {missing}\")\n        categories = category_order\n    else:\n        categories = sorted(categories)\n\n    # Validate reference_level\n    if reference_level &lt; 0 or reference_level &gt;= len(categories):\n        raise ValueError(f\"reference_level must be between 0 and {len(categories) - 1}\")\n\n    # Define the reference category\n    ref_category = categories[reference_level]\n    print(\"Reference category (dummy omitted):\", ref_category)\n\n    # Create dummy variables for all categories\n    for cat in categories:\n        dummy_col_name = var_name + \"_\" + str(cat).replace(\" \", \"_\")\n        dtrain = dtrain.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n        dtest = dtest.withColumn(dummy_col_name, when(col(var_name) == cat, 1).otherwise(0))\n\n    # List of dummy columns, excluding the reference category\n    dummy_cols = [var_name + \"_\" + str(cat).replace(\" \", \"_\") for cat in categories if cat != ref_category]\n\n    return dummy_cols, ref_category\n\n\n# Example usage without category_order:\n# dummy_cols_year, ref_category_year = add_dummy_variables('year', 0)\n\n# Example usage with category_order:\n# custom_order_wkday = ['sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday']\n# dummy_cols_wkday, ref_category_wkday = add_dummy_variables('wkday', reference_level=0, category_order = custom_order_wkday)\n\n\ndef regression_table(model, assembler):\n    \"\"\"\n    Creates a formatted regression table from a fitted LinearRegression model and its VectorAssembler,\n    and inserts a dashed horizontal line after the Intercept row. The table includes separate columns\n    for the 95% confidence interval lower and upper bounds for each coefficient (computed at the 5% significance level)\n    and an \"Observations\" row (using model.summary.numInstances) above the R² row.\n    The RMSE row is placed as the last row.\n\n    The columns are ordered as:\n        Metric | Value | Significance | Std. Error | p-value | 95% CI Lower | 95% CI Upper\n\n    For the \"Value\", \"Std. Error\", \"95% CI Lower\", and \"95% CI Upper\" columns, commas are inserted every three digits,\n    with 3 decimal places (except for Observations which is formatted as an integer with commas).\n\n    Parameters:\n        model: A fitted LinearRegression model (with a .summary attribute).\n        assembler: The VectorAssembler used to assemble the features for the model.\n\n    Returns:\n        A formatted string containing the regression table.\n    \"\"\"\n    # Extract coefficients and standard errors as NumPy arrays\n    coeffs = model.coefficients.toArray()\n\n    std_errors_all = np.array(model.summary.coefficientStandardErrors)\n\n    # Check if the intercept's standard error is included (one extra element)\n    if len(std_errors_all) == len(coeffs) + 1:\n        intercept_se = std_errors_all[0]\n        std_errors = std_errors_all[1:]\n    else:\n        intercept_se = None\n        std_errors = std_errors_all\n\n    # Compute t-statistics for feature coefficients (t = beta / SE(beta))\n    # t_stats = coeffs / std_errors\n    t_stats = model.summary.tValues\n\n    # Degrees of freedom: number of instances minus number of predictors minus 1 (for intercept)\n    df = model.summary.numInstances - len(coeffs) - 1\n\n    # Compute the t-critical value for a 95% confidence interval (two-tailed, 5% significance)\n    t_critical = stats.t.ppf(0.975, df)\n\n    # Compute two-tailed p-values for each feature coefficient\n    # p_values = [2 * (1 - stats.t.cdf(np.abs(t), df)) for t in t_stats]\n    p_values = model.summary.pValues\n\n    # Function to assign significance stars based on p-value\n    def significance_stars(p):\n        if p &lt; 0.01:\n            return \"***\"\n        elif p &lt; 0.05:\n            return \"**\"\n        elif p &lt; 0.1:\n            return \"*\"\n        else:\n            return \"\"\n\n    # Build the table rows.\n    # Order: Metric, Value, Significance, Std. Error, p-value, 95% CI Lower, 95% CI Upper.\n    table = []\n    for feature, beta, se, p in zip(assembler.getInputCols(), coeffs, std_errors, p_values):\n        ci_lower = beta - t_critical * se\n        ci_upper = beta + t_critical * se\n        table.append([\n            \"Beta: \" + feature,       # Metric name\n            beta,                     # Beta estimate (Value)\n            significance_stars(p),    # Significance stars\n            se,                       # Standard error\n            p,                        # p-value\n            ci_lower,                 # 95% CI lower bound\n            ci_upper                  # 95% CI upper bound\n        ])\n\n    # Compute and add the intercept row with its SE, p-value, significance, and CI (if available)\n    if intercept_se is not None:\n        intercept_t = model.intercept / intercept_se\n        intercept_p = 2 * (1 - stats.t.cdf(np.abs(intercept_t), df))\n        intercept_sig = significance_stars(intercept_p)\n        ci_intercept_lower = model.intercept - t_critical * intercept_se\n        ci_intercept_upper = model.intercept + t_critical * intercept_se\n    else:\n        intercept_se = \"\"\n        intercept_p = \"\"\n        intercept_sig = \"\"\n        ci_intercept_lower = \"\"\n        ci_intercept_upper = \"\"\n\n    table.append([\n        \"Intercept\",\n        model.intercept,\n        intercept_sig,\n        intercept_se,\n        intercept_p,\n        ci_intercept_lower,\n        ci_intercept_upper\n    ])\n\n    # Append overall model metrics:\n    # Insert an Observations row using model.summary.numInstances,\n    # then an R² row, and finally the RMSE row as the last row.\n    table.append([\"Observations\", model.summary.numInstances, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"R²\", model.summary.r2, \"\", \"\", \"\", \"\", \"\"])\n    table.append([\"RMSE\", model.summary.rootMeanSquaredError, \"\", \"\", \"\", \"\", \"\"])\n\n    # Format the table.\n    # For the \"Value\" (index 1), \"Std. Error\" (index 3), \"95% CI Lower\" (index 5), and \"95% CI Upper\" (index 6) columns,\n    # format with commas and 3 decimal places, except for Observations which should be an integer with commas.\n    # For the p-value (index 4), format to 3 decimal places.\n    formatted_table = []\n    for row in table:\n        formatted_row = []\n        for i, item in enumerate(row):\n            if row[0] == \"Observations\" and i == 1 and isinstance(item, (int, float, np.floating)) and item != \"\":\n                # Format Observations as integer with commas, no decimals.\n                formatted_row.append(f\"{int(item):,}\")\n            elif isinstance(item, (int, float, np.floating)) and item != \"\":\n                if i in [1, 3, 5, 6]:\n                    formatted_row.append(f\"{item:,.3f}\")\n                elif i == 4:\n                    formatted_row.append(f\"{item:.3f}\")\n                else:\n                    formatted_row.append(f\"{item:.3f}\")\n            else:\n                formatted_row.append(item)\n        formatted_table.append(formatted_row)\n\n    # Generate the table string using tabulate.\n    table_str = tabulate(\n        formatted_table,\n        headers=[\"Metric\", \"Value\", \"Sig.\", \"Std. Error\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"],\n        tablefmt=\"pretty\",\n        colalign=(\"left\", \"right\", \"center\", \"right\", \"right\", \"right\", \"right\")\n    )\n\n    # Insert a dashed line after the Intercept row for clarity.\n    lines = table_str.split(\"\\n\")\n    dash_line = '-' * len(lines[0])\n    for i, line in enumerate(lines):\n        if \"Intercept\" in line and not line.strip().startswith('+'):\n            lines.insert(i+1, dash_line)\n            break\n\n    return \"\\n\".join(lines)\n\n\ndef add_interaction_terms(var_list1, var_list2, var_list3=None):\n    \"\"\"\n    Creates interaction term columns in the global DataFrames dtrain and dtest.\n\n    For two sets of variable names (which may represent categorical (dummy) or continuous variables),\n    this function creates two-way interactions by multiplying each variable in var_list1 with each\n    variable in var_list2.\n\n    Optionally, if a third list of variable names (var_list3) is provided, the function also creates\n    three-way interactions among each variable in var_list1, each variable in var_list2, and each variable\n    in var_list3.\n\n    Parameters:\n        var_list1 (list): List of column names for the first set of variables.\n        var_list2 (list): List of column names for the second set of variables.\n        var_list3 (list, optional): List of column names for the third set of variables for three-way interactions.\n\n    Returns:\n        A flat list of new interaction column names.\n    \"\"\"\n    global dtrain, dtest\n\n    interaction_cols = []\n\n    # Create two-way interactions between var_list1 and var_list2.\n    for var1 in var_list1:\n        for var2 in var_list2:\n            col_name = f\"{var1}_*_{var2}\"\n            dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\"))\n            interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list1 and var_list3.\n    if var_list3 is not None:\n      for var1 in var_list1:\n          for var3 in var_list3:\n              col_name = f\"{var1}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # Create two-way interactions between var_list2 and var_list3.\n    if var_list3 is not None:\n      for var2 in var_list2:\n          for var3 in var_list3:\n              col_name = f\"{var2}_*_{var3}\"\n              dtrain = dtrain.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              dtest = dtest.withColumn(col_name, col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n              interaction_cols.append(col_name)\n\n    # If a third list is provided, create three-way interactions.\n    if var_list3 is not None:\n        for var1 in var_list1:\n            for var2 in var_list2:\n                for var3 in var_list3:\n                    col_name = f\"{var1}_*_{var2}_*_{var3}\"\n                    dtrain = dtrain.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    dtest = dtest.withColumn(col_name, col(var1).cast(\"double\") * col(var2).cast(\"double\") * col(var3).cast(\"double\"))\n                    interaction_cols.append(col_name)\n\n    return interaction_cols\n\n # Example\n # interaction_cols_brand_price = add_interaction_terms(dummy_cols_brand, ['log_price'])\n # interaction_cols_brand_ad_price = add_interaction_terms(dummy_cols_brand, dummy_cols_ad, ['log_price'])"
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#loading-dataframe---beer-markets",
    "href": "posts/hw3-beer/hw3_part2.html#loading-dataframe---beer-markets",
    "title": "HW 3 - Beer Markets",
    "section": "Loading DataFrame - Beer Markets",
    "text": "Loading DataFrame - Beer Markets\n\nbeer_markets = pd.read_csv(\n  'https://bcdanl.github.io/data/beer_markets_all_cleaned.csv'\n)"
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#filtering-dataframe",
    "href": "posts/hw3-beer/hw3_part2.html#filtering-dataframe",
    "title": "HW 3 - Beer Markets",
    "section": "Filtering DataFrame",
    "text": "Filtering DataFrame\nWe will be focusing on observations that are either a can or non-refillable bottle in the beer_markets DataFrame. Additionally, we will be using the log of beer volume as one of our predictors.\n\nbeer_df = beer_markets.loc[\n  (beer_markets['container'] == 'CAN') | (beer_markets['container'] == 'NON REFILLABLE BOTTLE')\n]\n\nbeer_df\n\n\n  \n    \n\n\n\n\n\n\nhousehold\nX_purchase_desc\nquantity\nbrand\ndollar_spent\nbeer_floz\nprice_floz\ncontainer\npromo\nregion\n...\nage\nemployment\ndegree\noccupation\nethnic\nmicrowave\ndishwasher\ntvcable\nsinglefamilyhome\nnpeople\n\n\n\n\n0\n2000946\nBUD LT BR CN 12P\n1\nBUD_LIGHT\n8.14\n144.0\n0.056528\nCAN\nFalse\nCENTRAL\n...\n50+\nnone\nGrad\nnone/retired/student\nwhite\nTrue\nTrue\npremium\nFalse\n1\n\n\n1\n2003036\nBUD LT BR CN 24P\n1\nBUD_LIGHT\n17.48\n288.0\n0.060694\nCAN\nFalse\nSOUTH\n...\n50+\nfull\nCollege\nclerical/sales/service\nwhite\nTrue\nTrue\nbasic\nTrue\n2\n\n\n2\n2003036\nBUD LT BR CN 24P\n2\nBUD_LIGHT\n33.92\n576.0\n0.058889\nCAN\nFalse\nSOUTH\n...\n50+\nfull\nCollege\nclerical/sales/service\nwhite\nTrue\nTrue\nbasic\nTrue\n2\n\n\n3\n2003036\nBUD LT BR CN 30P\n2\nBUD_LIGHT\n34.74\n720.0\n0.048250\nCAN\nFalse\nSOUTH\n...\n50+\nfull\nCollege\nclerical/sales/service\nwhite\nTrue\nTrue\nbasic\nTrue\n2\n\n\n4\n2003036\nBUD LT BR CN 36P\n2\nBUD_LIGHT\n40.48\n864.0\n0.046852\nCAN\nFalse\nSOUTH\n...\n50+\nfull\nCollege\nclerical/sales/service\nwhite\nTrue\nTrue\nbasic\nTrue\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n73110\n9158319\nNATURAL LT BR CN 24P\n1\nNATURAL_LIGHT\n11.49\n288.0\n0.039896\nCAN\nTrue\nSOUTH\n...\n50+\nnone\nHS\nnone/retired/student\nblack\nTrue\nTrue\npremium\nTrue\n2\n\n\n73111\n9158319\nNATURAL LT BR CN 24P\n1\nNATURAL_LIGHT\n9.97\n288.0\n0.034618\nCAN\nFalse\nSOUTH\n...\n50+\nnone\nHS\nnone/retired/student\nblack\nTrue\nTrue\npremium\nTrue\n2\n\n\n73112\n9158319\nNATURAL LT BR CN 24P\n1\nNATURAL_LIGHT\n9.98\n288.0\n0.034653\nCAN\nFalse\nSOUTH\n...\n50+\nnone\nHS\nnone/retired/student\nblack\nTrue\nTrue\npremium\nTrue\n2\n\n\n73113\n9164033\nNATURAL LT BR CN 30P\n1\nNATURAL_LIGHT\n12.98\n360.0\n0.036056\nCAN\nFalse\nWEST\n...\n50+\nfull\nCollege\nprof\nhispanic\nTrue\nTrue\nbasic\nTrue\n5plus\n\n\n73114\n9164155\nNATURAL LT BR CN 24P\n1\nNATURAL_LIGHT\n13.49\n288.0\n0.046840\nCAN\nFalse\nSOUTH\n...\n50+\nfull\nHS\nlabor/craft/military/farm\nwhite\nTrue\nTrue\nnone\nTrue\n2\n\n\n\n\n\n53015 rows × 26 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ndf = spark.createDataFrame(beer_df)\n\n\ndf = df.withColumn('log_beer_floz', log('beer_floz'))\n\n\ndf = df.withColumn('log_price_floz', log('price_floz'))"
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#split-into-training-and-test-dataframes",
    "href": "posts/hw3-beer/hw3_part2.html#split-into-training-and-test-dataframes",
    "title": "HW 3 - Beer Markets",
    "section": "Split into Training and Test DataFrames",
    "text": "Split into Training and Test DataFrames\nFirst, we will split our data into training and test DataFrames. 67% of observations will belong to the training DataFrame and the remaining 33% will go to the test DataFrame.\n\ndtrain, dtest = df.randomSplit([.67, .33], seed = 1234)"
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#overview-of-linear-regression-models",
    "href": "posts/hw3-beer/hw3_part2.html#overview-of-linear-regression-models",
    "title": "HW 3 - Beer Markets",
    "section": "Overview of Linear Regression Models",
    "text": "Overview of Linear Regression Models\nFor the beer_markets DataFrame, we will consider and compare three different Linear Regression models. In Model 1, there are three independent or predictor variables: dummy variables for different beer markets, a dummy variable for whether or not the container is a can, and the log of the volume of beer in fluid ounces. Model 2 has the same independent variables as Model 1 in addition to the interaction term between different beer brands and the log of beer volume. Model 3 has the same independent variables as Model 2 in addition to three more interaction terms: whether or not promotion was used and the log of beer volume, brand and promotion usage, and brand, promotion usage, and the log of beer volume. For Models 2 and 3, the interaction terms show us how the relationship between one predictor and the outcome variable changes as the value of another predictor changes. For all three models, the dependent or outcome variable is the log of price per fluid ounce of beer. Using linear regression, we can predict the log price per fluid ounce based on the independent variables given in the model."
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#training-the-models",
    "href": "posts/hw3-beer/hw3_part2.html#training-the-models",
    "title": "HW 3 - Beer Markets",
    "section": "Training the Models",
    "text": "Training the Models\nBefore we begin training our models, we must add dummy variables for our categorical variables. To do this, we will apply our UDF for adding dummy variables. All observations must be numerical in order to train the Linear Regression Model.\n\ndummy_cols_brand, ref_category_brand = add_dummy_variables('brand', 0, category_order=None)\ndummy_cols_market, ref_category_market = add_dummy_variables('market', 5, category_order=None)\ndummy_cols_container, ref_category_container = add_dummy_variables('container', 0, category_order=None)\n\nReference category (dummy omitted): BUD_LIGHT\nReference category (dummy omitted): BUFFALO-ROCHESTER\nReference category (dummy omitted): CAN\n\n\n\nModel 1\n\nx_cols_1 = ['log_beer_floz']\n\nassembler_predictors_1 = (\n    x_cols_1 +\n    dummy_cols_brand + dummy_cols_market + dummy_cols_container\n)\n\n\nassembler_1 = VectorAssembler(\n    inputCols = assembler_predictors_1,\n    outputCol = \"predictors\"\n)\n\ndtrain_1 = assembler_1.transform(dtrain)\ndtest_1  = assembler_1.transform(dtest)\n\n# training model\nmodel_1 = (\n        LinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"log_price_floz\")\n    .fit(dtrain_1)\n)\n\n# making prediction\ndtrain_1 = model_1.transform(dtrain_1)\ndtest_1 = model_1.transform(dtest_1)\n\n\nSummary\n\nprint(dtest_1.select([\"prediction\", \"price_floz\"]).show())\n\n+-------------------+------------------+\n|         prediction|        price_floz|\n+-------------------+------------------+\n| -2.792350778968452|0.0659027777777777|\n| -2.864445041558655|0.0565277777777777|\n|-2.8788697218373027|           0.06375|\n|  -3.27220818906385|0.0392222222222222|\n| -3.327091138288483|0.0388611111111111|\n| -3.327091138288483|0.0388611111111111|\n| -3.327091138288483|0.0415833333333333|\n|-3.1275589685986342|0.0406944444444444|\n| -2.766643192389631|0.0620833333333333|\n|-2.7807239771704384|0.0740277777777777|\n| -2.958268062960834|0.0485763888888888|\n|-2.8454523724376664|0.0554861111111111|\n|-2.8454523724376664|0.0763194444444444|\n|-2.9003353216622996|           0.04625|\n|-2.9003353216622996|           0.04625|\n| -2.969479660376425|0.0444166666666666|\n| -2.969479660376425|0.0471944444444444|\n| -2.969479660376425|0.0471944444444444|\n| -2.969479660376425|           0.05275|\n| -2.890809854407121|0.0588888888888888|\n+-------------------+------------------+\nonly showing top 20 rows\n\nNone\n\n\n\nprint(regression_table(model_1, assembler_1))\n\n+-----------------------------------+--------+------+------------+---------+--------------+--------------+\n| Metric                            |  Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+-----------------------------------+--------+------+------------+---------+--------------+--------------+\n| Beta: log_beer_floz               | -0.135 | ***  |      0.003 |   0.000 |       -0.142 |       -0.129 |\n| Beta: brand_BUSCH_LIGHT           | -0.265 | ***  |      0.003 |   0.000 |       -0.271 |       -0.259 |\n| Beta: brand_COORS_LIGHT           | -0.004 |      |      0.003 |   0.224 |       -0.009 |        0.002 |\n| Beta: brand_MILLER_LITE           | -0.017 | ***  |      0.003 |   0.000 |       -0.022 |       -0.011 |\n| Beta: brand_NATURAL_LIGHT         | -0.320 | ***  |      0.014 |   0.000 |       -0.348 |       -0.293 |\n| Beta: market_ALBANY               |  0.031 |  **  |      0.012 |   0.025 |        0.008 |        0.055 |\n| Beta: market_ATLANTA              |  0.083 | ***  |      0.015 |   0.000 |        0.054 |        0.112 |\n| Beta: market_BALTIMORE            |  0.098 | ***  |      0.012 |   0.000 |        0.075 |        0.122 |\n| Beta: market_BIRMINGHAM           |  0.129 | ***  |      0.013 |   0.000 |        0.104 |        0.154 |\n| Beta: market_BOSTON               |  0.122 | ***  |      0.012 |   0.000 |        0.099 |        0.145 |\n| Beta: market_CHARLOTTE            |  0.016 |      |      0.011 |   0.173 |       -0.007 |        0.038 |\n| Beta: market_CHICAGO              | -0.002 |      |      0.012 |   0.852 |       -0.025 |        0.021 |\n| Beta: market_CINCINNATI           |  0.092 | ***  |      0.012 |   0.000 |        0.068 |        0.115 |\n| Beta: market_CLEVELAND            |  0.061 | ***  |      0.011 |   0.000 |        0.039 |        0.083 |\n| Beta: market_COLUMBUS             |  0.081 | ***  |      0.011 |   0.000 |        0.059 |        0.102 |\n| Beta: market_DALLAS               |  0.208 | ***  |      0.013 |   0.000 |        0.182 |        0.234 |\n| Beta: market_DENVER               |  0.116 | ***  |      0.013 |   0.000 |        0.091 |        0.142 |\n| Beta: market_DES_MOINES           |  0.148 | ***  |      0.011 |   0.000 |        0.126 |        0.171 |\n| Beta: market_DETROIT              |  0.098 | ***  |      0.031 |   0.000 |        0.037 |        0.159 |\n| Beta: market_EXURBAN_NJ           |  0.141 | ***  |      0.029 |   0.000 |        0.085 |        0.198 |\n| Beta: market_EXURBAN_NY           |  0.077 | ***  |      0.013 |   0.007 |        0.051 |        0.103 |\n| Beta: market_GRAND_RAPIDS         |  0.088 | ***  |      0.017 |   0.000 |        0.055 |        0.120 |\n| Beta: market_HARTFORD-NEW_HAVEN   |  0.129 | ***  |      0.011 |   0.000 |        0.106 |        0.151 |\n| Beta: market_HOUSTON              |  0.115 | ***  |      0.012 |   0.000 |        0.091 |        0.139 |\n| Beta: market_INDIANAPOLIS         |  0.095 | ***  |      0.016 |   0.000 |        0.064 |        0.127 |\n| Beta: market_JACKSONVILLE         |  0.135 | ***  |      0.014 |   0.000 |        0.107 |        0.162 |\n| Beta: market_KANSAS_CITY          |  0.071 | ***  |      0.015 |   0.000 |        0.041 |        0.101 |\n| Beta: market_LITTLE_ROCK          |  0.101 | ***  |      0.011 |   0.000 |        0.079 |        0.123 |\n| Beta: market_LOS_ANGELES          |  0.019 |      |      0.013 |   0.100 |       -0.008 |        0.045 |\n| Beta: market_LOUISVILLE           |  0.075 | ***  |      0.015 |   0.000 |        0.045 |        0.105 |\n| Beta: market_MEMPHIS              |  0.160 | ***  |      0.011 |   0.000 |        0.138 |        0.181 |\n| Beta: market_MIAMI                |  0.107 | ***  |      0.013 |   0.000 |        0.082 |        0.132 |\n| Beta: market_MILWAUKEE            |  0.038 | ***  |      0.013 |   0.003 |        0.013 |        0.063 |\n| Beta: market_MINNEAPOLIS          |  0.131 | ***  |      0.013 |   0.000 |        0.106 |        0.156 |\n| Beta: market_NASHVILLE            |  0.173 | ***  |      0.013 |   0.000 |        0.147 |        0.198 |\n| Beta: market_NEW_ORLEANS-MOBILE   |  0.143 | ***  |      0.013 |   0.000 |        0.118 |        0.168 |\n| Beta: market_OKLAHOMA_CITY-TULSA  |  0.159 | ***  |      0.012 |   0.000 |        0.135 |        0.183 |\n| Beta: market_OMAHA                |  0.141 | ***  |      0.012 |   0.000 |        0.117 |        0.165 |\n| Beta: market_ORLANDO              |  0.132 | ***  |      0.015 |   0.000 |        0.103 |        0.161 |\n| Beta: market_PHILADELPHIA         |  0.096 | ***  |      0.011 |   0.000 |        0.074 |        0.118 |\n| Beta: market_PHOENIX              |  0.154 | ***  |      0.016 |   0.000 |        0.122 |        0.186 |\n| Beta: market_PITTSBURGH           |  0.088 | ***  |      0.014 |   0.000 |        0.061 |        0.116 |\n| Beta: market_PORTLAND             |  0.104 | ***  |      0.012 |   0.000 |        0.080 |        0.128 |\n| Beta: market_RALEIGH-DURHAM       |  0.094 | ***  |      0.012 |   0.000 |        0.070 |        0.119 |\n| Beta: market_RICHMOND             |  0.029 |  **  |      0.017 |   0.018 |       -0.004 |        0.062 |\n| Beta: market_RURAL_ALABAMA        |  0.165 | ***  |      0.020 |   0.000 |        0.125 |        0.204 |\n| Beta: market_RURAL_ARKANSAS       |  0.178 | ***  |      0.013 |   0.000 |        0.153 |        0.203 |\n| Beta: market_RURAL_CALIFORNIA     |  0.038 | ***  |      0.071 |   0.003 |       -0.101 |        0.177 |\n| Beta: market_RURAL_COLORADO       |  0.171 |  **  |      0.014 |   0.016 |        0.143 |        0.198 |\n| Beta: market_RURAL_FLORIDA        |  0.060 | ***  |      0.015 |   0.000 |        0.031 |        0.090 |\n| Beta: market_RURAL_GEORGIA        |  0.151 | ***  |      0.020 |   0.000 |        0.111 |        0.190 |\n| Beta: market_RURAL_IDAHO          |  0.172 | ***  |      0.012 |   0.000 |        0.149 |        0.196 |\n| Beta: market_RURAL_ILLINOIS       |  0.016 |      |      0.015 |   0.196 |       -0.014 |        0.046 |\n| Beta: market_RURAL_INDIANA        |  0.095 | ***  |      0.012 |   0.000 |        0.071 |        0.118 |\n| Beta: market_RURAL_IOWA           |  0.090 | ***  |      0.021 |   0.000 |        0.048 |        0.132 |\n| Beta: market_RURAL_KANSAS         |  0.108 | ***  |      0.019 |   0.000 |        0.070 |        0.146 |\n| Beta: market_RURAL_KENTUCKY       |  0.152 | ***  |      0.016 |   0.000 |        0.121 |        0.183 |\n| Beta: market_RURAL_LOUISIANA      |  0.115 | ***  |      0.016 |   0.000 |        0.084 |        0.146 |\n| Beta: market_RURAL_MAINE          |  0.100 | ***  |      0.013 |   0.000 |        0.074 |        0.125 |\n| Beta: market_RURAL_MICHIGAN       |  0.095 | ***  |      0.024 |   0.000 |        0.048 |        0.143 |\n| Beta: market_RURAL_MINNESOTA      |  0.244 | ***  |      0.016 |   0.000 |        0.212 |        0.276 |\n| Beta: market_RURAL_MISSISSIPPI    |  0.050 | ***  |      0.014 |   0.002 |        0.022 |        0.077 |\n| Beta: market_RURAL_MISSOURI       |  0.105 | ***  |      0.016 |   0.000 |        0.074 |        0.137 |\n| Beta: market_RURAL_MONTANA        |  0.114 | ***  |      0.024 |   0.000 |        0.066 |        0.162 |\n| Beta: market_RURAL_NEBRASKA       |  0.169 | ***  |      0.014 |   0.000 |        0.141 |        0.197 |\n| Beta: market_RURAL_NEVADA         |  0.038 | ***  |      0.044 |   0.007 |       -0.048 |        0.125 |\n| Beta: market_RURAL_NEW_HAMPSHIRE  |  0.036 |      |      0.016 |   0.420 |        0.004 |        0.067 |\n| Beta: market_RURAL_NEW_MEXICO     |  0.165 | ***  |      0.078 |   0.000 |        0.013 |        0.318 |\n| Beta: market_RURAL_NEW_YORK       | -0.062 |      |      0.013 |   0.426 |       -0.087 |       -0.036 |\n| Beta: market_RURAL_NORTH_CAROLINA |  0.007 |      |      0.024 |   0.613 |       -0.040 |        0.053 |\n| Beta: market_RURAL_NORTH_DAKOTA   |  0.227 | ***  |      0.018 |   0.000 |        0.192 |        0.262 |\n| Beta: market_RURAL_OHIO           |  0.098 | ***  |      0.037 |   0.000 |        0.027 |        0.170 |\n| Beta: market_RURAL_OKLAHOMA       |  0.138 | ***  |      0.047 |   0.000 |        0.046 |        0.230 |\n| Beta: market_RURAL_OREGON         |  0.036 |      |      0.016 |   0.447 |        0.005 |        0.066 |\n| Beta: market_RURAL_PENNSYLVANIA   |  0.127 | ***  |      0.012 |   0.000 |        0.104 |        0.150 |\n| Beta: market_RURAL_SOUTH_CAROLINA |  0.060 | ***  |      0.023 |   0.000 |        0.015 |        0.104 |\n| Beta: market_RURAL_SOUTH_DAKOTA   |  0.072 | ***  |      0.016 |   0.002 |        0.041 |        0.103 |\n| Beta: market_RURAL_TENNESSEE      |  0.192 | ***  |      0.011 |   0.000 |        0.170 |        0.215 |\n| Beta: market_RURAL_TEXAS          |  0.173 | ***  |      0.022 |   0.000 |        0.130 |        0.216 |\n| Beta: market_RURAL_VERMONT        |  0.092 | ***  |      0.020 |   0.000 |        0.053 |        0.130 |\n| Beta: market_RURAL_VIRGINIA       |  0.013 |      |      0.016 |   0.494 |       -0.018 |        0.045 |\n| Beta: market_RURAL_WASHINGTON     |  0.140 | ***  |      0.018 |   0.000 |        0.105 |        0.175 |\n| Beta: market_RURAL_WEST_VIRGINIA  | -0.075 | ***  |      0.011 |   0.000 |       -0.098 |       -0.053 |\n| Beta: market_RURAL_WISCONSIN      |  0.041 | ***  |      0.037 |   0.000 |       -0.032 |        0.114 |\n| Beta: market_RURAL_WYOMING        |  0.157 | ***  |      0.012 |   0.000 |        0.133 |        0.181 |\n| Beta: market_SACRAMENTO           |  0.016 |      |      0.018 |   0.192 |       -0.018 |        0.050 |\n| Beta: market_SALT_LAKE_CITY       |  0.130 | ***  |      0.011 |   0.000 |        0.108 |        0.151 |\n| Beta: market_SAN_ANTONIO          |  0.147 | ***  |      0.013 |   0.000 |        0.121 |        0.173 |\n| Beta: market_SAN_DIEGO            | -0.003 |      |      0.012 |   0.821 |       -0.027 |        0.021 |\n| Beta: market_SAN_FRANCISCO        |  0.078 | ***  |      0.013 |   0.000 |        0.054 |        0.103 |\n| Beta: market_SEATTLE              |  0.106 | ***  |      0.012 |   0.000 |        0.083 |        0.129 |\n| Beta: market_ST_LOUIS             |  0.015 |      |      0.015 |   0.202 |       -0.015 |        0.045 |\n| Beta: market_SURBURBAN_NJ         | -0.046 | ***  |      0.015 |   0.002 |       -0.076 |       -0.016 |\n| Beta: market_SURBURBAN_NY         |  0.090 | ***  |      0.017 |   0.000 |        0.057 |        0.123 |\n| Beta: market_SYRACUSE             | -0.035 |  **  |      0.011 |   0.035 |       -0.056 |       -0.014 |\n| Beta: market_TAMPA                |  0.116 | ***  |      0.013 |   0.000 |        0.091 |        0.141 |\n| Beta: market_URBAN_NY             |  0.147 | ***  |      0.013 |   0.000 |        0.121 |        0.172 |\n| Beta: market_WASHINGTON_DC        |  0.098 | ***  |      0.013 |   0.000 |        0.073 |        0.123 |\n| Intercept                         | -2.207 | ***  |      0.001 |   0.000 |       -2.210 |       -2.205 |\n----------------------------------------------------------------------------------------------------------\n| Observations                      | 35,486 |      |            |         |              |              |\n| R²                                |  0.511 |      |            |         |              |              |\n| RMSE                              |  0.172 |      |            |         |              |              |\n+-----------------------------------+--------+------+------------+---------+--------------+--------------+\n\n\n\n\n\nModel 2\nWe must add one more predictor to Model 2. We will add an interaction term between beer brand and log of beer volume.\n\ninteraction_cols_brand_log_floz = add_interaction_terms(dummy_cols_brand, ['log_beer_floz'], var_list3=None)\n\n\nassembler_predictors_2 = (\n    x_cols_1 +\n    dummy_cols_brand + dummy_cols_market + dummy_cols_container +\n    interaction_cols_brand_log_floz\n)\n\n\nassembler_2 = VectorAssembler(\n    inputCols = assembler_predictors_2,\n    outputCol = \"predictors\"\n)\n\ndtrain_2 = assembler_2.transform(dtrain)\ndtest_2  = assembler_2.transform(dtest)\n\n# training model\nmodel_2 = (\n        LinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"log_price_floz\")\n    .fit(dtrain_2)\n)\n\n# making prediction\ndtrain_2 = model_2.transform(dtrain_2)\ndtest_2 = model_2.transform(dtest_2)\n\n\nSummary\n\nprint(regression_table(model_2, assembler_2))\n\n+-------------------------------------------+--------+------+------------+---------+--------------+--------------+\n| Metric                                    |  Value | Sig. | Std. Error | p-value | 95% CI Lower | 95% CI Upper |\n+-------------------------------------------+--------+------+------------+---------+--------------+--------------+\n| Beta: log_beer_floz                       | -0.142 | ***  |      0.027 |   0.000 |       -0.195 |       -0.090 |\n| Beta: brand_BUSCH_LIGHT                   | -0.146 | ***  |      0.024 |   0.000 |       -0.192 |       -0.100 |\n| Beta: brand_COORS_LIGHT                   | -0.108 | ***  |      0.022 |   0.000 |       -0.150 |       -0.065 |\n| Beta: brand_MILLER_LITE                   |  0.089 | ***  |      0.022 |   0.000 |        0.046 |        0.132 |\n| Beta: brand_NATURAL_LIGHT                 | -0.581 | ***  |      0.014 |   0.000 |       -0.609 |       -0.554 |\n| Beta: market_ALBANY                       |  0.034 |  **  |      0.012 |   0.015 |        0.010 |        0.057 |\n| Beta: market_ATLANTA                      |  0.084 | ***  |      0.015 |   0.000 |        0.055 |        0.113 |\n| Beta: market_BALTIMORE                    |  0.104 | ***  |      0.012 |   0.000 |        0.081 |        0.127 |\n| Beta: market_BIRMINGHAM                   |  0.138 | ***  |      0.013 |   0.000 |        0.113 |        0.163 |\n| Beta: market_BOSTON                       |  0.125 | ***  |      0.012 |   0.000 |        0.102 |        0.147 |\n| Beta: market_CHARLOTTE                    |  0.012 |      |      0.011 |   0.294 |       -0.010 |        0.035 |\n| Beta: market_CHICAGO                      | -0.006 |      |      0.012 |   0.607 |       -0.029 |        0.017 |\n| Beta: market_CINCINNATI                   |  0.088 | ***  |      0.012 |   0.000 |        0.064 |        0.111 |\n| Beta: market_CLEVELAND                    |  0.058 | ***  |      0.011 |   0.000 |        0.036 |        0.080 |\n| Beta: market_COLUMBUS                     |  0.081 | ***  |      0.011 |   0.000 |        0.059 |        0.102 |\n| Beta: market_DALLAS                       |  0.222 | ***  |      0.013 |   0.000 |        0.195 |        0.248 |\n| Beta: market_DENVER                       |  0.116 | ***  |      0.013 |   0.000 |        0.091 |        0.141 |\n| Beta: market_DES_MOINES                   |  0.144 | ***  |      0.011 |   0.000 |        0.122 |        0.166 |\n| Beta: market_DETROIT                      |  0.098 | ***  |      0.031 |   0.000 |        0.037 |        0.158 |\n| Beta: market_EXURBAN_NJ                   |  0.146 | ***  |      0.028 |   0.000 |        0.090 |        0.202 |\n| Beta: market_EXURBAN_NY                   |  0.076 | ***  |      0.013 |   0.008 |        0.050 |        0.101 |\n| Beta: market_GRAND_RAPIDS                 |  0.084 | ***  |      0.016 |   0.000 |        0.052 |        0.117 |\n| Beta: market_HARTFORD-NEW_HAVEN           |  0.128 | ***  |      0.011 |   0.000 |        0.106 |        0.151 |\n| Beta: market_HOUSTON                      |  0.115 | ***  |      0.012 |   0.000 |        0.091 |        0.139 |\n| Beta: market_INDIANAPOLIS                 |  0.093 | ***  |      0.016 |   0.000 |        0.061 |        0.124 |\n| Beta: market_JACKSONVILLE                 |  0.129 | ***  |      0.014 |   0.000 |        0.101 |        0.157 |\n| Beta: market_KANSAS_CITY                  |  0.071 | ***  |      0.015 |   0.000 |        0.041 |        0.101 |\n| Beta: market_LITTLE_ROCK                  |  0.101 | ***  |      0.011 |   0.000 |        0.078 |        0.123 |\n| Beta: market_LOS_ANGELES                  |  0.012 |      |      0.013 |   0.275 |       -0.014 |        0.038 |\n| Beta: market_LOUISVILLE                   |  0.072 | ***  |      0.015 |   0.000 |        0.043 |        0.102 |\n| Beta: market_MEMPHIS                      |  0.161 | ***  |      0.011 |   0.000 |        0.139 |        0.182 |\n| Beta: market_MIAMI                        |  0.109 | ***  |      0.013 |   0.000 |        0.084 |        0.134 |\n| Beta: market_MILWAUKEE                    |  0.038 | ***  |      0.012 |   0.003 |        0.014 |        0.063 |\n| Beta: market_MINNEAPOLIS                  |  0.134 | ***  |      0.013 |   0.000 |        0.109 |        0.159 |\n| Beta: market_NASHVILLE                    |  0.176 | ***  |      0.013 |   0.000 |        0.150 |        0.201 |\n| Beta: market_NEW_ORLEANS-MOBILE           |  0.135 | ***  |      0.013 |   0.000 |        0.110 |        0.160 |\n| Beta: market_OKLAHOMA_CITY-TULSA          |  0.157 | ***  |      0.012 |   0.000 |        0.133 |        0.181 |\n| Beta: market_OMAHA                        |  0.140 | ***  |      0.012 |   0.000 |        0.116 |        0.165 |\n| Beta: market_ORLANDO                      |  0.132 | ***  |      0.015 |   0.000 |        0.103 |        0.162 |\n| Beta: market_PHILADELPHIA                 |  0.095 | ***  |      0.011 |   0.000 |        0.073 |        0.117 |\n| Beta: market_PHOENIX                      |  0.157 | ***  |      0.016 |   0.000 |        0.125 |        0.188 |\n| Beta: market_PITTSBURGH                   |  0.086 | ***  |      0.014 |   0.000 |        0.059 |        0.114 |\n| Beta: market_PORTLAND                     |  0.105 | ***  |      0.012 |   0.000 |        0.081 |        0.129 |\n| Beta: market_RALEIGH-DURHAM               |  0.096 | ***  |      0.012 |   0.000 |        0.072 |        0.120 |\n| Beta: market_RICHMOND                     |  0.029 |  **  |      0.017 |   0.020 |       -0.004 |        0.061 |\n| Beta: market_RURAL_ALABAMA                |  0.166 | ***  |      0.020 |   0.000 |        0.127 |        0.205 |\n| Beta: market_RURAL_ARKANSAS               |  0.187 | ***  |      0.013 |   0.000 |        0.162 |        0.212 |\n| Beta: market_RURAL_CALIFORNIA             |  0.035 | ***  |      0.071 |   0.005 |       -0.103 |        0.174 |\n| Beta: market_RURAL_COLORADO               |  0.171 |  **  |      0.014 |   0.015 |        0.144 |        0.199 |\n| Beta: market_RURAL_FLORIDA                |  0.050 | ***  |      0.015 |   0.000 |        0.021 |        0.080 |\n| Beta: market_RURAL_GEORGIA                |  0.147 | ***  |      0.020 |   0.000 |        0.107 |        0.186 |\n| Beta: market_RURAL_IDAHO                  |  0.171 | ***  |      0.012 |   0.000 |        0.147 |        0.194 |\n| Beta: market_RURAL_ILLINOIS               |  0.017 |      |      0.015 |   0.162 |       -0.013 |        0.047 |\n| Beta: market_RURAL_INDIANA                |  0.105 | ***  |      0.012 |   0.000 |        0.082 |        0.129 |\n| Beta: market_RURAL_IOWA                   |  0.092 | ***  |      0.021 |   0.000 |        0.051 |        0.134 |\n| Beta: market_RURAL_KANSAS                 |  0.110 | ***  |      0.019 |   0.000 |        0.072 |        0.148 |\n| Beta: market_RURAL_KENTUCKY               |  0.154 | ***  |      0.016 |   0.000 |        0.123 |        0.184 |\n| Beta: market_RURAL_LOUISIANA              |  0.111 | ***  |      0.016 |   0.000 |        0.079 |        0.142 |\n| Beta: market_RURAL_MAINE                  |  0.096 | ***  |      0.013 |   0.000 |        0.070 |        0.121 |\n| Beta: market_RURAL_MICHIGAN               |  0.093 | ***  |      0.024 |   0.000 |        0.046 |        0.140 |\n| Beta: market_RURAL_MINNESOTA              |  0.257 | ***  |      0.016 |   0.000 |        0.225 |        0.289 |\n| Beta: market_RURAL_MISSISSIPPI            |  0.046 | ***  |      0.014 |   0.005 |        0.019 |        0.073 |\n| Beta: market_RURAL_MISSOURI               |  0.109 | ***  |      0.016 |   0.000 |        0.078 |        0.141 |\n| Beta: market_RURAL_MONTANA                |  0.114 | ***  |      0.024 |   0.000 |        0.066 |        0.161 |\n| Beta: market_RURAL_NEBRASKA               |  0.170 | ***  |      0.014 |   0.000 |        0.143 |        0.198 |\n| Beta: market_RURAL_NEVADA                 |  0.038 | ***  |      0.044 |   0.007 |       -0.048 |        0.125 |\n| Beta: market_RURAL_NEW_HAMPSHIRE          |  0.031 |      |      0.016 |   0.481 |       -0.000 |        0.062 |\n| Beta: market_RURAL_NEW_MEXICO             |  0.161 | ***  |      0.077 |   0.000 |        0.009 |        0.313 |\n| Beta: market_RURAL_NEW_YORK               | -0.059 |      |      0.013 |   0.448 |       -0.084 |       -0.033 |\n| Beta: market_RURAL_NORTH_CAROLINA         |  0.024 |  *   |      0.023 |   0.069 |       -0.022 |        0.070 |\n| Beta: market_RURAL_NORTH_DAKOTA           |  0.231 | ***  |      0.018 |   0.000 |        0.196 |        0.266 |\n| Beta: market_RURAL_OHIO                   |  0.100 | ***  |      0.036 |   0.000 |        0.028 |        0.171 |\n| Beta: market_RURAL_OKLAHOMA               |  0.139 | ***  |      0.047 |   0.000 |        0.047 |        0.231 |\n| Beta: market_RURAL_OREGON                 |  0.038 |      |      0.015 |   0.421 |        0.007 |        0.068 |\n| Beta: market_RURAL_PENNSYLVANIA           |  0.127 | ***  |      0.012 |   0.000 |        0.104 |        0.149 |\n| Beta: market_RURAL_SOUTH_CAROLINA         |  0.058 | ***  |      0.023 |   0.000 |        0.014 |        0.102 |\n| Beta: market_RURAL_SOUTH_DAKOTA           |  0.071 | ***  |      0.016 |   0.002 |        0.041 |        0.102 |\n| Beta: market_RURAL_TENNESSEE              |  0.194 | ***  |      0.011 |   0.000 |        0.172 |        0.217 |\n| Beta: market_RURAL_TEXAS                  |  0.175 | ***  |      0.022 |   0.000 |        0.132 |        0.218 |\n| Beta: market_RURAL_VERMONT                |  0.081 | ***  |      0.019 |   0.000 |        0.043 |        0.119 |\n| Beta: market_RURAL_VIRGINIA               |  0.015 |      |      0.016 |   0.448 |       -0.017 |        0.046 |\n| Beta: market_RURAL_WASHINGTON             |  0.141 | ***  |      0.018 |   0.000 |        0.106 |        0.176 |\n| Beta: market_RURAL_WEST_VIRGINIA          | -0.074 | ***  |      0.011 |   0.000 |       -0.097 |       -0.052 |\n| Beta: market_RURAL_WISCONSIN              |  0.040 | ***  |      0.037 |   0.000 |       -0.032 |        0.113 |\n| Beta: market_RURAL_WYOMING                |  0.154 | ***  |      0.012 |   0.000 |        0.130 |        0.178 |\n| Beta: market_SACRAMENTO                   |  0.014 |      |      0.017 |   0.235 |       -0.020 |        0.049 |\n| Beta: market_SALT_LAKE_CITY               |  0.127 | ***  |      0.011 |   0.000 |        0.106 |        0.148 |\n| Beta: market_SAN_ANTONIO                  |  0.143 | ***  |      0.013 |   0.000 |        0.117 |        0.169 |\n| Beta: market_SAN_DIEGO                    | -0.005 |      |      0.012 |   0.703 |       -0.029 |        0.019 |\n| Beta: market_SAN_FRANCISCO                |  0.077 | ***  |      0.013 |   0.000 |        0.053 |        0.102 |\n| Beta: market_SEATTLE                      |  0.099 | ***  |      0.012 |   0.000 |        0.076 |        0.122 |\n| Beta: market_ST_LOUIS                     |  0.014 |      |      0.015 |   0.215 |       -0.015 |        0.044 |\n| Beta: market_SURBURBAN_NJ                 | -0.047 | ***  |      0.015 |   0.002 |       -0.077 |       -0.017 |\n| Beta: market_SURBURBAN_NY                 |  0.087 | ***  |      0.017 |   0.000 |        0.054 |        0.120 |\n| Beta: market_SYRACUSE                     | -0.040 |  **  |      0.011 |   0.015 |       -0.061 |       -0.020 |\n| Beta: market_TAMPA                        |  0.114 | ***  |      0.013 |   0.000 |        0.088 |        0.139 |\n| Beta: market_URBAN_NY                     |  0.149 | ***  |      0.013 |   0.000 |        0.123 |        0.174 |\n| Beta: market_WASHINGTON_DC                |  0.095 | ***  |      0.005 |   0.000 |        0.086 |        0.105 |\n| Beta: brand_BUSCH_LIGHT_*_log_beer_floz   | -0.021 | ***  |      0.004 |   0.000 |       -0.029 |       -0.013 |\n| Beta: brand_COORS_LIGHT_*_log_beer_floz   |  0.019 | ***  |      0.004 |   0.000 |        0.011 |        0.027 |\n| Beta: brand_MILLER_LITE_*_log_beer_floz   | -0.019 | ***  |      0.004 |   0.000 |       -0.027 |       -0.011 |\n| Beta: brand_NATURAL_LIGHT_*_log_beer_floz |  0.048 | ***  |      0.018 |   0.000 |        0.012 |        0.083 |\n| Intercept                                 | -2.169 | ***  |      0.003 |   0.000 |       -2.174 |       -2.163 |\n------------------------------------------------------------------------------------------------------------------\n| Observations                              | 35,486 |      |            |         |              |              |\n| R²                                        |  0.516 |      |            |         |              |              |\n| RMSE                                      |  0.171 |      |            |         |              |              |\n+-------------------------------------------+--------+------+------------+---------+--------------+--------------+\n\n\n\nprint(dtest_2.select([\"prediction\", \"price_floz\"]).show())\n\n+-------------------+------------------+\n|         prediction|        price_floz|\n+-------------------+------------------+\n|  -2.79184159063699|0.0659027777777777|\n|-2.8594121622298223|0.0565277777777777|\n| -2.869616423365115|           0.06375|\n|-3.2968652189031418|0.0392222222222222|\n|-3.3630798574977083|0.0388611111111111|\n|-3.3630798574977083|0.0388611111111111|\n|-3.3630798574977083|0.0415833333333333|\n|-3.1358115955296135|0.0406944444444444|\n| -2.760190593306609|0.0620833333333333|\n|-2.7771575669310384|0.0740277777777777|\n| -2.958054745817342|0.0485763888888888|\n|-2.8540147644649227|0.0554861111111111|\n|-2.8540147644649227|0.0763194444444444|\n| -2.904018769588993|           0.04625|\n| -2.904018769588993|           0.04625|\n|-2.9670163656768107|0.0444166666666666|\n|-2.9670163656768107|0.0471944444444444|\n|-2.9670163656768107|0.0471944444444444|\n|-2.9670163656768107|           0.05275|\n| -2.891047126501169|0.0588888888888888|\n+-------------------+------------------+\nonly showing top 20 rows\n\nNone\n\n\n\n\n\nModel 3\nSimilarly to Model 2, we must add more predictors for Model 3. Here, we will add a dummy variable for promotion and several more interaction terms. We will consider the interaction between bran and beer volume, promotion and beer volume, brand and promotion, and brand, promotion, and beer volume.\n\ndummy_cols_promo, ref_category_promo = add_dummy_variables('promo', 0)\n\nReference category (dummy omitted): False\n\n\n\ninteraction_cols_brand_log_floz = add_interaction_terms(dummy_cols_brand, ['log_beer_floz'], var_list3=None)\ninteraction_cols_promo_log_floz = add_interaction_terms(dummy_cols_promo, ['log_beer_floz'], var_list3=None)\ninteraction_cols_brand_promo = add_interaction_terms(dummy_cols_brand, dummy_cols_promo, var_list3=None)\ninteraction_cols_brand_promo_log_floz = add_interaction_terms(dummy_cols_brand, dummy_cols_promo, ['log_beer_floz'])\n\n\nassembler_predictors_3 = (\n    x_cols_1 +\n    dummy_cols_brand + dummy_cols_market + dummy_cols_container +\n    interaction_cols_brand_log_floz +\n    interaction_cols_promo_log_floz +\n    interaction_cols_brand_promo +\n    interaction_cols_brand_promo_log_floz\n)\n\n\nassembler_3 = VectorAssembler(\n    inputCols = assembler_predictors_3,\n    outputCol = \"predictors\"\n)\n\ndtrain_3 = assembler_3.transform(dtrain)\ndtest_3  = assembler_3.transform(dtest)\n\n# training model\nmodel_3 = (\n        LinearRegression(featuresCol=\"predictors\",\n                                labelCol=\"log_price_floz\")\n    .fit(dtrain_3)\n)\n\n# making prediction\ndtrain_3 = model_3.transform(dtrain_3)\ndtest_3 = model_3.transform(dtest_3)\n\n\nSummary\n\nbeta_values = model_3.coefficients.toArray()\nfeature_names = assembler_3.getInputCols()\n\nbeta_table = []\nfor i in range(len(feature_names)):\n    beta_table.append([feature_names[i], beta_values[i]])\n\ntabulate(beta_table)\n\n'------------------------------------------------  -----------\\nlog_beer_floz                                     -0.136455\\nbrand_BUSCH_LIGHT                                 -0.0907828\\nbrand_COORS_LIGHT                                 -0.0737586\\nbrand_MILLER_LITE                                  0.133081\\nbrand_NATURAL_LIGHT                               -0.475828\\nmarket_ALBANY                                      0.0253785\\nmarket_ATLANTA                                     0.0763817\\nmarket_BALTIMORE                                   0.0933113\\nmarket_BIRMINGHAM                                  0.130783\\nmarket_BOSTON                                      0.120143\\nmarket_CHARLOTTE                                   0.0233496\\nmarket_CHICAGO                                    -0.00188634\\nmarket_CINCINNATI                                  0.0837354\\nmarket_CLEVELAND                                   0.0522246\\nmarket_COLUMBUS                                    0.0802702\\nmarket_DALLAS                                      0.226722\\nmarket_DENVER                                      0.129319\\nmarket_DES_MOINES                                  0.135893\\nmarket_DETROIT                                     0.0990648\\nmarket_EXURBAN_NJ                                  0.13442\\nmarket_EXURBAN_NY                                  0.0667537\\nmarket_GRAND_RAPIDS                                0.0826997\\nmarket_HARTFORD-NEW_HAVEN                          0.121944\\nmarket_HOUSTON                                     0.115509\\nmarket_INDIANAPOLIS                                0.0891036\\nmarket_JACKSONVILLE                                0.130776\\nmarket_KANSAS_CITY                                 0.0634972\\nmarket_LITTLE_ROCK                                 0.097217\\nmarket_LOS_ANGELES                                 0.0207496\\nmarket_LOUISVILLE                                  0.0713703\\nmarket_MEMPHIS                                     0.15059\\nmarket_MIAMI                                       0.109035\\nmarket_MILWAUKEE                                   0.0388026\\nmarket_MINNEAPOLIS                                 0.127782\\nmarket_NASHVILLE                                   0.176059\\nmarket_NEW_ORLEANS-MOBILE                          0.126602\\nmarket_OKLAHOMA_CITY-TULSA                         0.149795\\nmarket_OMAHA                                       0.141953\\nmarket_ORLANDO                                     0.135769\\nmarket_PHILADELPHIA                                0.0831244\\nmarket_PHOENIX                                     0.162801\\nmarket_PITTSBURGH                                  0.0798706\\nmarket_PORTLAND                                    0.106474\\nmarket_RALEIGH-DURHAM                              0.0886988\\nmarket_RICHMOND                                    0.0218335\\nmarket_RURAL_ALABAMA                               0.162651\\nmarket_RURAL_ARKANSAS                              0.178275\\nmarket_RURAL_CALIFORNIA                            0.0341722\\nmarket_RURAL_COLORADO                              0.178842\\nmarket_RURAL_FLORIDA                               0.048985\\nmarket_RURAL_GEORGIA                               0.140319\\nmarket_RURAL_IDAHO                                 0.167423\\nmarket_RURAL_ILLINOIS                              0.0143436\\nmarket_RURAL_INDIANA                               0.104391\\nmarket_RURAL_IOWA                                  0.0879043\\nmarket_RURAL_KANSAS                                0.102565\\nmarket_RURAL_KENTUCKY                              0.148982\\nmarket_RURAL_LOUISIANA                             0.101282\\nmarket_RURAL_MAINE                                 0.0952734\\nmarket_RURAL_MICHIGAN                              0.0860976\\nmarket_RURAL_MINNESOTA                             0.250628\\nmarket_RURAL_MISSISSIPPI                           0.041842\\nmarket_RURAL_MISSOURI                              0.100376\\nmarket_RURAL_MONTANA                               0.125706\\nmarket_RURAL_NEBRASKA                              0.166878\\nmarket_RURAL_NEVADA                                0.0365143\\nmarket_RURAL_NEW_HAMPSHIRE                         0.0245408\\nmarket_RURAL_NEW_MEXICO                            0.151578\\nmarket_RURAL_NEW_YORK                             -0.0721252\\nmarket_RURAL_NORTH_CAROLINA                        0.0120202\\nmarket_RURAL_NORTH_DAKOTA                          0.226628\\nmarket_RURAL_OHIO                                  0.0947752\\nmarket_RURAL_OKLAHOMA                              0.125907\\nmarket_RURAL_OREGON                                0.0401582\\nmarket_RURAL_PENNSYLVANIA                          0.116653\\nmarket_RURAL_SOUTH_CAROLINA                        0.0593814\\nmarket_RURAL_SOUTH_DAKOTA                          0.0652301\\nmarket_RURAL_TENNESSEE                             0.205072\\nmarket_RURAL_TEXAS                                 0.172253\\nmarket_RURAL_VERMONT                               0.0837308\\nmarket_RURAL_VIRGINIA                              0.00853721\\nmarket_RURAL_WASHINGTON                            0.164909\\nmarket_RURAL_WEST_VIRGINIA                        -0.0849587\\nmarket_RURAL_WISCONSIN                             0.0389056\\nmarket_RURAL_WYOMING                               0.147967\\nmarket_SACRAMENTO                                  0.0217214\\nmarket_SALT_LAKE_CITY                              0.124676\\nmarket_SAN_ANTONIO                                 0.13696\\nmarket_SAN_DIEGO                                  -0.0029509\\nmarket_SAN_FRANCISCO                               0.0846383\\nmarket_SEATTLE                                     0.110073\\nmarket_ST_LOUIS                                    0.0168797\\nmarket_SURBURBAN_NJ                               -0.0575105\\nmarket_SURBURBAN_NY                                0.0850405\\nmarket_SYRACUSE                                   -0.0509936\\nmarket_TAMPA                                       0.113092\\nmarket_URBAN_NY                                    0.143904\\nmarket_WASHINGTON_DC                               0.0894661\\nbrand_BUSCH_LIGHT_*_log_beer_floz                 -0.01597\\nbrand_COORS_LIGHT_*_log_beer_floz                  0.00641913\\nbrand_MILLER_LITE_*_log_beer_floz                 -0.0136419\\nbrand_NATURAL_LIGHT_*_log_beer_floz                0.0139777\\npromo_True_*_log_beer_floz                        -0.00484056\\nbrand_BUSCH_LIGHT_*_promo_True                    -0.183267\\nbrand_COORS_LIGHT_*_promo_True                    -0.118051\\nbrand_MILLER_LITE_*_promo_True                    -0.198857\\nbrand_NATURAL_LIGHT_*_promo_True                  -0.214949\\nbrand_BUSCH_LIGHT_*_promo_True                    -0.183267\\nbrand_COORS_LIGHT_*_promo_True                    -0.118051\\nbrand_MILLER_LITE_*_promo_True                    -0.198857\\nbrand_NATURAL_LIGHT_*_promo_True                  -0.214949\\nbrand_BUSCH_LIGHT_*_log_beer_floz                 -0.01597\\nbrand_COORS_LIGHT_*_log_beer_floz                  0.00641913\\nbrand_MILLER_LITE_*_log_beer_floz                 -0.0136419\\nbrand_NATURAL_LIGHT_*_log_beer_floz                0.0139777\\npromo_True_*_log_beer_floz                        -0.00484056\\nbrand_BUSCH_LIGHT_*_promo_True_*_log_beer_floz     0.067596\\nbrand_COORS_LIGHT_*_promo_True_*_log_beer_floz     0.0411765\\nbrand_MILLER_LITE_*_promo_True_*_log_beer_floz     0.071225\\nbrand_NATURAL_LIGHT_*_promo_True_*_log_beer_floz   0.0779679\\n------------------------------------------------  -----------'\n\n\n\nprint(dtest_3.select([\"prediction\", \"price_floz\"]).show())\n\n+-------------------+------------------+\n|         prediction|        price_floz|\n+-------------------+------------------+\n| -2.783825132146986|0.0659027777777777|\n| -2.852181232185633|0.0565277777777777|\n|-2.8567168160602634|           0.06375|\n|-3.2846580127148473|0.0392222222222222|\n| -3.367438927218041|0.0388611111111111|\n| -3.329453847087418|0.0388611111111111|\n| -3.329453847087418|0.0415833333333333|\n|-3.1540302069906416|0.0406944444444444|\n|-2.7676073259050664|0.0620833333333333|\n|-2.7590519598928407|0.0740277777777777|\n| -3.001588386461357|0.0485763888888888|\n|-2.9218829165297717|0.0554861111111111|\n|-2.8423075221373013|0.0763194444444444|\n|-2.9592348912579673|           0.04625|\n|-2.9592348912579673|           0.04625|\n|-3.0062928141417586|0.0444166666666666|\n|-3.0062928141417586|0.0471944444444444|\n|-3.0062928141417586|0.0471944444444444|\n|-2.9555763843672302|           0.05275|\n|-2.8847264383485465|0.0588888888888888|\n+-------------------+------------------+\nonly showing top 20 rows\n\nNone\n\n\n\n\nInterpreting Beta Estimates (Model 3)\n\nThe beta value for market_ALBANY for Model 3 is 0.0253785.\nThe beta value for market_EXURBAN_NY is 0.0667537.\nThe beta value for market_RURAL_NEW_YORK is -0.0721252.\nThe beta value for market_SUBURBAN_NY is 0.0850405.\nThe beta value for market_SYRACUSE is -0.0509936.\nThe beta value for market_URBAN_NY is 0.143904.\n\nAll of the above beta values for different beer markets from Model 3 are very close to zero. This suggests that the predictor variable, market, has little impact on the dependent or target variable, log price per fluid ounce."
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#model-comparison",
    "href": "posts/hw3-beer/hw3_part2.html#model-comparison",
    "title": "HW 3 - Beer Markets",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nBeta Estimates\nThe beta value for log beer volume in Model 1 is -0.135. The beta value for log beer volume from Model 2 is -0.142. The beta value from Model 3 is -0.136. All three beta values for log beer volume are between -1 and 0. This suggests price inelasticity, meaning a precentage change in log beer volume would lead to less than a percentage change in log beer price per fluid ounce. However, all three beta values are also very close to 0. This shows that the percentage change in beer price is relatively insenstive to a percentage change in beer volume.\nThe beta value for the interaction term between promo and log beer volume from Model 3 is -0.0048. This value is less negative than the beta value for log beer volume, suggesting that the use of promotion decreases price senstivity. In addition, this beta value is even closer to 0, meaning a change in beer volume leads to relatively no change in beer price while using promo.\n\n\nResidual Plots\n\nModel 1\n\n# Convert test predictions to Pandas\ndfpd_1 = dtest_1.select([\"prediction\", \"price_floz\"]).toPandas()\ndfpd_1[\"residual\"] = dfpd_1[\"price_floz\"] - dfpd_1[\"prediction\"]\nplt.scatter(dfpd_1[\"prediction\"], dfpd_1[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd_1[\"residual\"], dfpd_1[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()\n\n\n\n\nOn the residual plot for Model 1, the residuals are centered around zero, meaning that on average, the predictions are correct. However, the plot shows some curvature, suggesting that there may be some systematic errors.\n\n\nModel 2\n\n# Convert test predictions to Pandas\ndfpd_2 = dtest_2.select([\"prediction\", \"price_floz\"]).toPandas()\ndfpd_2[\"residual\"] = dfpd_2[\"price_floz\"] - dfpd_2[\"prediction\"]\nplt.scatter(dfpd_2[\"prediction\"], dfpd_2[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd_2[\"residual\"], dfpd_2[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()\n\n\n\n\nFor Model 2, the residuals are centered around 0, like Model 1. This indicates that the model’s predictions are correct on average. The plot is curved but has less curvature than Model 1, showing that it may have less systematic errors.\n\n\nModel 3\n\n# Convert test predictions to Pandas\ndfpd_3 = dtest_3.select([\"prediction\", \"price_floz\"]).toPandas()\ndfpd_3[\"residual\"] = dfpd_3[\"price_floz\"] - dfpd_3[\"prediction\"]\nplt.scatter(dfpd_3[\"prediction\"], dfpd_3[\"residual\"], alpha=0.2, color=\"darkgray\")\n\n# Use lowess smoothing for the trend line\nsmoothed = sm.nonparametric.lowess(dfpd_3[\"residual\"], dfpd_3[\"prediction\"])\nplt.plot(smoothed[:, 0], smoothed[:, 1], color=\"darkblue\")\nplt.axhline(y=0, color=\"red\", linestyle=\"--\")\nplt.xlabel(\"Predicted y (Model)\")\nplt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot for Model\")\nplt.show()\n\n\n\n\nLike Models 1 and 2, the residuals for Model 3 are centered around 0. This shows that the model’s predictions are correct on average. The plot is slightly curved but has less curvature than Models 1 and 2, showing that it may have less systematic errors."
  },
  {
    "objectID": "posts/hw3-beer/hw3_part2.html#conclusion",
    "href": "posts/hw3-beer/hw3_part2.html#conclusion",
    "title": "HW 3 - Beer Markets",
    "section": "Conclusion",
    "text": "Conclusion\nI prefer Model 3 because its residual plot shows the least amount of possible systematic errors with the predictions still being correct on average. However, the beta values for the beer markets are very close to 0, meaning that the market has little impact on the dependent variable. The market variable could be removed from this model and a different variable could be tested in its place."
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html",
    "href": "posts/python basics/python_basics_blog.html",
    "title": "Python Basics",
    "section": "",
    "text": "A variable is a name that refers to a value.\n\na = 10\nprint(a)\n\n10\n\n\nThe most basic built-in data types that we’ll need to know about are integers, floats, strings, and booleans.\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\n[10, 1.23, 'like this', True, None]\n\n\nlist"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#variables",
    "href": "posts/python basics/python_basics_blog.html#variables",
    "title": "Python Basics",
    "section": "",
    "text": "A variable is a name that refers to a value.\n\na = 10\nprint(a)\n\n10\n\n\nThe most basic built-in data types that we’ll need to know about are integers, floats, strings, and booleans.\n\nlist_example = [10, 1.23, \"like this\", True, None]\nprint(list_example)\ntype(list_example)\n\n[10, 1.23, 'like this', True, None]\n\n\nlist"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#booleans-and-conditions",
    "href": "posts/python basics/python_basics_blog.html#booleans-and-conditions",
    "title": "Python Basics",
    "section": "Booleans and Conditions",
    "text": "Booleans and Conditions\nBoolean data have either True or False value.\nConditions are expressions that evaluate as booleans.\n\n20 == '20'\n\nFalse\n\n\nExisting booleans can be combined, which create a boolean when executed.\n\nx = 4.0\ny = .5\nz = 3*y - x\n\nx &lt; y or 3*y &lt; x\n\nTrue\n\n\n\nboolean_condition1 = 10 == 20\nprint(boolean_condition1)\n\nboolean_condition2 = 10 == '10'\nprint(boolean_condition2)\n\nFalse\nFalse\n\n\nThe real power of conditions comes when we start to use them in more complex examples, such as if statements.\n\nname = \"Geneseo\"\nscore = 99\n\nif name == \"Geneseo\" and score &gt; 90:\n    print(\"Geneseo, you achieved a high score.\")\n\nif name == \"Geneseo\" or score &gt; 90:\n    print(\"You could be called Geneseo or have a high score\")\n\nif name != \"Geneseo\" and score &gt; 90:\n    print(\"You are not called Geneseo and you have a high score\")\n\nGeneseo, you achieved a high score.\nYou could be called Geneseo or have a high score\n\n\nThe if-else chain:\n\nscore = 98\n\nif score == 100:\n    print(\"Top marks!\")\nelif score &gt; 90 and score &lt; 100:\n    print(\"High score!\")\nelif score &gt; 10 and score &lt;= 90:\n    pass\nelse:\n    print(\"Better luck next time.\")\n\nHigh score!"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#for-loops",
    "href": "posts/python basics/python_basics_blog.html#for-loops",
    "title": "Python Basics",
    "section": "For Loops",
    "text": "For Loops\nA loop is a way of executing a similar piece of code over and over in a similar way. The most useful type is for loops.\n\nname_list = [\"Ben\", \"Chris\", \"Kate\", \"Mary\"]\n\nfor name in name_list:\n    print(name)\n\nBen\nChris\nKate\nMary"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#dictionaries",
    "href": "posts/python basics/python_basics_blog.html#dictionaries",
    "title": "Python Basics",
    "section": "Dictionaries",
    "text": "Dictionaries\nA dictionary maps one set of variables to another (one-to-one or many-to-one).\n\ncities_to_temps = {\"Paris\": 28, \"London\": 22, \"Seville\": 36, \"Wellesley\": 29}\n\ncities_to_temps.keys()\ncities_to_temps.values()\ncities_to_temps.items()\n\ndict_items([('Paris', 28), ('London', 22), ('Seville', 36), ('Wellesley', 29)])"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#running-on-empty",
    "href": "posts/python basics/python_basics_blog.html#running-on-empty",
    "title": "Python Basics",
    "section": "Running on Empty",
    "text": "Running on Empty\nCreating empty containers is useful when creating loops. The commands to create empty lists, tuples, dictionaries, and sets are lst = [], tup=(), dic={}, and st = set() respectively."
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#slicing-methods",
    "href": "posts/python basics/python_basics_blog.html#slicing-methods",
    "title": "Python Basics",
    "section": "Slicing Methods",
    "text": "Slicing Methods\nWe can extract a substring (a part of a string) from a string by using a slice.\n\nletters = 'abcdefghij'\nletters[:]\n\n'abcdefghij'\n\n\n\nletters = 'abcdefghij'\nletters[4:]\nletters[2:]\nletters[-3:]\nletters[-50:]\n\n'abcdefghij'\n\n\n\nletters = 'abcdefghij'\nletters[2:5]\nletters[-26:-24]\nletters[35:37]\n\n''\n\n\nWe can extract a single value from a list by specifying its index:\n\nsuny = ['Geneseo', 'Brockport', 'Oswego', 'Binghamton',\n        'Stony Brook', 'New Paltz']\n\n\nsuny[0]\nsuny[1]\nsuny[2]\n\n'Oswego'\n\n\nExample from Classwork 4.3:\n\nfare = \"$10.00\"\ntip = \"2.00$\"\ntax = \"$ 0.80\"\ntotal = fare[0:2] + tip[0] + tax[3:6]\nprint(\"The total trip cost is:\", total)\n\nThe total trip cost is: $12.80"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#functions-arguments-parameters",
    "href": "posts/python basics/python_basics_blog.html#functions-arguments-parameters",
    "title": "Python Basics",
    "section": "Functions, Arguments, Parameters",
    "text": "Functions, Arguments, Parameters\nA function can take any number and type of input parameters and return any number and type of output results.\n\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep = \"!\")\nprint(\"Cherry\", \"Strawberry\", \"Key Lime\", sep=\" \")\n\nCherry Strawberry Key Lime\nCherry!Strawberry!Key Lime\nCherry Strawberry Key Lime\n\n\nA function can accept inputs called arguments.\nA parameter is an expected function argument.\nExample from Classwork 4.4:\n\nlist_variable = [100,144,169,1000,8]\nx =max(list_variable)\nprint('The largest value in the list is:',x)\n\nThe largest value in the list is: 1000"
  },
  {
    "objectID": "posts/python basics/python_basics_blog.html#installing-modules-packages-and-libraries",
    "href": "posts/python basics/python_basics_blog.html#installing-modules-packages-and-libraries",
    "title": "Python Basics",
    "section": "Installing Modules, Packages, and Libraries",
    "text": "Installing Modules, Packages, and Libraries\nTo install a module module_name on your Google Colab, run:\n\n!pip install module_name\n\nFrom Classwork 4.5: Import the pandas library as pd. Install the itables package. From itables, import the functions init_notebook_mode and show.\n\nimport pandas as pd\n!pip install itables\nfrom itables import init_notebook_mode\nfrom itables import show\n\nCollecting itables\n  Downloading itables-1.7.0-py3-none-any.whl (200 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/200.9 kB ? eta -:--:--     ━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━ 71.7/200.9 kB 1.9 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.9/200.9 kB 2.9 MB/s eta 0:00:00\nRequirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from itables) (7.34.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from itables) (1.5.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from itables) (1.25.2)\nRequirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (67.7.2)\nCollecting jedi&gt;=0.16 (from IPython-&gt;itables)\n  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 7.3 MB/s eta 0:00:00\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.7.5)\nRequirement already satisfied: traitlets&gt;=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (3.0.43)\nRequirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (2.16.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (0.1.6)\nRequirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.10/dist-packages (from IPython-&gt;itables) (4.9.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;itables) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;itables) (2023.4)\nRequirement already satisfied: parso&lt;0.9.0,&gt;=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi&gt;=0.16-&gt;IPython-&gt;itables) (0.8.3)\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&gt;4.3-&gt;IPython-&gt;itables) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,&lt;3.1.0,&gt;=2.0.0-&gt;IPython-&gt;itables) (0.2.13)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;itables) (1.16.0)\nInstalling collected packages: jedi, itables\nSuccessfully installed itables-1.7.0 jedi-0.19.1"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "DANL Project",
    "section": "",
    "text": "About this project 👏\nThis project will address the various financial actions taken by various parties to address climate change."
  },
  {
    "objectID": "project.html#variable-description",
    "href": "project.html#variable-description",
    "title": "DANL Project",
    "section": "2.1 Variable Description",
    "text": "2.1 Variable Description\n\nParty: a party (country) that provides a funding contribution to recipient country/region for their cliamte change project.\nRecipient country/region: Recipient country or region\nProject/programme/activity: Details in the climate change project\nType of support:\n\nadaptation if the climate change project is related to adaptation project.\nmitigation if the climate change project is related to mitigation project.\n\nYear: Year that funding contribution is committed or provided.\nContribution: An amount of funding contribution for the climate change project (in USD).\nStatus:\n\ncommitted if a party commits to providing the funding contribution for the climate change project, but the funding contribution is NOT actually provided.\nprovided if the funding contribution was provided for the climate change project.\n\nEnergy:\n\nTRUE if the project is energy-related;\nFALSE otherwise."
  },
  {
    "objectID": "project.html#summary-statistics",
    "href": "project.html#summary-statistics",
    "title": "DANL Project",
    "section": "2.2 Summary Statistics",
    "text": "2.2 Summary Statistics\n\n\n\n  \n\n\n\nskim(climate_finance) %&gt;% \n  select(-n_missing)\n\n\nData summary\n\n\nName\nclimate_finance\n\n\nNumber of rows\n16853\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nlogical\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nParty\n1.00\n5\n24\n0\n34\n0\n\n\nRecipient country/region\n1.00\n3\n293\n0\n1310\n0\n\n\nProject/programme/activity\n0.68\n3\n1473\n0\n7908\n0\n\n\nType of support\n1.00\n10\n10\n0\n2\n0\n\n\nStatus\n1.00\n8\n9\n0\n2\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\ncomplete_rate\nmean\ncount\n\n\n\n\nEnergy\n1\n0.19\nFAL: 13710, TRU: 3143\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n1\n2015.72\n1.82\n2011\n2015\n2016\n2017\n2018\n▁▂▆▅▇\n\n\nContribution\n1\n3311433.39\n9714907.28\n3000\n100000\n420000\n2200000\n100000000\n▇▁▁▁▁"
  },
  {
    "objectID": "project.html#financial-contributions-to-other-countries",
    "href": "project.html#financial-contributions-to-other-countries",
    "title": "DANL Project",
    "section": "3.1 Financial Contributions to Other Countries",
    "text": "3.1 Financial Contributions to Other Countries\nVarious parties in the climate_finance data frame have made positive financial contributions to other countries for adaptation projects. Here, we will find the number of parties that made a positive contribution to another country for every year between 2011 and 2018.\n\npositive_contributions &lt;- climate_finance %&gt;% \n  filter(Status == \"provided\",                       \n         `Type of support` == \"adaptation\") %&gt;%      \n  group_by(Party, Year) %&gt;%                          \n  summarise(Contribution = sum(Contribution, na.rm = T)) %&gt;%  \n  filter(Contribution &gt; 0) %&gt;%                       \n  group_by(Party) %&gt;%                                \n  count() %&gt;%                                        \n  filter(n == 2018 - 2011 + 1)  %&gt;%                  \n  select(Party) %&gt;% \n  distinct()    \n\n\nnrow(positive_contributions)\n\n[1] 8"
  },
  {
    "objectID": "project.html#types-of-contributions",
    "href": "project.html#types-of-contributions",
    "title": "DANL Project",
    "section": "3.2 Types of Contributions",
    "text": "3.2 Types of Contributions\nThere are two different types of contributions that a party can make: adaptation and mitigation. The type of contribution is decided based on the type of project it supports. Adaptation contributions go to an adaptation climate change project. A mitigation contribution goes to a mitigation climate change project.\nFirst, we will calculate the ratio between adaptation contribution and mitigation contribution for each type of Status for each Party each year.\n\nratio &lt;- climate_finance %&gt;% \n  group_by(Party, Year, Status, `Type of support`) %&gt;% \n  summarise(Contribution = sum(Contribution, na.rm = T)) %&gt;% \n  filter(Contribution != 0) %&gt;% \n  group_by(Party, Year, Status) %&gt;% \n  mutate(lag_Contribution = lag(Contribution), \n         am_ratio = lag_Contribution / Contribution ) %&gt;% \n  filter(!is.na(am_ratio)) %&gt;% \n  rename(mitigation = Contribution, \n         adaptation = lag_Contribution) %&gt;% \n  select(-`Type of support`) \n\nHere, we will visualize the distribution of the ratio between adaption contribution and mitigation contribution based on our calculation.\n\nggplot(ratio, aes(x = log(am_ratio))) +\n  geom_histogram(bins = 75) + labs(x = \"log(ratio)\", y = \"count\", title = \"Distribution of the Ratio of Contributions\") +\n  geom_vline(xintercept = 0, color = 'red', lty = 2)\n\n\n\n\nThis histogram depicts a generally normal distribution of the ratio between adaptation contribution and mitigation contribution."
  },
  {
    "objectID": "project.html#yearly-trend-of-total-funding-contributions-varies-by-energy-and-status",
    "href": "project.html#yearly-trend-of-total-funding-contributions-varies-by-energy-and-status",
    "title": "DANL Project",
    "section": "3.3 Yearly trend of total funding contributions varies by Energy and Status",
    "text": "3.3 Yearly trend of total funding contributions varies by Energy and Status\nWith these graphs we can see the amount of committed funding has been increasing at a significant rate while provided funding seems to stay constant. The gap we see in provided funding size between the energy and non energy sectors is likely due to these projects requiring a significant amount of funds upfront.\n\nclimate_finance %&gt;% \n  group_by(Energy, Status, Year) %&gt;% \n  summarise(funding_tot = sum(Contribution, na.rm = T)) %&gt;% \n  ggplot(aes(x = Year, y = funding_tot)) +\n  geom_line(aes(color = Status)) +\n  geom_point() +\n  facet_wrap(Energy ~.) +\n  scale_y_comma()"
  },
  {
    "objectID": "project.html#yearly-contribution-varying-by-energy-and-status",
    "href": "project.html#yearly-contribution-varying-by-energy-and-status",
    "title": "DANL Project",
    "section": "3.4 Yearly Contribution varying by Energy and Status",
    "text": "3.4 Yearly Contribution varying by Energy and Status\nFor both sectors (energy and non-energy), the amount of the committed funding has been increasing yearly, while the amount of provided funding has stayed relatively constant. Energy related projects usually require a much greater upfront cost than non-energy related projects, hence why there is a gap for the provided amounts.\n\nggplot(climate_finance,\n       aes(color = `Type of support`, x = log(Contribution))) +\n  geom_freqpoly() +\n  facet_wrap(.~ Status) +\n  theme(legend.position = 'top')"
  },
  {
    "objectID": "spotify_willie_nelson.html",
    "href": "spotify_willie_nelson.html",
    "title": "Spotify Data Frame",
    "section": "",
    "text": "Below is the Spotify Data Frame that reads the file spotify_all.csv containing data of Spotify users’ playlist information (Source: Spotify Million Playlist Dataset Challenge).\nimport pandas as pd\nfrom google.colab import data_table\ndata_table.enable_dataframe_formatter()\n\nspotify = pd.read_csv('https://bcdanl.github.io/data/spotify_all.csv')\nspotify\n\nWarning: total number of rows (198005) exceeds max_rows (20000). Falling back to pandas display.\n\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\nartist_name\ntrack_name\nduration_ms\nalbum_name\n\n\n\n\n0\n0\nThrowbacks\n0\nMissy Elliott\nLose Control (feat. Ciara & Fat Man Scoop)\n226863\nThe Cookbook\n\n\n1\n0\nThrowbacks\n1\nBritney Spears\nToxic\n198800\nIn The Zone\n\n\n2\n0\nThrowbacks\n2\nBeyoncé\nCrazy In Love\n235933\nDangerously In Love (Alben für die Ewigkeit)\n\n\n3\n0\nThrowbacks\n3\nJustin Timberlake\nRock Your Body\n267266\nJustified\n\n\n4\n0\nThrowbacks\n4\nShaggy\nIt Wasn't Me\n227600\nHot Shot\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n198000\n999998\n✝️\n6\nChris Tomlin\nWaterfall\n209573\nLove Ran Red\n\n\n198001\n999998\n✝️\n7\nChris Tomlin\nThe Roar\n220106\nLove Ran Red\n\n\n198002\n999998\n✝️\n8\nCrowder\nLift Your Head Weary Sinner (Chains)\n224666\nNeon Steeple\n\n\n198003\n999998\n✝️\n9\nChris Tomlin\nWe Fall Down\n280960\nHow Great Is Our God: The Essential Collection\n\n\n198004\n999998\n✝️\n10\nCaleb and Kelsey\n10,000 Reasons / What a Beautiful Name\n178189\n10,000 Reasons / What a Beautiful Name\n\n\n\n\n\n198005 rows × 7 columns"
  },
  {
    "objectID": "spotify_willie_nelson.html#variable-description",
    "href": "spotify_willie_nelson.html#variable-description",
    "title": "Spotify Data Frame",
    "section": "Variable Description",
    "text": "Variable Description\n\npid: playlist ID; unique ID for playlist\nplaylist_name: a name of playlist\npos: a position of the track within a playlist (starting from 0)\nartist_name: name of the track’s primary artist\ntrack_name: name of the track\nduration_ms: duration of the track in milliseconds\nalbum_name: name of the track’s album"
  },
  {
    "objectID": "spotify_willie_nelson.html#favorite-artist",
    "href": "spotify_willie_nelson.html#favorite-artist",
    "title": "Spotify Data Frame",
    "section": "Favorite Artist",
    "text": "Favorite Artist\nMy favorite artist in the Spotify data frame is Willie Nelson.\nAll of Willie Nelson’s songs in the Spotify data frame and their details are displayed below:\n\n(\n    spotify\n    .set_index('artist_name')\n    .loc['Willie Nelson']\n)\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n9\nold country\n0\nHighwayman\n182973\nNashville Rebel\n\n\nWillie Nelson\n9\nold country\n1\nHighwayman\n182973\nNashville Rebel\n\n\nWillie Nelson\n83\nFlorida\n66\nAlways On My Mind\n212666\nAlways On My Mind\n\n\nWillie Nelson\n83\nFlorida\n67\nLuckenback Texas\n94520\nCountry Outlaws\n\n\nWillie Nelson\n90\nFor the Road\n67\nHighwayman\n182973\nNashville Rebel\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nWillie Nelson\n999873\ndads\n2\nI Gotta Get Drunk\n129759\nWalking the Line\n\n\nWillie Nelson\n999873\ndads\n6\nPancho and Lefty\n286066\nWalking the Line\n\n\nWillie Nelson\n999873\ndads\n45\nSeven Spanish Angels (With Ray Charles)\n229533\nHalf Nelson\n\n\nWillie Nelson\n999897\nKings\n153\nOn the Road Again\n153106\nGreatest Hits (& Some That Will Be)\n\n\nWillie Nelson\n999936\nRoad Trip\n67\nOn the Road Again\n153106\nGreatest Hits (& Some That Will Be)\n\n\n\n\n\n62 rows × 6 columns"
  },
  {
    "objectID": "spotify_willie_nelson.html#number-of-songs",
    "href": "spotify_willie_nelson.html#number-of-songs",
    "title": "Spotify Data Frame",
    "section": "Number of Songs",
    "text": "Number of Songs\n\nwillie_nelson = (\n    spotify\n    .set_index('artist_name')\n    .loc['Willie Nelson']\n)\nlen(willie_nelson)\n\n62\n\n\nThere are 62 Willie Nelson songs in this data frame."
  },
  {
    "objectID": "spotify_willie_nelson.html#longest-song",
    "href": "spotify_willie_nelson.html#longest-song",
    "title": "Spotify Data Frame",
    "section": "Longest Song",
    "text": "Longest Song\n\n(\n    willie_nelson\n    .sort_values(['duration_ms'], ascending = False)\n    .head(1)\n)\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n114\nWillie\n13\nSunday Mornin' Comin' Down\n422173\nWillie Nelson Sings Kristofferson\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nThe longest Willie Nelson song in this data set is “Sunday Mornin’ Comin’ Down” at 422173 milliseconds."
  },
  {
    "objectID": "spotify_willie_nelson.html#shortest-song",
    "href": "spotify_willie_nelson.html#shortest-song",
    "title": "Spotify Data Frame",
    "section": "Shortest Song",
    "text": "Shortest Song\n\n(\n    willie_nelson\n    .sort_values(['duration_ms'], ascending = True)\n    .head(1)\n)\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n83\nFlorida\n67\nLuckenback Texas\n94520\nCountry Outlaws\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\nThe shortest Willie Nelson song is “Luckenback Texas” at 94520 milliseconds."
  },
  {
    "objectID": "spotify_willie_nelson.html#songs-between-150000-and-200000-ms-in-the-willie-playlist",
    "href": "spotify_willie_nelson.html#songs-between-150000-and-200000-ms-in-the-willie-playlist",
    "title": "Spotify Data Frame",
    "section": "Songs between 150000 and 200000 ms in the “Willie” playlist",
    "text": "Songs between 150000 and 200000 ms in the “Willie” playlist\nBelow are all songs in the “Willie” playlist that are greater than 150000 ms but less than 200000 ms:\n\ngreater_than_15 = willie_nelson['duration_ms'] &gt; 150000\nless_than_20 = willie_nelson['duration_ms'] &lt; 200000\nwillie_playlist = willie_nelson['playlist_name'] == 'Willie'\nwillie_nelson[greater_than_15 & less_than_20 & willie_playlist]\n\n\n  \n    \n\n\n\n\n\n\npid\nplaylist_name\npos\ntrack_name\nduration_ms\nalbum_name\n\n\nartist_name\n\n\n\n\n\n\n\n\n\n\nWillie Nelson\n114\nWillie\n1\nLittle House on the Hill\n182080\nGod's Problem Child\n\n\nWillie Nelson\n114\nWillie\n3\nNothing I Can Do About It Now\n199160\nA Horse Called Music\n\n\nWillie Nelson\n114\nWillie\n8\nNight Life\n197813\nFor the Good Times: A Tribute to Ray Price\n\n\nWillie Nelson\n114\nWillie\n10\nIf You Can Touch Her At All\n183866\nFunny How Time Slips Away - The Best Of\n\n\nWillie Nelson\n114\nWillie\n11\nStay All Night (Stay A Little Longer)\n154160\nShotgun Willie\n\n\nWillie Nelson\n114\nWillie\n12\nSad Songs And Waltzes\n185213\nShotgun Willie\n\n\nWillie Nelson\n114\nWillie\n19\nFly Me To The Moon\n169960\nAmerican Classic\n\n\nWillie Nelson\n114\nWillie\n21\nMona Lisa\n151826\nSomewhere over the Rainbow\n\n\nWillie Nelson\n114\nWillie\n22\nTexas On A Saturday Night (With Mel Tillis)\n159533\nHalf Nelson\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nwillie_songs = willie_nelson[greater_than_15 & less_than_20 & willie_playlist]\nlen(willie_songs)\n\n9\n\n\nThere are 9 songs in the “Willie” playlist within this range."
  }
]